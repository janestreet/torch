// THIS FILE IS AUTOMATICALLY GENERATED, DO NOT EDIT BY HAND!

raw_tensor atg___and__(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::__and__(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___and__tensor_(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::__and__(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___iand__(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).__iand__(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___iand__tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).__iand__(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___ilshift__(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).__ilshift__(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___ilshift__tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).__ilshift__(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___ior__(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).__ior__(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___ior__tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).__ior__(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___irshift__(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).__irshift__(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___irshift__tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).__irshift__(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___ixor__(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).__ixor__(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___ixor__tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).__ixor__(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___lshift__(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::__lshift__(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___lshift__scalar_out_(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::__lshift___out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___lshift__tensor_(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::__lshift__(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___lshift__tensor_out_(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::__lshift___out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___or__(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::__or__(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___or__tensor_(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::__or__(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___rshift__(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::__rshift__(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___rshift__scalar_out_(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::__rshift___out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___rshift__tensor_(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::__rshift__(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___rshift__tensor_out_(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::__rshift___out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___xor__(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::__xor__(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg___xor__tensor_(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::__xor__(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__adaptive_avg_pool2d(gc_tensor self, int64_t *output_size_data, int output_size_len) {
  PROTECT(
    torch::Tensor results__ = torch::_adaptive_avg_pool2d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__adaptive_avg_pool2d_backward(gc_tensor grad_output, gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_adaptive_avg_pool2d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__adaptive_avg_pool2d_backward_out(gc_tensor out, gc_tensor grad_output, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_adaptive_avg_pool2d_backward_out(out_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__adaptive_avg_pool2d_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_adaptive_avg_pool2d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__adaptive_avg_pool3d(gc_tensor self, int64_t *output_size_data, int output_size_len) {
  PROTECT(
    torch::Tensor results__ = torch::_adaptive_avg_pool3d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__adaptive_avg_pool3d_backward(gc_tensor grad_output, gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_adaptive_avg_pool3d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__adaptive_avg_pool3d_backward_out(gc_tensor out, gc_tensor grad_output, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_adaptive_avg_pool3d_backward_out(out_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__adaptive_avg_pool3d_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_adaptive_avg_pool3d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__add_batch_dim(gc_tensor self, int64_t batch_dim, int64_t level) {
  PROTECT(
    torch::Tensor results__ = torch::_add_batch_dim(tensor_from_ocaml(self), batch_dim, level);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__add_relu(gc_tensor self, gc_tensor other, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::_add_relu(tensor_from_ocaml(self), tensor_from_ocaml(other), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__add_relu_(gc_tensor self, gc_tensor other, scalar alpha) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::_add_relu_(self_local, tensor_from_ocaml(other), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__add_relu_out(gc_tensor out, gc_tensor self, gc_tensor other, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_add_relu_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__add_relu_scalar(gc_tensor self, scalar other, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::_add_relu(tensor_from_ocaml(self), *other, alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__add_relu_scalar_(gc_tensor self, scalar other, scalar alpha) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::_add_relu_(self_local, *other, alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__add_relu_scalar_out(gc_tensor out, gc_tensor self, scalar other, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_add_relu_out(out_local, tensor_from_ocaml(self), *other, alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__addmm_activation(gc_tensor self, gc_tensor mat1, gc_tensor mat2, scalar beta, scalar alpha, int use_gelu) {
  PROTECT(
    torch::Tensor results__ = torch::_addmm_activation(tensor_from_ocaml(self), tensor_from_ocaml(mat1), tensor_from_ocaml(mat2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} , (bool)use_gelu);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__addmm_activation_out(gc_tensor out, gc_tensor self, gc_tensor mat1, gc_tensor mat2, scalar beta, scalar alpha, int use_gelu) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_addmm_activation_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mat1), tensor_from_ocaml(mat2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} , (bool)use_gelu);
    return tensor_to_ocaml(results__);
  )
}

void atg__aminmax(raw_tensor *out__, gc_tensor self) {
  PROTECT(
    auto results__ = torch::_aminmax(tensor_from_ocaml(self));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__aminmax_dim(raw_tensor *out__, gc_tensor self, int64_t dim, int keepdim) {
  PROTECT(
    auto results__ = torch::_aminmax(tensor_from_ocaml(self), dim, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__aminmax_dim_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor self, int64_t dim, int keepdim) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::_aminmax_out(out0_local, out1_local, tensor_from_ocaml(self), dim, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__aminmax_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor self) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::_aminmax_out(out0_local, out1_local, tensor_from_ocaml(self));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__amp_update_scale(raw_tensor *out__, gc_tensor self, gc_tensor growth_tracker, gc_tensor found_inf, double scale_growth_factor, double scale_backoff_factor, int64_t growth_interval) {
  PROTECT(
    auto results__ = torch::_amp_update_scale(tensor_from_ocaml(self), tensor_from_ocaml(growth_tracker), tensor_from_ocaml(found_inf), scale_growth_factor, scale_backoff_factor, growth_interval);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg__amp_update_scale_(gc_tensor self, gc_tensor growth_tracker, gc_tensor found_inf, double scale_growth_factor, double scale_backoff_factor, int64_t growth_interval) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  torch::Tensor growth_tracker_local = tensor_from_ocaml(growth_tracker);
  PROTECT(
    torch::Tensor results__ = torch::_amp_update_scale_(self_local, growth_tracker_local, tensor_from_ocaml(found_inf), scale_growth_factor, scale_backoff_factor, growth_interval);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__amp_update_scale_out(gc_tensor out, gc_tensor self, gc_tensor growth_tracker, gc_tensor found_inf, double scale_growth_factor, double scale_backoff_factor, int64_t growth_interval) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  torch::Tensor growth_tracker_local = tensor_from_ocaml(growth_tracker);
  PROTECT(
    torch::Tensor results__ = torch::_amp_update_scale_out(out_local, tensor_from_ocaml(self), growth_tracker_local, tensor_from_ocaml(found_inf), scale_growth_factor, scale_backoff_factor, growth_interval);
    return tensor_to_ocaml(results__);
  )
}

void atg__assert_scalar(scalar self, char * assert_msg) {
  PROTECT(
    torch::_assert_scalar(*self, std::string(assert_msg));
  )
}

void atg__assert_tensor_metadata(gc_tensor a, int64_t *size_data, int size_len, int64_t *stride_data, int stride_len, int dtype, int device) {
  PROTECT(
    torch::_assert_tensor_metadata(tensor_from_ocaml(a), size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(size_data, size_len)), stride_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(stride_data, stride_len)), torch::ScalarType(dtype), optional_device_of_int(device));
  )
}

raw_tensor atg__autocast_to_full_precision(gc_tensor self, int cuda_enabled, int cpu_enabled) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self)._autocast_to_full_precision((bool)cuda_enabled, (bool)cpu_enabled);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__autocast_to_reduced_precision(gc_tensor self, int cuda_enabled, int cpu_enabled, int cuda_dtype, int cpu_dtype) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self)._autocast_to_reduced_precision((bool)cuda_enabled, (bool)cpu_enabled, torch::ScalarType(cuda_dtype), torch::ScalarType(cpu_dtype));
    return tensor_to_ocaml(results__);
  )
}

void atg__batch_norm_no_update(raw_tensor *out__, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, double momentum, double eps) {
  PROTECT(
    auto results__ = torch::_batch_norm_no_update(tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, momentum, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

void atg__batch_norm_no_update_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor out3, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, double momentum, double eps) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  torch::Tensor out3_local = tensor_from_ocaml(out3);
  PROTECT(
    auto results__ = torch::_batch_norm_no_update_out(out0_local, out1_local, out2_local, out3_local, tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, momentum, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

void atg__batch_norm_with_update(raw_tensor *out__, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, double momentum, double eps) {
  torch::Tensor running_mean_local = tensor_from_ocaml(running_mean);
  torch::Tensor running_var_local = tensor_from_ocaml(running_var);
  PROTECT(
    auto results__ = torch::_batch_norm_with_update(tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, running_mean_local, running_var_local, momentum, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

void atg__batch_norm_with_update_functional(raw_tensor *out__, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, double momentum, double eps) {
  PROTECT(
    auto results__ = torch::_batch_norm_with_update_functional(tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, tensor_from_ocaml(running_mean), tensor_from_ocaml(running_var), momentum, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
    out__[4] = tensor_to_ocaml(std::get<4>(results__));
    out__[5] = tensor_to_ocaml(std::get<5>(results__));
  )
}

void atg__batch_norm_with_update_out(raw_tensor *out__, gc_tensor out, gc_tensor save_mean, gc_tensor save_invstd, gc_tensor reserve, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, double momentum, double eps) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  torch::Tensor save_mean_local = tensor_from_ocaml(save_mean);
  torch::Tensor save_invstd_local = tensor_from_ocaml(save_invstd);
  torch::Tensor reserve_local = tensor_from_ocaml(reserve);
  torch::Tensor running_mean_local = tensor_from_ocaml(running_mean);
  torch::Tensor running_var_local = tensor_from_ocaml(running_var);
  PROTECT(
    auto results__ = torch::_batch_norm_with_update_out(out_local, save_mean_local, save_invstd_local, reserve_local, tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, running_mean_local, running_var_local, momentum, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

raw_tensor atg__cast_byte(gc_tensor self, int non_blocking) {
  PROTECT(
    torch::Tensor results__ = torch::_cast_Byte(tensor_from_ocaml(self), (bool)non_blocking);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__cast_char(gc_tensor self, int non_blocking) {
  PROTECT(
    torch::Tensor results__ = torch::_cast_Char(tensor_from_ocaml(self), (bool)non_blocking);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__cast_double(gc_tensor self, int non_blocking) {
  PROTECT(
    torch::Tensor results__ = torch::_cast_Double(tensor_from_ocaml(self), (bool)non_blocking);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__cast_float(gc_tensor self, int non_blocking) {
  PROTECT(
    torch::Tensor results__ = torch::_cast_Float(tensor_from_ocaml(self), (bool)non_blocking);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__cast_half(gc_tensor self, int non_blocking) {
  PROTECT(
    torch::Tensor results__ = torch::_cast_Half(tensor_from_ocaml(self), (bool)non_blocking);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__cast_int(gc_tensor self, int non_blocking) {
  PROTECT(
    torch::Tensor results__ = torch::_cast_Int(tensor_from_ocaml(self), (bool)non_blocking);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__cast_long(gc_tensor self, int non_blocking) {
  PROTECT(
    torch::Tensor results__ = torch::_cast_Long(tensor_from_ocaml(self), (bool)non_blocking);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__cast_short(gc_tensor self, int non_blocking) {
  PROTECT(
    torch::Tensor results__ = torch::_cast_Short(tensor_from_ocaml(self), (bool)non_blocking);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__cdist_backward(gc_tensor grad, gc_tensor x1, gc_tensor x2, double p, gc_tensor cdist) {
  PROTECT(
    torch::Tensor results__ = torch::_cdist_backward(tensor_from_ocaml(grad), tensor_from_ocaml(x1), tensor_from_ocaml(x2), p, tensor_from_ocaml(cdist));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__cdist_backward_out(gc_tensor out, gc_tensor grad, gc_tensor x1, gc_tensor x2, double p, gc_tensor cdist) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_cdist_backward_out(out_local, tensor_from_ocaml(grad), tensor_from_ocaml(x1), tensor_from_ocaml(x2), p, tensor_from_ocaml(cdist));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__cholesky_solve_helper(gc_tensor self, gc_tensor A, int upper) {
  PROTECT(
    torch::Tensor results__ = torch::_cholesky_solve_helper(tensor_from_ocaml(self), tensor_from_ocaml(A), (bool)upper);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__cholesky_solve_helper_out(gc_tensor out, gc_tensor self, gc_tensor A, int upper) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_cholesky_solve_helper_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(A), (bool)upper);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__chunk_cat(gc_tensor *tensors_data, int tensors_len, int64_t dim, int64_t num_chunks) {
  PROTECT(
    torch::Tensor results__ = torch::_chunk_cat(of_carray_tensor(tensors_data, tensors_len), dim, num_chunks);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__chunk_cat_out(gc_tensor out, gc_tensor *tensors_data, int tensors_len, int64_t dim, int64_t num_chunks) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_chunk_cat_out(out_local, of_carray_tensor(tensors_data, tensors_len), dim, num_chunks);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__coalesce(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_coalesce(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__coalesce_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_coalesce_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__coalesced(gc_tensor self, int coalesced) {
  PROTECT(
    torch::Tensor results__ = torch::_coalesced(tensor_from_ocaml(self), (bool)coalesced);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__coalesced_(gc_tensor self, int coalesced) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self)._coalesced_((bool)coalesced);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__coalesced_out(gc_tensor out, gc_tensor self, int coalesced) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_coalesced_out(out_local, tensor_from_ocaml(self), (bool)coalesced);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__compute_linear_combination(gc_tensor input, gc_tensor coefficients) {
  PROTECT(
    torch::Tensor results__ = torch::_compute_linear_combination(tensor_from_ocaml(input), tensor_from_ocaml(coefficients));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__compute_linear_combination_out(gc_tensor out, gc_tensor input, gc_tensor coefficients) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_compute_linear_combination_out(out_local, tensor_from_ocaml(input), tensor_from_ocaml(coefficients));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__conj(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_conj(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__conj_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_conj_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__conj_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_conj_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__conj_physical(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_conj_physical(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__conj_physical_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_conj_physical_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__conv_depthwise2d(gc_tensor self, gc_tensor weight, int64_t *kernel_size_data, int kernel_size_len, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len) {
  PROTECT(
    torch::Tensor results__ = torch::_conv_depthwise2d(tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(kernel_size_data, kernel_size_len), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__conv_depthwise2d_out(gc_tensor out, gc_tensor self, gc_tensor weight, int64_t *kernel_size_data, int kernel_size_len, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_conv_depthwise2d_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(kernel_size_data, kernel_size_len), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__convert_indices_from_coo_to_csr(gc_tensor self, int64_t size, int out_int32) {
  PROTECT(
    torch::Tensor results__ = torch::_convert_indices_from_coo_to_csr(tensor_from_ocaml(self), size, (bool)out_int32);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__convert_indices_from_coo_to_csr_out(gc_tensor out, gc_tensor self, int64_t size, int out_int32) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_convert_indices_from_coo_to_csr_out(out_local, tensor_from_ocaml(self), size, (bool)out_int32);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__convert_indices_from_csr_to_coo(gc_tensor crow_indices, gc_tensor col_indices, int out_int32, int transpose) {
  PROTECT(
    torch::Tensor results__ = torch::_convert_indices_from_csr_to_coo(tensor_from_ocaml(crow_indices), tensor_from_ocaml(col_indices), (bool)out_int32, (bool)transpose);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__convert_indices_from_csr_to_coo_out(gc_tensor out, gc_tensor crow_indices, gc_tensor col_indices, int out_int32, int transpose) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_convert_indices_from_csr_to_coo_out(out_local, tensor_from_ocaml(crow_indices), tensor_from_ocaml(col_indices), (bool)out_int32, (bool)transpose);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__convert_weight_to_int4pack(gc_tensor self, int64_t innerKTiles) {
  PROTECT(
    torch::Tensor results__ = torch::_convert_weight_to_int4pack(tensor_from_ocaml(self), innerKTiles);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__convert_weight_to_int4pack_for_cpu(gc_tensor self, int64_t innerKTiles) {
  PROTECT(
    torch::Tensor results__ = torch::_convert_weight_to_int4pack_for_cpu(tensor_from_ocaml(self), innerKTiles);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__convolution(gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int transposed, int64_t *output_padding_data, int output_padding_len, int64_t groups, int benchmark, int deterministic, int cudnn_enabled, int allow_tf32) {
  PROTECT(
    torch::Tensor results__ = torch::_convolution(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)transposed, torch::IntArrayRef(output_padding_data, output_padding_len), groups, (bool)benchmark, (bool)deterministic, (bool)cudnn_enabled, (bool)allow_tf32);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__convolution_deprecated(gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int transposed, int64_t *output_padding_data, int output_padding_len, int64_t groups, int benchmark, int deterministic, int cudnn_enabled) {
  PROTECT(
    torch::Tensor results__ = torch::_convolution(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)transposed, torch::IntArrayRef(output_padding_data, output_padding_len), groups, (bool)benchmark, (bool)deterministic, (bool)cudnn_enabled);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__convolution_mode(gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, char * padding, int64_t *dilation_data, int dilation_len, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::_convolution_mode(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), std::string(padding), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__convolution_out(gc_tensor out, gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int transposed, int64_t *output_padding_data, int output_padding_len, int64_t groups, int benchmark, int deterministic, int cudnn_enabled, int allow_tf32) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_convolution_out(out_local, tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)transposed, torch::IntArrayRef(output_padding_data, output_padding_len), groups, (bool)benchmark, (bool)deterministic, (bool)cudnn_enabled, (bool)allow_tf32);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__copy_from(gc_tensor self, gc_tensor dst, int non_blocking) {
  PROTECT(
    torch::Tensor results__ = torch::_copy_from(tensor_from_ocaml(self), tensor_from_ocaml(dst), (bool)non_blocking);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__copy_from_and_resize(gc_tensor self, gc_tensor dst) {
  PROTECT(
    torch::Tensor results__ = torch::_copy_from_and_resize(tensor_from_ocaml(self), tensor_from_ocaml(dst));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__copy_from_and_resize_out(gc_tensor out, gc_tensor self, gc_tensor dst) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_copy_from_and_resize_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(dst));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__copy_from_out(gc_tensor out, gc_tensor self, gc_tensor dst, int non_blocking) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_copy_from_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(dst), (bool)non_blocking);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__cslt_compress(gc_tensor input) {
  PROTECT(
    torch::Tensor results__ = torch::_cslt_compress(tensor_from_ocaml(input));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__cslt_sparse_mm(gc_tensor compressed_A, gc_tensor dense_B, gc_tensor bias, gc_tensor alpha, int out_dtype, int transpose_result, int64_t alg_id, int64_t split_k, int split_k_one_kernel) {
  PROTECT(
    torch::Tensor results__ = torch::_cslt_sparse_mm(tensor_from_ocaml(compressed_A), tensor_from_ocaml(dense_B), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, alpha ? std::make_optional(tensor_from_ocaml(alpha)) : std::nullopt, torch::ScalarType(out_dtype), (bool)transpose_result, alg_id, split_k, (bool)split_k_one_kernel);
    return tensor_to_ocaml(results__);
  )
}

int64_t atg__cslt_sparse_mm_search(gc_tensor compressed_A, gc_tensor dense_B, gc_tensor bias, gc_tensor alpha, int out_dtype, int transpose_result) {
  PROTECT(
    return torch::_cslt_sparse_mm_search(tensor_from_ocaml(compressed_A), tensor_from_ocaml(dense_B), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, alpha ? std::make_optional(tensor_from_ocaml(alpha)) : std::nullopt, torch::ScalarType(out_dtype), (bool)transpose_result);
  )
  return 0;
}

void atg__ctc_loss(raw_tensor *out__, gc_tensor log_probs, gc_tensor targets, int64_t *input_lengths_data, int input_lengths_len, int64_t *target_lengths_data, int target_lengths_len, int64_t blank, int zero_infinity) {
  PROTECT(
    auto results__ = torch::_ctc_loss(tensor_from_ocaml(log_probs), tensor_from_ocaml(targets), torch::IntArrayRef(input_lengths_data, input_lengths_len), torch::IntArrayRef(target_lengths_data, target_lengths_len), blank, (bool)zero_infinity);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg__ctc_loss_backward(gc_tensor grad, gc_tensor log_probs, gc_tensor targets, int64_t *input_lengths_data, int input_lengths_len, int64_t *target_lengths_data, int target_lengths_len, gc_tensor neg_log_likelihood, gc_tensor log_alpha, int64_t blank, int zero_infinity) {
  PROTECT(
    torch::Tensor results__ = torch::_ctc_loss_backward(tensor_from_ocaml(grad), tensor_from_ocaml(log_probs), tensor_from_ocaml(targets), torch::IntArrayRef(input_lengths_data, input_lengths_len), torch::IntArrayRef(target_lengths_data, target_lengths_len), tensor_from_ocaml(neg_log_likelihood), tensor_from_ocaml(log_alpha), blank, (bool)zero_infinity);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__ctc_loss_backward_out(gc_tensor out, gc_tensor grad, gc_tensor log_probs, gc_tensor targets, int64_t *input_lengths_data, int input_lengths_len, int64_t *target_lengths_data, int target_lengths_len, gc_tensor neg_log_likelihood, gc_tensor log_alpha, int64_t blank, int zero_infinity) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_ctc_loss_backward_out(out_local, tensor_from_ocaml(grad), tensor_from_ocaml(log_probs), tensor_from_ocaml(targets), torch::IntArrayRef(input_lengths_data, input_lengths_len), torch::IntArrayRef(target_lengths_data, target_lengths_len), tensor_from_ocaml(neg_log_likelihood), tensor_from_ocaml(log_alpha), blank, (bool)zero_infinity);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__ctc_loss_backward_tensor(gc_tensor grad, gc_tensor log_probs, gc_tensor targets, gc_tensor input_lengths, gc_tensor target_lengths, gc_tensor neg_log_likelihood, gc_tensor log_alpha, int64_t blank, int zero_infinity) {
  PROTECT(
    torch::Tensor results__ = torch::_ctc_loss_backward(tensor_from_ocaml(grad), tensor_from_ocaml(log_probs), tensor_from_ocaml(targets), tensor_from_ocaml(input_lengths), tensor_from_ocaml(target_lengths), tensor_from_ocaml(neg_log_likelihood), tensor_from_ocaml(log_alpha), blank, (bool)zero_infinity);
    return tensor_to_ocaml(results__);
  )
}

void atg__ctc_loss_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor log_probs, gc_tensor targets, int64_t *input_lengths_data, int input_lengths_len, int64_t *target_lengths_data, int target_lengths_len, int64_t blank, int zero_infinity) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::_ctc_loss_out(out0_local, out1_local, tensor_from_ocaml(log_probs), tensor_from_ocaml(targets), torch::IntArrayRef(input_lengths_data, input_lengths_len), torch::IntArrayRef(target_lengths_data, target_lengths_len), blank, (bool)zero_infinity);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__ctc_loss_tensor(raw_tensor *out__, gc_tensor log_probs, gc_tensor targets, gc_tensor input_lengths, gc_tensor target_lengths, int64_t blank, int zero_infinity) {
  PROTECT(
    auto results__ = torch::_ctc_loss(tensor_from_ocaml(log_probs), tensor_from_ocaml(targets), tensor_from_ocaml(input_lengths), tensor_from_ocaml(target_lengths), blank, (bool)zero_infinity);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__ctc_loss_tensor_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor log_probs, gc_tensor targets, gc_tensor input_lengths, gc_tensor target_lengths, int64_t blank, int zero_infinity) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::_ctc_loss_out(out0_local, out1_local, tensor_from_ocaml(log_probs), tensor_from_ocaml(targets), tensor_from_ocaml(input_lengths), tensor_from_ocaml(target_lengths), blank, (bool)zero_infinity);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__cudnn_ctc_loss(raw_tensor *out__, gc_tensor log_probs, gc_tensor targets, int64_t *input_lengths_data, int input_lengths_len, int64_t *target_lengths_data, int target_lengths_len, int64_t blank, int deterministic, int zero_infinity) {
  PROTECT(
    auto results__ = torch::_cudnn_ctc_loss(tensor_from_ocaml(log_probs), tensor_from_ocaml(targets), torch::IntArrayRef(input_lengths_data, input_lengths_len), torch::IntArrayRef(target_lengths_data, target_lengths_len), blank, (bool)deterministic, (bool)zero_infinity);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__cudnn_ctc_loss_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor log_probs, gc_tensor targets, int64_t *input_lengths_data, int input_lengths_len, int64_t *target_lengths_data, int target_lengths_len, int64_t blank, int deterministic, int zero_infinity) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::_cudnn_ctc_loss_out(out0_local, out1_local, tensor_from_ocaml(log_probs), tensor_from_ocaml(targets), torch::IntArrayRef(input_lengths_data, input_lengths_len), torch::IntArrayRef(target_lengths_data, target_lengths_len), blank, (bool)deterministic, (bool)zero_infinity);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__cudnn_ctc_loss_tensor(raw_tensor *out__, gc_tensor log_probs, gc_tensor targets, gc_tensor input_lengths, gc_tensor target_lengths, int64_t blank, int deterministic, int zero_infinity) {
  PROTECT(
    auto results__ = torch::_cudnn_ctc_loss(tensor_from_ocaml(log_probs), tensor_from_ocaml(targets), tensor_from_ocaml(input_lengths), tensor_from_ocaml(target_lengths), blank, (bool)deterministic, (bool)zero_infinity);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg__cudnn_init_dropout_state(double dropout, int train, int64_t dropout_seed, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::_cudnn_init_dropout_state(dropout, (bool)train, dropout_seed, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__cudnn_init_dropout_state_out(gc_tensor out, double dropout, int train, int64_t dropout_seed) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_cudnn_init_dropout_state_out(out_local, dropout, (bool)train, dropout_seed);
    return tensor_to_ocaml(results__);
  )
}

void atg__cudnn_rnn(raw_tensor *out__, gc_tensor input, gc_tensor *weight_data, int weight_len, int64_t weight_stride0, gc_tensor weight_buf, gc_tensor hx, gc_tensor cx, int64_t mode, int64_t hidden_size, int64_t proj_size, int64_t num_layers, int batch_first, double dropout, int train, int bidirectional, int64_t *batch_sizes_data, int batch_sizes_len, gc_tensor dropout_state) {
  PROTECT(
    auto results__ = torch::_cudnn_rnn(tensor_from_ocaml(input), of_carray_tensor(weight_data, weight_len), weight_stride0, weight_buf ? std::make_optional(tensor_from_ocaml(weight_buf)) : std::nullopt, tensor_from_ocaml(hx), cx ? std::make_optional(tensor_from_ocaml(cx)) : std::nullopt, mode, hidden_size, proj_size, num_layers, (bool)batch_first, dropout, (bool)train, (bool)bidirectional, torch::IntArrayRef(batch_sizes_data, batch_sizes_len), dropout_state ? std::make_optional(tensor_from_ocaml(dropout_state)) : std::nullopt);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
    out__[4] = tensor_to_ocaml(std::get<4>(results__));
  )
}

raw_tensor atg__cudnn_rnn_flatten_weight(gc_tensor *weight_arr_data, int weight_arr_len, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t proj_size, int64_t num_layers, int batch_first, int bidirectional) {
  PROTECT(
    torch::Tensor results__ = torch::_cudnn_rnn_flatten_weight(of_carray_tensor(weight_arr_data, weight_arr_len), weight_stride0, input_size, mode, hidden_size, proj_size, num_layers, (bool)batch_first, (bool)bidirectional);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__cudnn_rnn_flatten_weight_out(gc_tensor out, gc_tensor *weight_arr_data, int weight_arr_len, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t proj_size, int64_t num_layers, int batch_first, int bidirectional) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_cudnn_rnn_flatten_weight_out(out_local, of_carray_tensor(weight_arr_data, weight_arr_len), weight_stride0, input_size, mode, hidden_size, proj_size, num_layers, (bool)batch_first, (bool)bidirectional);
    return tensor_to_ocaml(results__);
  )
}

void atg__cudnn_rnn_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor out3, gc_tensor out4, gc_tensor input, gc_tensor *weight_data, int weight_len, int64_t weight_stride0, gc_tensor weight_buf, gc_tensor hx, gc_tensor cx, int64_t mode, int64_t hidden_size, int64_t proj_size, int64_t num_layers, int batch_first, double dropout, int train, int bidirectional, int64_t *batch_sizes_data, int batch_sizes_len, gc_tensor dropout_state) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  torch::Tensor out3_local = tensor_from_ocaml(out3);
  torch::Tensor out4_local = tensor_from_ocaml(out4);
  PROTECT(
    auto results__ = torch::_cudnn_rnn_out(out0_local, out1_local, out2_local, out3_local, out4_local, tensor_from_ocaml(input), of_carray_tensor(weight_data, weight_len), weight_stride0, weight_buf ? std::make_optional(tensor_from_ocaml(weight_buf)) : std::nullopt, tensor_from_ocaml(hx), cx ? std::make_optional(tensor_from_ocaml(cx)) : std::nullopt, mode, hidden_size, proj_size, num_layers, (bool)batch_first, dropout, (bool)train, (bool)bidirectional, torch::IntArrayRef(batch_sizes_data, batch_sizes_len), dropout_state ? std::make_optional(tensor_from_ocaml(dropout_state)) : std::nullopt);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
    out__[4] = tensor_to_ocaml(std::get<4>(results__));
  )
}

int64_t atg__debug_has_internal_overlap(gc_tensor self) {
  PROTECT(
    return torch::_debug_has_internal_overlap(tensor_from_ocaml(self));
  )
  return 0;
}

raw_tensor atg__dim_arange(gc_tensor like, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::_dim_arange(tensor_from_ocaml(like), dim);
    return tensor_to_ocaml(results__);
  )
}

int64_t atg__dimi(gc_tensor self) {
  PROTECT(
    return tensor_from_ocaml(self)._dimI();
  )
  return 0;
}

int64_t atg__dimv(gc_tensor self) {
  PROTECT(
    return tensor_from_ocaml(self)._dimV();
  )
  return 0;
}

raw_tensor atg__dirichlet_grad(gc_tensor x, gc_tensor alpha, gc_tensor total) {
  PROTECT(
    torch::Tensor results__ = torch::_dirichlet_grad(tensor_from_ocaml(x), tensor_from_ocaml(alpha), tensor_from_ocaml(total));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__dirichlet_grad_out(gc_tensor out, gc_tensor x, gc_tensor alpha, gc_tensor total) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_dirichlet_grad_out(out_local, tensor_from_ocaml(x), tensor_from_ocaml(alpha), tensor_from_ocaml(total));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__dyn_quant_matmul_4bit(gc_tensor inp, gc_tensor packed_weights, int64_t block_size, int64_t in_features, int64_t out_features) {
  PROTECT(
    torch::Tensor results__ = torch::_dyn_quant_matmul_4bit(tensor_from_ocaml(inp), tensor_from_ocaml(packed_weights), block_size, in_features, out_features);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__dyn_quant_pack_4bit_weight(gc_tensor weights, gc_tensor scales_zeros, gc_tensor bias, int64_t block_size, int64_t in_features, int64_t out_features) {
  PROTECT(
    torch::Tensor results__ = torch::_dyn_quant_pack_4bit_weight(tensor_from_ocaml(weights), tensor_from_ocaml(scales_zeros), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, block_size, in_features, out_features);
    return tensor_to_ocaml(results__);
  )
}

void atg__efficient_attention_backward(raw_tensor *out__, gc_tensor grad_out_, gc_tensor query, gc_tensor key, gc_tensor value, gc_tensor bias, gc_tensor out, gc_tensor cu_seqlens_q, gc_tensor cu_seqlens_k, int64_t max_seqlen_q, int64_t max_seqlen_k, gc_tensor logsumexp, double dropout_p, gc_tensor philox_seed, gc_tensor philox_offset, int64_t custom_mask_type, int bias_requires_grad, double scale_v, int scale_null, int64_t num_splits_key_v, int num_splits_key_null, int64_t window_size_v, int window_size_null, int shared_storage_dqdkdv) {
  PROTECT(
    auto results__ = torch::_efficient_attention_backward(tensor_from_ocaml(grad_out_), tensor_from_ocaml(query), tensor_from_ocaml(key), tensor_from_ocaml(value), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, tensor_from_ocaml(out), cu_seqlens_q ? std::make_optional(tensor_from_ocaml(cu_seqlens_q)) : std::nullopt, cu_seqlens_k ? std::make_optional(tensor_from_ocaml(cu_seqlens_k)) : std::nullopt, max_seqlen_q, max_seqlen_k, tensor_from_ocaml(logsumexp), dropout_p, tensor_from_ocaml(philox_seed), tensor_from_ocaml(philox_offset), custom_mask_type, (bool)bias_requires_grad, scale_null ? c10::nullopt : c10::optional<double>(scale_v), num_splits_key_null ? c10::nullopt : c10::optional<int64_t>(num_splits_key_v), window_size_null ? c10::nullopt : c10::optional<int64_t>(window_size_v), (bool)shared_storage_dqdkdv);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

raw_tensor atg__efficientzerotensor(int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::_efficientzerotensor(torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__efficientzerotensor_out(gc_tensor out, int64_t *size_data, int size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_efficientzerotensor_out(out_local, torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

void atg__embedding_bag(raw_tensor *out__, gc_tensor weight, gc_tensor indices, gc_tensor offsets, int scale_grad_by_freq, int64_t mode, int sparse, gc_tensor per_sample_weights, int include_last_offset, int64_t padding_idx) {
  PROTECT(
    auto results__ = torch::_embedding_bag(tensor_from_ocaml(weight), tensor_from_ocaml(indices), tensor_from_ocaml(offsets), (bool)scale_grad_by_freq, mode, (bool)sparse, per_sample_weights ? std::make_optional(tensor_from_ocaml(per_sample_weights)) : std::nullopt, (bool)include_last_offset, padding_idx);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

raw_tensor atg__embedding_bag_backward(gc_tensor grad, gc_tensor indices, gc_tensor offsets, gc_tensor offset2bag, gc_tensor bag_size, gc_tensor maximum_indices, int64_t num_weights, int scale_grad_by_freq, int64_t mode, int sparse, gc_tensor per_sample_weights, int64_t padding_idx) {
  PROTECT(
    torch::Tensor results__ = torch::_embedding_bag_backward(tensor_from_ocaml(grad), tensor_from_ocaml(indices), tensor_from_ocaml(offsets), tensor_from_ocaml(offset2bag), tensor_from_ocaml(bag_size), tensor_from_ocaml(maximum_indices), num_weights, (bool)scale_grad_by_freq, mode, (bool)sparse, per_sample_weights ? std::make_optional(tensor_from_ocaml(per_sample_weights)) : std::nullopt, padding_idx);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__embedding_bag_dense_backward(gc_tensor grad, gc_tensor indices, gc_tensor offset2bag, gc_tensor bag_size, gc_tensor maximum_indices, int64_t num_weights, int scale_grad_by_freq, int64_t mode, gc_tensor per_sample_weights, int64_t padding_idx) {
  PROTECT(
    torch::Tensor results__ = torch::_embedding_bag_dense_backward(tensor_from_ocaml(grad), tensor_from_ocaml(indices), tensor_from_ocaml(offset2bag), tensor_from_ocaml(bag_size), tensor_from_ocaml(maximum_indices), num_weights, (bool)scale_grad_by_freq, mode, per_sample_weights ? std::make_optional(tensor_from_ocaml(per_sample_weights)) : std::nullopt, padding_idx);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__embedding_bag_dense_backward_out(gc_tensor out, gc_tensor grad, gc_tensor indices, gc_tensor offset2bag, gc_tensor bag_size, gc_tensor maximum_indices, int64_t num_weights, int scale_grad_by_freq, int64_t mode, gc_tensor per_sample_weights, int64_t padding_idx) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_embedding_bag_dense_backward_out(out_local, tensor_from_ocaml(grad), tensor_from_ocaml(indices), tensor_from_ocaml(offset2bag), tensor_from_ocaml(bag_size), tensor_from_ocaml(maximum_indices), num_weights, (bool)scale_grad_by_freq, mode, per_sample_weights ? std::make_optional(tensor_from_ocaml(per_sample_weights)) : std::nullopt, padding_idx);
    return tensor_to_ocaml(results__);
  )
}

void atg__embedding_bag_forward_only(raw_tensor *out__, gc_tensor weight, gc_tensor indices, gc_tensor offsets, int scale_grad_by_freq, int64_t mode, int sparse, gc_tensor per_sample_weights, int include_last_offset, int64_t padding_idx) {
  PROTECT(
    auto results__ = torch::_embedding_bag_forward_only(tensor_from_ocaml(weight), tensor_from_ocaml(indices), tensor_from_ocaml(offsets), (bool)scale_grad_by_freq, mode, (bool)sparse, per_sample_weights ? std::make_optional(tensor_from_ocaml(per_sample_weights)) : std::nullopt, (bool)include_last_offset, padding_idx);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

void atg__embedding_bag_forward_only_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor out3, gc_tensor weight, gc_tensor indices, gc_tensor offsets, int scale_grad_by_freq, int64_t mode, int sparse, gc_tensor per_sample_weights, int include_last_offset, int64_t padding_idx) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  torch::Tensor out3_local = tensor_from_ocaml(out3);
  PROTECT(
    auto results__ = torch::_embedding_bag_forward_only_out(out0_local, out1_local, out2_local, out3_local, tensor_from_ocaml(weight), tensor_from_ocaml(indices), tensor_from_ocaml(offsets), (bool)scale_grad_by_freq, mode, (bool)sparse, per_sample_weights ? std::make_optional(tensor_from_ocaml(per_sample_weights)) : std::nullopt, (bool)include_last_offset, padding_idx);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

void atg__embedding_bag_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor out3, gc_tensor weight, gc_tensor indices, gc_tensor offsets, int scale_grad_by_freq, int64_t mode, int sparse, gc_tensor per_sample_weights, int include_last_offset, int64_t padding_idx) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  torch::Tensor out3_local = tensor_from_ocaml(out3);
  PROTECT(
    auto results__ = torch::_embedding_bag_out(out0_local, out1_local, out2_local, out3_local, tensor_from_ocaml(weight), tensor_from_ocaml(indices), tensor_from_ocaml(offsets), (bool)scale_grad_by_freq, mode, (bool)sparse, per_sample_weights ? std::make_optional(tensor_from_ocaml(per_sample_weights)) : std::nullopt, (bool)include_last_offset, padding_idx);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

raw_tensor atg__embedding_bag_per_sample_weights_backward(gc_tensor grad, gc_tensor weight, gc_tensor indices, gc_tensor offsets, gc_tensor offset2bag, int64_t mode, int64_t padding_idx) {
  PROTECT(
    torch::Tensor results__ = torch::_embedding_bag_per_sample_weights_backward(tensor_from_ocaml(grad), tensor_from_ocaml(weight), tensor_from_ocaml(indices), tensor_from_ocaml(offsets), tensor_from_ocaml(offset2bag), mode, padding_idx);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__embedding_bag_per_sample_weights_backward_out(gc_tensor out, gc_tensor grad, gc_tensor weight, gc_tensor indices, gc_tensor offsets, gc_tensor offset2bag, int64_t mode, int64_t padding_idx) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_embedding_bag_per_sample_weights_backward_out(out_local, tensor_from_ocaml(grad), tensor_from_ocaml(weight), tensor_from_ocaml(indices), tensor_from_ocaml(offsets), tensor_from_ocaml(offset2bag), mode, padding_idx);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__embedding_bag_sparse_backward(gc_tensor grad, gc_tensor indices, gc_tensor offsets, gc_tensor offset2bag, gc_tensor bag_size, int64_t num_weights, int scale_grad_by_freq, int64_t mode, gc_tensor per_sample_weights, int64_t padding_idx) {
  PROTECT(
    torch::Tensor results__ = torch::_embedding_bag_sparse_backward(tensor_from_ocaml(grad), tensor_from_ocaml(indices), tensor_from_ocaml(offsets), tensor_from_ocaml(offset2bag), tensor_from_ocaml(bag_size), num_weights, (bool)scale_grad_by_freq, mode, per_sample_weights ? std::make_optional(tensor_from_ocaml(per_sample_weights)) : std::nullopt, padding_idx);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__empty_affine_quantized(int64_t *size_data, int size_len, int options_kind, int options_device, double scale, int64_t zero_point) {
  PROTECT(
    torch::Tensor results__ = torch::_empty_affine_quantized(torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)), scale, zero_point);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__empty_affine_quantized_out(gc_tensor out, int64_t *size_data, int size_len, double scale, int64_t zero_point) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_empty_affine_quantized_out(out_local, torch::IntArrayRef(size_data, size_len), scale, zero_point);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__empty_per_channel_affine_quantized(int64_t *size_data, int size_len, gc_tensor scales, gc_tensor zero_points, int64_t axis, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::_empty_per_channel_affine_quantized(torch::IntArrayRef(size_data, size_len), tensor_from_ocaml(scales), tensor_from_ocaml(zero_points), axis, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__empty_per_channel_affine_quantized_out(gc_tensor out, int64_t *size_data, int size_len, gc_tensor scales, gc_tensor zero_points, int64_t axis) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_empty_per_channel_affine_quantized_out(out_local, torch::IntArrayRef(size_data, size_len), tensor_from_ocaml(scales), tensor_from_ocaml(zero_points), axis);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__euclidean_dist(gc_tensor x1, gc_tensor x2) {
  PROTECT(
    torch::Tensor results__ = torch::_euclidean_dist(tensor_from_ocaml(x1), tensor_from_ocaml(x2));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__euclidean_dist_out(gc_tensor out, gc_tensor x1, gc_tensor x2) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_euclidean_dist_out(out_local, tensor_from_ocaml(x1), tensor_from_ocaml(x2));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__fake_quantize_learnable_per_channel_affine(gc_tensor self, gc_tensor scale, gc_tensor zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor) {
  PROTECT(
    torch::Tensor results__ = torch::_fake_quantize_learnable_per_channel_affine(tensor_from_ocaml(self), tensor_from_ocaml(scale), tensor_from_ocaml(zero_point), axis, quant_min, quant_max, grad_factor);
    return tensor_to_ocaml(results__);
  )
}

void atg__fake_quantize_learnable_per_channel_affine_backward(raw_tensor *out__, gc_tensor grad, gc_tensor self, gc_tensor scale, gc_tensor zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor) {
  PROTECT(
    auto results__ = torch::_fake_quantize_learnable_per_channel_affine_backward(tensor_from_ocaml(grad), tensor_from_ocaml(self), tensor_from_ocaml(scale), tensor_from_ocaml(zero_point), axis, quant_min, quant_max, grad_factor);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

raw_tensor atg__fake_quantize_learnable_per_channel_affine_out(gc_tensor out, gc_tensor self, gc_tensor scale, gc_tensor zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_fake_quantize_learnable_per_channel_affine_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(scale), tensor_from_ocaml(zero_point), axis, quant_min, quant_max, grad_factor);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__fake_quantize_learnable_per_tensor_affine(gc_tensor self, gc_tensor scale, gc_tensor zero_point, int64_t quant_min, int64_t quant_max, double grad_factor) {
  PROTECT(
    torch::Tensor results__ = torch::_fake_quantize_learnable_per_tensor_affine(tensor_from_ocaml(self), tensor_from_ocaml(scale), tensor_from_ocaml(zero_point), quant_min, quant_max, grad_factor);
    return tensor_to_ocaml(results__);
  )
}

void atg__fake_quantize_learnable_per_tensor_affine_backward(raw_tensor *out__, gc_tensor grad, gc_tensor self, gc_tensor scale, gc_tensor zero_point, int64_t quant_min, int64_t quant_max, double grad_factor) {
  PROTECT(
    auto results__ = torch::_fake_quantize_learnable_per_tensor_affine_backward(tensor_from_ocaml(grad), tensor_from_ocaml(self), tensor_from_ocaml(scale), tensor_from_ocaml(zero_point), quant_min, quant_max, grad_factor);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

raw_tensor atg__fake_quantize_learnable_per_tensor_affine_out(gc_tensor out, gc_tensor self, gc_tensor scale, gc_tensor zero_point, int64_t quant_min, int64_t quant_max, double grad_factor) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_fake_quantize_learnable_per_tensor_affine_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(scale), tensor_from_ocaml(zero_point), quant_min, quant_max, grad_factor);
    return tensor_to_ocaml(results__);
  )
}

void atg__fake_quantize_per_tensor_affine_cachemask_tensor_qparams(raw_tensor *out__, gc_tensor self, gc_tensor scale, gc_tensor zero_point, gc_tensor fake_quant_enabled, int64_t quant_min, int64_t quant_max) {
  PROTECT(
    auto results__ = torch::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams(tensor_from_ocaml(self), tensor_from_ocaml(scale), tensor_from_ocaml(zero_point), tensor_from_ocaml(fake_quant_enabled), quant_min, quant_max);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__fake_quantize_per_tensor_affine_cachemask_tensor_qparams_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor self, gc_tensor scale, gc_tensor zero_point, gc_tensor fake_quant_enabled, int64_t quant_min, int64_t quant_max) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams_out(out0_local, out1_local, tensor_from_ocaml(self), tensor_from_ocaml(scale), tensor_from_ocaml(zero_point), tensor_from_ocaml(fake_quant_enabled), quant_min, quant_max);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg__fft_c2c(gc_tensor self, int64_t *dim_data, int dim_len, int64_t normalization, int forward) {
  PROTECT(
    torch::Tensor results__ = torch::_fft_c2c(tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), normalization, (bool)forward);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__fft_c2c_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int64_t normalization, int forward) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_fft_c2c_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), normalization, (bool)forward);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__fft_c2r(gc_tensor self, int64_t *dim_data, int dim_len, int64_t normalization, int64_t last_dim_size) {
  PROTECT(
    torch::Tensor results__ = torch::_fft_c2r(tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), normalization, last_dim_size);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__fft_c2r_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int64_t normalization, int64_t last_dim_size) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_fft_c2r_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), normalization, last_dim_size);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__fft_r2c(gc_tensor self, int64_t *dim_data, int dim_len, int64_t normalization, int onesided) {
  PROTECT(
    torch::Tensor results__ = torch::_fft_r2c(tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), normalization, (bool)onesided);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__fft_r2c_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int64_t normalization, int onesided) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_fft_r2c_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), normalization, (bool)onesided);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__fill_mem_eff_dropout_mask_(gc_tensor self, double dropout_p, int64_t seed, int64_t offset) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::_fill_mem_eff_dropout_mask_(self_local, dropout_p, seed, offset);
    return tensor_to_ocaml(results__);
  )
}

void atg__flash_attention_backward(raw_tensor *out__, gc_tensor grad_out, gc_tensor query, gc_tensor key, gc_tensor value, gc_tensor out, gc_tensor logsumexp, gc_tensor cum_seq_q, gc_tensor cum_seq_k, int64_t max_q, int64_t max_k, double dropout_p, int is_causal, gc_tensor rng_state, gc_tensor unused, double scale_v, int scale_null, int64_t window_size_left_v, int window_size_left_null, int64_t window_size_right_v, int window_size_right_null) {
  PROTECT(
    auto results__ = torch::_flash_attention_backward(tensor_from_ocaml(grad_out), tensor_from_ocaml(query), tensor_from_ocaml(key), tensor_from_ocaml(value), tensor_from_ocaml(out), tensor_from_ocaml(logsumexp), tensor_from_ocaml(cum_seq_q), tensor_from_ocaml(cum_seq_k), max_q, max_k, dropout_p, (bool)is_causal, tensor_from_ocaml(rng_state), tensor_from_ocaml(unused), scale_null ? c10::nullopt : c10::optional<double>(scale_v), window_size_left_null ? c10::nullopt : c10::optional<int64_t>(window_size_left_v), window_size_right_null ? c10::nullopt : c10::optional<int64_t>(window_size_right_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

raw_tensor atg__foobar(gc_tensor self, int arg1, int arg2, int arg3) {
  PROTECT(
    torch::Tensor results__ = torch::_foobar(tensor_from_ocaml(self), (bool)arg1, (bool)arg2, (bool)arg3);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__foobar_out(gc_tensor out, gc_tensor self, int arg1, int arg2, int arg3) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_foobar_out(out_local, tensor_from_ocaml(self), (bool)arg1, (bool)arg2, (bool)arg3);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__functional_assert_async(gc_tensor self, char * assert_msg, gc_tensor dep_token) {
  PROTECT(
    torch::Tensor results__ = torch::_functional_assert_async(tensor_from_ocaml(self), std::string(assert_msg), tensor_from_ocaml(dep_token));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__functional_assert_scalar(scalar self, char * assert_msg, gc_tensor dep_token) {
  PROTECT(
    torch::Tensor results__ = torch::_functional_assert_scalar(*self, std::string(assert_msg), tensor_from_ocaml(dep_token));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__functional_sym_constrain_range(scalar size, int64_t min_v, int min_null, int64_t max_v, int max_null, gc_tensor dep_token) {
  PROTECT(
    torch::Tensor results__ = torch::_functional_sym_constrain_range(*size, min_null ? c10::nullopt : c10::optional<int64_t>(min_v), max_null ? c10::nullopt : c10::optional<int64_t>(max_v), tensor_from_ocaml(dep_token));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__functional_sym_constrain_range_for_size(scalar size, int64_t min_v, int min_null, int64_t max_v, int max_null, gc_tensor dep_token) {
  PROTECT(
    torch::Tensor results__ = torch::_functional_sym_constrain_range_for_size(*size, min_null ? c10::nullopt : c10::optional<int64_t>(min_v), max_null ? c10::nullopt : c10::optional<int64_t>(max_v), tensor_from_ocaml(dep_token));
    return tensor_to_ocaml(results__);
  )
}

void atg__fused_adagrad(gc_tensor *out_data, int out_len, gc_tensor *self_data, int self_len, gc_tensor *grads_data, int grads_len, gc_tensor *state_sums_data, int state_sums_len, gc_tensor *state_steps_data, int state_steps_len, double lr, double lr_decay, double weight_decay, double eps, int maximize, gc_tensor grad_scale, gc_tensor found_inf) {
  PROTECT(
    torch::_fused_adagrad_out(of_carray_tensor(out_data, out_len), of_carray_tensor(self_data, self_len), of_carray_tensor(grads_data, grads_len), of_carray_tensor(state_sums_data, state_sums_len), of_carray_tensor(state_steps_data, state_steps_len), lr, lr_decay, weight_decay, eps, (bool)maximize, grad_scale ? std::make_optional(tensor_from_ocaml(grad_scale)) : std::nullopt, found_inf ? std::make_optional(tensor_from_ocaml(found_inf)) : std::nullopt);
  )
}

void atg__fused_adagrad_(gc_tensor *self_data, int self_len, gc_tensor *grads_data, int grads_len, gc_tensor *state_sums_data, int state_sums_len, gc_tensor *state_steps_data, int state_steps_len, double lr, double lr_decay, double weight_decay, double eps, int maximize, gc_tensor grad_scale, gc_tensor found_inf) {
  PROTECT(
    torch::_fused_adagrad_(of_carray_tensor(self_data, self_len), of_carray_tensor(grads_data, grads_len), of_carray_tensor(state_sums_data, state_sums_len), of_carray_tensor(state_steps_data, state_steps_len), lr, lr_decay, weight_decay, eps, (bool)maximize, grad_scale ? std::make_optional(tensor_from_ocaml(grad_scale)) : std::nullopt, found_inf ? std::make_optional(tensor_from_ocaml(found_inf)) : std::nullopt);
  )
}

void atg__fused_adam(gc_tensor *out_data, int out_len, gc_tensor *self_data, int self_len, gc_tensor *grads_data, int grads_len, gc_tensor *exp_avgs_data, int exp_avgs_len, gc_tensor *exp_avg_sqs_data, int exp_avg_sqs_len, gc_tensor *max_exp_avg_sqs_data, int max_exp_avg_sqs_len, gc_tensor *state_steps_data, int state_steps_len, double lr, double beta1, double beta2, double weight_decay, double eps, int amsgrad, int maximize, gc_tensor grad_scale, gc_tensor found_inf) {
  PROTECT(
    torch::_fused_adam_out(of_carray_tensor(out_data, out_len), of_carray_tensor(self_data, self_len), of_carray_tensor(grads_data, grads_len), of_carray_tensor(exp_avgs_data, exp_avgs_len), of_carray_tensor(exp_avg_sqs_data, exp_avg_sqs_len), of_carray_tensor(max_exp_avg_sqs_data, max_exp_avg_sqs_len), of_carray_tensor(state_steps_data, state_steps_len), lr, beta1, beta2, weight_decay, eps, (bool)amsgrad, (bool)maximize, grad_scale ? std::make_optional(tensor_from_ocaml(grad_scale)) : std::nullopt, found_inf ? std::make_optional(tensor_from_ocaml(found_inf)) : std::nullopt);
  )
}

void atg__fused_adam_(gc_tensor *self_data, int self_len, gc_tensor *grads_data, int grads_len, gc_tensor *exp_avgs_data, int exp_avgs_len, gc_tensor *exp_avg_sqs_data, int exp_avg_sqs_len, gc_tensor *max_exp_avg_sqs_data, int max_exp_avg_sqs_len, gc_tensor *state_steps_data, int state_steps_len, double lr, double beta1, double beta2, double weight_decay, double eps, int amsgrad, int maximize, gc_tensor grad_scale, gc_tensor found_inf) {
  PROTECT(
    torch::_fused_adam_(of_carray_tensor(self_data, self_len), of_carray_tensor(grads_data, grads_len), of_carray_tensor(exp_avgs_data, exp_avgs_len), of_carray_tensor(exp_avg_sqs_data, exp_avg_sqs_len), of_carray_tensor(max_exp_avg_sqs_data, max_exp_avg_sqs_len), of_carray_tensor(state_steps_data, state_steps_len), lr, beta1, beta2, weight_decay, eps, (bool)amsgrad, (bool)maximize, grad_scale ? std::make_optional(tensor_from_ocaml(grad_scale)) : std::nullopt, found_inf ? std::make_optional(tensor_from_ocaml(found_inf)) : std::nullopt);
  )
}

void atg__fused_adam_tensor_lr_(gc_tensor *self_data, int self_len, gc_tensor *grads_data, int grads_len, gc_tensor *exp_avgs_data, int exp_avgs_len, gc_tensor *exp_avg_sqs_data, int exp_avg_sqs_len, gc_tensor *max_exp_avg_sqs_data, int max_exp_avg_sqs_len, gc_tensor *state_steps_data, int state_steps_len, gc_tensor lr, double beta1, double beta2, double weight_decay, double eps, int amsgrad, int maximize, gc_tensor grad_scale, gc_tensor found_inf) {
  PROTECT(
    torch::_fused_adam_(of_carray_tensor(self_data, self_len), of_carray_tensor(grads_data, grads_len), of_carray_tensor(exp_avgs_data, exp_avgs_len), of_carray_tensor(exp_avg_sqs_data, exp_avg_sqs_len), of_carray_tensor(max_exp_avg_sqs_data, max_exp_avg_sqs_len), of_carray_tensor(state_steps_data, state_steps_len), tensor_from_ocaml(lr), beta1, beta2, weight_decay, eps, (bool)amsgrad, (bool)maximize, grad_scale ? std::make_optional(tensor_from_ocaml(grad_scale)) : std::nullopt, found_inf ? std::make_optional(tensor_from_ocaml(found_inf)) : std::nullopt);
  )
}

void atg__fused_adam_tensor_lr_out(gc_tensor *out_data, int out_len, gc_tensor *self_data, int self_len, gc_tensor *grads_data, int grads_len, gc_tensor *exp_avgs_data, int exp_avgs_len, gc_tensor *exp_avg_sqs_data, int exp_avg_sqs_len, gc_tensor *max_exp_avg_sqs_data, int max_exp_avg_sqs_len, gc_tensor *state_steps_data, int state_steps_len, gc_tensor lr, double beta1, double beta2, double weight_decay, double eps, int amsgrad, int maximize, gc_tensor grad_scale, gc_tensor found_inf) {
  PROTECT(
    torch::_fused_adam_out(of_carray_tensor(out_data, out_len), of_carray_tensor(self_data, self_len), of_carray_tensor(grads_data, grads_len), of_carray_tensor(exp_avgs_data, exp_avgs_len), of_carray_tensor(exp_avg_sqs_data, exp_avg_sqs_len), of_carray_tensor(max_exp_avg_sqs_data, max_exp_avg_sqs_len), of_carray_tensor(state_steps_data, state_steps_len), tensor_from_ocaml(lr), beta1, beta2, weight_decay, eps, (bool)amsgrad, (bool)maximize, grad_scale ? std::make_optional(tensor_from_ocaml(grad_scale)) : std::nullopt, found_inf ? std::make_optional(tensor_from_ocaml(found_inf)) : std::nullopt);
  )
}

void atg__fused_adamw(gc_tensor *out_data, int out_len, gc_tensor *self_data, int self_len, gc_tensor *grads_data, int grads_len, gc_tensor *exp_avgs_data, int exp_avgs_len, gc_tensor *exp_avg_sqs_data, int exp_avg_sqs_len, gc_tensor *max_exp_avg_sqs_data, int max_exp_avg_sqs_len, gc_tensor *state_steps_data, int state_steps_len, double lr, double beta1, double beta2, double weight_decay, double eps, int amsgrad, int maximize, gc_tensor grad_scale, gc_tensor found_inf) {
  PROTECT(
    torch::_fused_adamw_out(of_carray_tensor(out_data, out_len), of_carray_tensor(self_data, self_len), of_carray_tensor(grads_data, grads_len), of_carray_tensor(exp_avgs_data, exp_avgs_len), of_carray_tensor(exp_avg_sqs_data, exp_avg_sqs_len), of_carray_tensor(max_exp_avg_sqs_data, max_exp_avg_sqs_len), of_carray_tensor(state_steps_data, state_steps_len), lr, beta1, beta2, weight_decay, eps, (bool)amsgrad, (bool)maximize, grad_scale ? std::make_optional(tensor_from_ocaml(grad_scale)) : std::nullopt, found_inf ? std::make_optional(tensor_from_ocaml(found_inf)) : std::nullopt);
  )
}

void atg__fused_adamw_(gc_tensor *self_data, int self_len, gc_tensor *grads_data, int grads_len, gc_tensor *exp_avgs_data, int exp_avgs_len, gc_tensor *exp_avg_sqs_data, int exp_avg_sqs_len, gc_tensor *max_exp_avg_sqs_data, int max_exp_avg_sqs_len, gc_tensor *state_steps_data, int state_steps_len, double lr, double beta1, double beta2, double weight_decay, double eps, int amsgrad, int maximize, gc_tensor grad_scale, gc_tensor found_inf) {
  PROTECT(
    torch::_fused_adamw_(of_carray_tensor(self_data, self_len), of_carray_tensor(grads_data, grads_len), of_carray_tensor(exp_avgs_data, exp_avgs_len), of_carray_tensor(exp_avg_sqs_data, exp_avg_sqs_len), of_carray_tensor(max_exp_avg_sqs_data, max_exp_avg_sqs_len), of_carray_tensor(state_steps_data, state_steps_len), lr, beta1, beta2, weight_decay, eps, (bool)amsgrad, (bool)maximize, grad_scale ? std::make_optional(tensor_from_ocaml(grad_scale)) : std::nullopt, found_inf ? std::make_optional(tensor_from_ocaml(found_inf)) : std::nullopt);
  )
}

void atg__fused_adamw_tensor_lr_(gc_tensor *self_data, int self_len, gc_tensor *grads_data, int grads_len, gc_tensor *exp_avgs_data, int exp_avgs_len, gc_tensor *exp_avg_sqs_data, int exp_avg_sqs_len, gc_tensor *max_exp_avg_sqs_data, int max_exp_avg_sqs_len, gc_tensor *state_steps_data, int state_steps_len, gc_tensor lr, double beta1, double beta2, double weight_decay, double eps, int amsgrad, int maximize, gc_tensor grad_scale, gc_tensor found_inf) {
  PROTECT(
    torch::_fused_adamw_(of_carray_tensor(self_data, self_len), of_carray_tensor(grads_data, grads_len), of_carray_tensor(exp_avgs_data, exp_avgs_len), of_carray_tensor(exp_avg_sqs_data, exp_avg_sqs_len), of_carray_tensor(max_exp_avg_sqs_data, max_exp_avg_sqs_len), of_carray_tensor(state_steps_data, state_steps_len), tensor_from_ocaml(lr), beta1, beta2, weight_decay, eps, (bool)amsgrad, (bool)maximize, grad_scale ? std::make_optional(tensor_from_ocaml(grad_scale)) : std::nullopt, found_inf ? std::make_optional(tensor_from_ocaml(found_inf)) : std::nullopt);
  )
}

void atg__fused_adamw_tensor_lr_out(gc_tensor *out_data, int out_len, gc_tensor *self_data, int self_len, gc_tensor *grads_data, int grads_len, gc_tensor *exp_avgs_data, int exp_avgs_len, gc_tensor *exp_avg_sqs_data, int exp_avg_sqs_len, gc_tensor *max_exp_avg_sqs_data, int max_exp_avg_sqs_len, gc_tensor *state_steps_data, int state_steps_len, gc_tensor lr, double beta1, double beta2, double weight_decay, double eps, int amsgrad, int maximize, gc_tensor grad_scale, gc_tensor found_inf) {
  PROTECT(
    torch::_fused_adamw_out(of_carray_tensor(out_data, out_len), of_carray_tensor(self_data, self_len), of_carray_tensor(grads_data, grads_len), of_carray_tensor(exp_avgs_data, exp_avgs_len), of_carray_tensor(exp_avg_sqs_data, exp_avg_sqs_len), of_carray_tensor(max_exp_avg_sqs_data, max_exp_avg_sqs_len), of_carray_tensor(state_steps_data, state_steps_len), tensor_from_ocaml(lr), beta1, beta2, weight_decay, eps, (bool)amsgrad, (bool)maximize, grad_scale ? std::make_optional(tensor_from_ocaml(grad_scale)) : std::nullopt, found_inf ? std::make_optional(tensor_from_ocaml(found_inf)) : std::nullopt);
  )
}

void atg__fused_dropout(raw_tensor *out__, gc_tensor self, double p) {
  PROTECT(
    auto results__ = torch::_fused_dropout(tensor_from_ocaml(self), p);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__fused_dropout_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor self, double p) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::_fused_dropout_out(out0_local, out1_local, tensor_from_ocaml(self), p);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__fused_moving_avg_obs_fq_helper(raw_tensor *out__, gc_tensor self, gc_tensor observer_on, gc_tensor fake_quant_on, gc_tensor running_min, gc_tensor running_max, gc_tensor scale, gc_tensor zero_point, double averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, int per_row_fake_quant, int symmetric_quant) {
  torch::Tensor running_min_local = tensor_from_ocaml(running_min);
  torch::Tensor running_max_local = tensor_from_ocaml(running_max);
  torch::Tensor scale_local = tensor_from_ocaml(scale);
  torch::Tensor zero_point_local = tensor_from_ocaml(zero_point);
  PROTECT(
    auto results__ = torch::_fused_moving_avg_obs_fq_helper(tensor_from_ocaml(self), tensor_from_ocaml(observer_on), tensor_from_ocaml(fake_quant_on), running_min_local, running_max_local, scale_local, zero_point_local, averaging_const, quant_min, quant_max, ch_axis, (bool)per_row_fake_quant, (bool)symmetric_quant);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__fused_moving_avg_obs_fq_helper_functional(raw_tensor *out__, gc_tensor self, gc_tensor observer_on, gc_tensor fake_quant_on, gc_tensor running_min, gc_tensor running_max, gc_tensor scale, gc_tensor zero_point, double averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, int per_row_fake_quant, int symmetric_quant) {
  PROTECT(
    auto results__ = torch::_fused_moving_avg_obs_fq_helper_functional(tensor_from_ocaml(self), tensor_from_ocaml(observer_on), tensor_from_ocaml(fake_quant_on), tensor_from_ocaml(running_min), tensor_from_ocaml(running_max), tensor_from_ocaml(scale), tensor_from_ocaml(zero_point), averaging_const, quant_min, quant_max, ch_axis, (bool)per_row_fake_quant, (bool)symmetric_quant);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
    out__[4] = tensor_to_ocaml(std::get<4>(results__));
    out__[5] = tensor_to_ocaml(std::get<5>(results__));
  )
}

void atg__fused_moving_avg_obs_fq_helper_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor self, gc_tensor observer_on, gc_tensor fake_quant_on, gc_tensor running_min, gc_tensor running_max, gc_tensor scale, gc_tensor zero_point, double averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, int per_row_fake_quant, int symmetric_quant) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor running_min_local = tensor_from_ocaml(running_min);
  torch::Tensor running_max_local = tensor_from_ocaml(running_max);
  torch::Tensor scale_local = tensor_from_ocaml(scale);
  torch::Tensor zero_point_local = tensor_from_ocaml(zero_point);
  PROTECT(
    auto results__ = torch::_fused_moving_avg_obs_fq_helper_out(out0_local, out1_local, tensor_from_ocaml(self), tensor_from_ocaml(observer_on), tensor_from_ocaml(fake_quant_on), running_min_local, running_max_local, scale_local, zero_point_local, averaging_const, quant_min, quant_max, ch_axis, (bool)per_row_fake_quant, (bool)symmetric_quant);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

int64_t atg__fused_sdp_choice(gc_tensor query, gc_tensor key, gc_tensor value, gc_tensor attn_mask, double dropout_p, int is_causal, double scale_v, int scale_null, int enable_gqa) {
  PROTECT(
    return torch::_fused_sdp_choice(tensor_from_ocaml(query), tensor_from_ocaml(key), tensor_from_ocaml(value), attn_mask ? std::make_optional(tensor_from_ocaml(attn_mask)) : std::nullopt, dropout_p, (bool)is_causal, scale_null ? c10::nullopt : c10::optional<double>(scale_v), (bool)enable_gqa);
  )
  return 0;
}

void atg__fused_sgd(gc_tensor *out_data, int out_len, gc_tensor *self_data, int self_len, gc_tensor *grads_data, int grads_len, gc_tensor *momentum_buffer_list_data, int momentum_buffer_list_len, double weight_decay, double momentum, double lr, double dampening, int nesterov, int maximize, int is_first_step, gc_tensor grad_scale, gc_tensor found_inf) {
  PROTECT(
    torch::_fused_sgd_out(of_carray_tensor(out_data, out_len), of_carray_tensor(self_data, self_len), of_carray_tensor(grads_data, grads_len), of_carray_tensor(momentum_buffer_list_data, momentum_buffer_list_len), weight_decay, momentum, lr, dampening, (bool)nesterov, (bool)maximize, (bool)is_first_step, grad_scale ? std::make_optional(tensor_from_ocaml(grad_scale)) : std::nullopt, found_inf ? std::make_optional(tensor_from_ocaml(found_inf)) : std::nullopt);
  )
}

void atg__fused_sgd_(gc_tensor *self_data, int self_len, gc_tensor *grads_data, int grads_len, gc_tensor *momentum_buffer_list_data, int momentum_buffer_list_len, double weight_decay, double momentum, double lr, double dampening, int nesterov, int maximize, int is_first_step, gc_tensor grad_scale, gc_tensor found_inf) {
  PROTECT(
    torch::_fused_sgd_(of_carray_tensor(self_data, self_len), of_carray_tensor(grads_data, grads_len), of_carray_tensor(momentum_buffer_list_data, momentum_buffer_list_len), weight_decay, momentum, lr, dampening, (bool)nesterov, (bool)maximize, (bool)is_first_step, grad_scale ? std::make_optional(tensor_from_ocaml(grad_scale)) : std::nullopt, found_inf ? std::make_optional(tensor_from_ocaml(found_inf)) : std::nullopt);
  )
}

void atg__fused_sgd_tensor_lr_(gc_tensor *self_data, int self_len, gc_tensor *grads_data, int grads_len, gc_tensor *momentum_buffer_list_data, int momentum_buffer_list_len, double weight_decay, double momentum, gc_tensor lr, double dampening, int nesterov, int maximize, int is_first_step, gc_tensor grad_scale, gc_tensor found_inf) {
  PROTECT(
    torch::_fused_sgd_(of_carray_tensor(self_data, self_len), of_carray_tensor(grads_data, grads_len), of_carray_tensor(momentum_buffer_list_data, momentum_buffer_list_len), weight_decay, momentum, tensor_from_ocaml(lr), dampening, (bool)nesterov, (bool)maximize, (bool)is_first_step, grad_scale ? std::make_optional(tensor_from_ocaml(grad_scale)) : std::nullopt, found_inf ? std::make_optional(tensor_from_ocaml(found_inf)) : std::nullopt);
  )
}

void atg__fused_sgd_tensor_lr_out(gc_tensor *out_data, int out_len, gc_tensor *self_data, int self_len, gc_tensor *grads_data, int grads_len, gc_tensor *momentum_buffer_list_data, int momentum_buffer_list_len, double weight_decay, double momentum, gc_tensor lr, double dampening, int nesterov, int maximize, int is_first_step, gc_tensor grad_scale, gc_tensor found_inf) {
  PROTECT(
    torch::_fused_sgd_out(of_carray_tensor(out_data, out_len), of_carray_tensor(self_data, self_len), of_carray_tensor(grads_data, grads_len), of_carray_tensor(momentum_buffer_list_data, momentum_buffer_list_len), weight_decay, momentum, tensor_from_ocaml(lr), dampening, (bool)nesterov, (bool)maximize, (bool)is_first_step, grad_scale ? std::make_optional(tensor_from_ocaml(grad_scale)) : std::nullopt, found_inf ? std::make_optional(tensor_from_ocaml(found_inf)) : std::nullopt);
  )
}

raw_tensor atg__fw_primal(gc_tensor self, int64_t level) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self)._fw_primal(level);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__fw_primal_copy(gc_tensor self, int64_t level) {
  PROTECT(
    torch::Tensor results__ = torch::_fw_primal_copy(tensor_from_ocaml(self), level);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__fw_primal_copy_out(gc_tensor out, gc_tensor self, int64_t level) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_fw_primal_copy_out(out_local, tensor_from_ocaml(self), level);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__gather_sparse_backward(gc_tensor self, int64_t dim, gc_tensor index, gc_tensor grad) {
  PROTECT(
    torch::Tensor results__ = torch::_gather_sparse_backward(tensor_from_ocaml(self), dim, tensor_from_ocaml(index), tensor_from_ocaml(grad));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__grid_sampler_2d_cpu_fallback(gc_tensor input, gc_tensor grid, int64_t interpolation_mode, int64_t padding_mode, int align_corners) {
  PROTECT(
    torch::Tensor results__ = torch::_grid_sampler_2d_cpu_fallback(tensor_from_ocaml(input), tensor_from_ocaml(grid), interpolation_mode, padding_mode, (bool)align_corners);
    return tensor_to_ocaml(results__);
  )
}

void atg__grid_sampler_2d_cpu_fallback_backward(raw_tensor *out__, gc_tensor grad_output, gc_tensor input, gc_tensor grid, int64_t interpolation_mode, int64_t padding_mode, int align_corners) {
  PROTECT(
    auto results__ = torch::_grid_sampler_2d_cpu_fallback_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(input), tensor_from_ocaml(grid), interpolation_mode, padding_mode, (bool)align_corners);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg__grid_sampler_2d_cpu_fallback_out(gc_tensor out, gc_tensor input, gc_tensor grid, int64_t interpolation_mode, int64_t padding_mode, int align_corners) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_grid_sampler_2d_cpu_fallback_out(out_local, tensor_from_ocaml(input), tensor_from_ocaml(grid), interpolation_mode, padding_mode, (bool)align_corners);
    return tensor_to_ocaml(results__);
  )
}

int atg__has_compatible_shallow_copy_type(gc_tensor self, gc_tensor from) {
  PROTECT(
    return torch::_has_compatible_shallow_copy_type(tensor_from_ocaml(self), tensor_from_ocaml(from));
  )
  return 0;
}

int atg__has_same_storage_numel(gc_tensor self, gc_tensor other) {
  PROTECT(
    return torch::_has_same_storage_numel(tensor_from_ocaml(self), tensor_from_ocaml(other));
  )
  return 0;
}

raw_tensor *atg__histogramdd_bin_edges(gc_tensor self, int64_t *bins_data, int bins_len, double *range_data, int range_len, gc_tensor weight, int density) {
  PROTECT(
    auto results__ = torch::_histogramdd_bin_edges(tensor_from_ocaml(self), torch::IntArrayRef(bins_data, bins_len), at::ArrayRef<double>(range_data, range_len), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, (bool)density);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

void atg__histogramdd_bin_edges_out(gc_tensor *out_data, int out_len, gc_tensor self, int64_t *bins_data, int bins_len, double *range_data, int range_len, gc_tensor weight, int density) {
  PROTECT(
    torch::_histogramdd_bin_edges_out(of_carray_tensor(out_data, out_len), tensor_from_ocaml(self), torch::IntArrayRef(bins_data, bins_len), at::ArrayRef<double>(range_data, range_len), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, (bool)density);
  )
}

raw_tensor atg__histogramdd_from_bin_cts(gc_tensor self, int64_t *bins_data, int bins_len, double *range_data, int range_len, gc_tensor weight, int density) {
  PROTECT(
    torch::Tensor results__ = torch::_histogramdd_from_bin_cts(tensor_from_ocaml(self), torch::IntArrayRef(bins_data, bins_len), at::ArrayRef<double>(range_data, range_len), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, (bool)density);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__histogramdd_from_bin_cts_out(gc_tensor out, gc_tensor self, int64_t *bins_data, int bins_len, double *range_data, int range_len, gc_tensor weight, int density) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_histogramdd_from_bin_cts_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(bins_data, bins_len), at::ArrayRef<double>(range_data, range_len), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, (bool)density);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__histogramdd_from_bin_tensors(gc_tensor self, gc_tensor *bins_data, int bins_len, gc_tensor weight, int density) {
  PROTECT(
    torch::Tensor results__ = torch::_histogramdd_from_bin_tensors(tensor_from_ocaml(self), of_carray_tensor(bins_data, bins_len), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, (bool)density);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__histogramdd_from_bin_tensors_out(gc_tensor out, gc_tensor self, gc_tensor *bins_data, int bins_len, gc_tensor weight, int density) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_histogramdd_from_bin_tensors_out(out_local, tensor_from_ocaml(self), of_carray_tensor(bins_data, bins_len), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, (bool)density);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__indices(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self)._indices();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__indices_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_indices_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__indices_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_indices_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__int_mm(gc_tensor self, gc_tensor mat2) {
  PROTECT(
    torch::Tensor results__ = torch::_int_mm(tensor_from_ocaml(self), tensor_from_ocaml(mat2));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__int_mm_out(gc_tensor out, gc_tensor self, gc_tensor mat2) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_int_mm_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mat2));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__is_all_true(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_is_all_true(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__is_any_true(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_is_any_true(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

int atg__is_zerotensor(gc_tensor self) {
  PROTECT(
    return torch::_is_zerotensor(tensor_from_ocaml(self));
  )
  return 0;
}

raw_tensor atg__lazy_clone(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_lazy_clone(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

void atg__linalg_check_errors(gc_tensor info, char * api_name, int is_matrix) {
  PROTECT(
    torch::_linalg_check_errors(tensor_from_ocaml(info), std::string(api_name), (bool)is_matrix);
  )
}

void atg__linalg_det(raw_tensor *out__, gc_tensor A) {
  PROTECT(
    auto results__ = torch::_linalg_det(tensor_from_ocaml(A));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__linalg_det_result(raw_tensor *out__, gc_tensor result, gc_tensor LU, gc_tensor pivots, gc_tensor A) {
  torch::Tensor result_local = tensor_from_ocaml(result);
  torch::Tensor LU_local = tensor_from_ocaml(LU);
  torch::Tensor pivots_local = tensor_from_ocaml(pivots);
  PROTECT(
    auto results__ = torch::_linalg_det_out(result_local, LU_local, pivots_local, tensor_from_ocaml(A));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__linalg_eigh(raw_tensor *out__, gc_tensor A, char * UPLO, int compute_v) {
  PROTECT(
    auto results__ = torch::_linalg_eigh(tensor_from_ocaml(A), std::string(UPLO), (bool)compute_v);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__linalg_eigh_eigenvalues(raw_tensor *out__, gc_tensor eigenvalues, gc_tensor eigenvectors, gc_tensor A, char * UPLO, int compute_v) {
  torch::Tensor eigenvalues_local = tensor_from_ocaml(eigenvalues);
  torch::Tensor eigenvectors_local = tensor_from_ocaml(eigenvectors);
  PROTECT(
    auto results__ = torch::_linalg_eigh_out(eigenvalues_local, eigenvectors_local, tensor_from_ocaml(A), std::string(UPLO), (bool)compute_v);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg__linalg_eigvals(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_linalg_eigvals(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

void atg__linalg_slogdet(raw_tensor *out__, gc_tensor A) {
  PROTECT(
    auto results__ = torch::_linalg_slogdet(tensor_from_ocaml(A));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

void atg__linalg_slogdet_sign(raw_tensor *out__, gc_tensor sign, gc_tensor logabsdet, gc_tensor LU, gc_tensor pivots, gc_tensor A) {
  torch::Tensor sign_local = tensor_from_ocaml(sign);
  torch::Tensor logabsdet_local = tensor_from_ocaml(logabsdet);
  torch::Tensor LU_local = tensor_from_ocaml(LU);
  torch::Tensor pivots_local = tensor_from_ocaml(pivots);
  PROTECT(
    auto results__ = torch::_linalg_slogdet_out(sign_local, logabsdet_local, LU_local, pivots_local, tensor_from_ocaml(A));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

void atg__linalg_solve_ex(raw_tensor *out__, gc_tensor A, gc_tensor B, int left, int check_errors) {
  PROTECT(
    auto results__ = torch::_linalg_solve_ex(tensor_from_ocaml(A), tensor_from_ocaml(B), (bool)left, (bool)check_errors);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

void atg__linalg_solve_ex_result(raw_tensor *out__, gc_tensor result, gc_tensor LU, gc_tensor pivots, gc_tensor info, gc_tensor A, gc_tensor B, int left, int check_errors) {
  torch::Tensor result_local = tensor_from_ocaml(result);
  torch::Tensor LU_local = tensor_from_ocaml(LU);
  torch::Tensor pivots_local = tensor_from_ocaml(pivots);
  torch::Tensor info_local = tensor_from_ocaml(info);
  PROTECT(
    auto results__ = torch::_linalg_solve_ex_out(result_local, LU_local, pivots_local, info_local, tensor_from_ocaml(A), tensor_from_ocaml(B), (bool)left, (bool)check_errors);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

void atg__linalg_svd(raw_tensor *out__, gc_tensor A, int full_matrices, int compute_uv, char * driver_v, int driver_null) {
  PROTECT(
    auto results__ = torch::_linalg_svd(tensor_from_ocaml(A), (bool)full_matrices, (bool)compute_uv, driver_null ? c10::nullopt : c10::optional<c10::string_view>(driver_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__linalg_svd_u(raw_tensor *out__, gc_tensor U, gc_tensor S, gc_tensor Vh, gc_tensor A, int full_matrices, int compute_uv, char * driver_v, int driver_null) {
  torch::Tensor U_local = tensor_from_ocaml(U);
  torch::Tensor S_local = tensor_from_ocaml(S);
  torch::Tensor Vh_local = tensor_from_ocaml(Vh);
  PROTECT(
    auto results__ = torch::_linalg_svd_out(U_local, S_local, Vh_local, tensor_from_ocaml(A), (bool)full_matrices, (bool)compute_uv, driver_null ? c10::nullopt : c10::optional<c10::string_view>(driver_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

raw_tensor atg__log_softmax(gc_tensor self, int64_t dim, int half_to_float) {
  PROTECT(
    torch::Tensor results__ = torch::_log_softmax(tensor_from_ocaml(self), dim, (bool)half_to_float);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__log_softmax_backward_data(gc_tensor grad_output, gc_tensor output, int64_t dim, int input_dtype) {
  PROTECT(
    torch::Tensor results__ = torch::_log_softmax_backward_data(tensor_from_ocaml(grad_output), tensor_from_ocaml(output), dim, torch::ScalarType(input_dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__log_softmax_backward_data_out(gc_tensor out, gc_tensor grad_output, gc_tensor output, int64_t dim, int input_dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_log_softmax_backward_data_out(out_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(output), dim, torch::ScalarType(input_dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__log_softmax_out(gc_tensor out, gc_tensor self, int64_t dim, int half_to_float) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_log_softmax_out(out_local, tensor_from_ocaml(self), dim, (bool)half_to_float);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__logcumsumexp(gc_tensor self, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::_logcumsumexp(tensor_from_ocaml(self), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__logcumsumexp_out(gc_tensor out, gc_tensor self, int64_t dim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_logcumsumexp_out(out_local, tensor_from_ocaml(self), dim);
    return tensor_to_ocaml(results__);
  )
}

void atg__lstm_mps(raw_tensor *out__, gc_tensor input, gc_tensor *hx_data, int hx_len, gc_tensor *params_data, int params_len, int has_biases, int64_t num_layers, double dropout, int train, int bidirectional, int batch_first) {
  PROTECT(
    auto results__ = torch::_lstm_mps(tensor_from_ocaml(input), of_carray_tensor(hx_data, hx_len), of_carray_tensor(params_data, params_len), (bool)has_biases, num_layers, dropout, (bool)train, (bool)bidirectional, (bool)batch_first);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
    out__[4] = tensor_to_ocaml(std::get<4>(results__));
    out__[5] = tensor_to_ocaml(std::get<5>(results__));
  )
}

void atg__lstm_mps_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor out3, gc_tensor out4, gc_tensor out5, gc_tensor input, gc_tensor *hx_data, int hx_len, gc_tensor *params_data, int params_len, int has_biases, int64_t num_layers, double dropout, int train, int bidirectional, int batch_first) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  torch::Tensor out3_local = tensor_from_ocaml(out3);
  torch::Tensor out4_local = tensor_from_ocaml(out4);
  torch::Tensor out5_local = tensor_from_ocaml(out5);
  PROTECT(
    auto results__ = torch::_lstm_mps_out(out0_local, out1_local, out2_local, out3_local, out4_local, out5_local, tensor_from_ocaml(input), of_carray_tensor(hx_data, hx_len), of_carray_tensor(params_data, params_len), (bool)has_biases, num_layers, dropout, (bool)train, (bool)bidirectional, (bool)batch_first);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
    out__[4] = tensor_to_ocaml(std::get<4>(results__));
    out__[5] = tensor_to_ocaml(std::get<5>(results__));
  )
}

void atg__lu_with_info(raw_tensor *out__, gc_tensor self, int pivot, int check_errors) {
  PROTECT(
    auto results__ = torch::_lu_with_info(tensor_from_ocaml(self), (bool)pivot, (bool)check_errors);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

raw_tensor atg__make_dep_token(int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::_make_dep_token(at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__make_dual(gc_tensor primal, gc_tensor tangent, int64_t level) {
  PROTECT(
    torch::Tensor results__ = torch::_make_dual(tensor_from_ocaml(primal), tensor_from_ocaml(tangent), level);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__make_dual_copy(gc_tensor primal, gc_tensor tangent, int64_t level) {
  PROTECT(
    torch::Tensor results__ = torch::_make_dual_copy(tensor_from_ocaml(primal), tensor_from_ocaml(tangent), level);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__make_dual_copy_out(gc_tensor out, gc_tensor primal, gc_tensor tangent, int64_t level) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_make_dual_copy_out(out_local, tensor_from_ocaml(primal), tensor_from_ocaml(tangent), level);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__make_per_channel_quantized_tensor(gc_tensor self, gc_tensor scale, gc_tensor zero_point, int64_t axis) {
  PROTECT(
    torch::Tensor results__ = torch::_make_per_channel_quantized_tensor(tensor_from_ocaml(self), tensor_from_ocaml(scale), tensor_from_ocaml(zero_point), axis);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__make_per_channel_quantized_tensor_out(gc_tensor out, gc_tensor self, gc_tensor scale, gc_tensor zero_point, int64_t axis) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_make_per_channel_quantized_tensor_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(scale), tensor_from_ocaml(zero_point), axis);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__make_per_tensor_quantized_tensor(gc_tensor self, double scale, int64_t zero_point) {
  PROTECT(
    torch::Tensor results__ = torch::_make_per_tensor_quantized_tensor(tensor_from_ocaml(self), scale, zero_point);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__make_per_tensor_quantized_tensor_out(gc_tensor out, gc_tensor self, double scale, int64_t zero_point) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_make_per_tensor_quantized_tensor_out(out_local, tensor_from_ocaml(self), scale, zero_point);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__masked_scale(gc_tensor self, gc_tensor mask, double scale) {
  PROTECT(
    torch::Tensor results__ = torch::_masked_scale(tensor_from_ocaml(self), tensor_from_ocaml(mask), scale);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__masked_scale_out(gc_tensor out, gc_tensor self, gc_tensor mask, double scale) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_masked_scale_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mask), scale);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__masked_softmax(gc_tensor self, gc_tensor mask, int64_t dim_v, int dim_null, int64_t mask_type_v, int mask_type_null) {
  PROTECT(
    torch::Tensor results__ = torch::_masked_softmax(tensor_from_ocaml(self), tensor_from_ocaml(mask), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), mask_type_null ? c10::nullopt : c10::optional<int64_t>(mask_type_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__masked_softmax_backward(gc_tensor grad_output, gc_tensor output, gc_tensor mask, int64_t dim_v, int dim_null) {
  PROTECT(
    torch::Tensor results__ = torch::_masked_softmax_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(output), tensor_from_ocaml(mask), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__masked_softmax_backward_out(gc_tensor out, gc_tensor grad_output, gc_tensor output, gc_tensor mask, int64_t dim_v, int dim_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_masked_softmax_backward_out(out_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(output), tensor_from_ocaml(mask), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__masked_softmax_out(gc_tensor out, gc_tensor self, gc_tensor mask, int64_t dim_v, int dim_null, int64_t mask_type_v, int mask_type_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_masked_softmax_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mask), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), mask_type_null ? c10::nullopt : c10::optional<int64_t>(mask_type_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__mixed_dtypes_linear(gc_tensor input, gc_tensor weight, gc_tensor scale, gc_tensor bias, char * activation_v, int activation_null) {
  PROTECT(
    torch::Tensor results__ = torch::_mixed_dtypes_linear(tensor_from_ocaml(input), tensor_from_ocaml(weight), tensor_from_ocaml(scale), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, activation_null ? c10::nullopt : c10::optional<c10::string_view>(activation_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__mkldnn_reshape(gc_tensor self, int64_t *shape_data, int shape_len) {
  PROTECT(
    torch::Tensor results__ = torch::_mkldnn_reshape(tensor_from_ocaml(self), torch::IntArrayRef(shape_data, shape_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__mkldnn_reshape_out(gc_tensor out, gc_tensor self, int64_t *shape_data, int shape_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_mkldnn_reshape_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(shape_data, shape_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__mkldnn_transpose(gc_tensor self, int64_t dim0, int64_t dim1) {
  PROTECT(
    torch::Tensor results__ = torch::_mkldnn_transpose(tensor_from_ocaml(self), dim0, dim1);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__mkldnn_transpose_(gc_tensor self, int64_t dim0, int64_t dim1) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::_mkldnn_transpose_(self_local, dim0, dim1);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__mkldnn_transpose_out(gc_tensor out, gc_tensor self, int64_t dim0, int64_t dim1) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_mkldnn_transpose_out(out_local, tensor_from_ocaml(self), dim0, dim1);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__mps_convolution(gc_tensor self, gc_tensor weight, gc_tensor bias, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::_mps_convolution(tensor_from_ocaml(self), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__mps_convolution_out(gc_tensor out, gc_tensor self, gc_tensor weight, gc_tensor bias, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_mps_convolution_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__mps_convolution_transpose(gc_tensor self, gc_tensor weight, int64_t *padding_data, int padding_len, int64_t *output_padding_data, int output_padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::_mps_convolution_transpose(tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(output_padding_data, output_padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__mps_convolution_transpose_out(gc_tensor out, gc_tensor self, gc_tensor weight, int64_t *padding_data, int padding_len, int64_t *output_padding_data, int output_padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_mps_convolution_transpose_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(output_padding_data, output_padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

void atg__native_batch_norm_legit(raw_tensor *out__, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, int training, double momentum, double eps) {
  torch::Tensor running_mean_local = tensor_from_ocaml(running_mean);
  torch::Tensor running_var_local = tensor_from_ocaml(running_var);
  PROTECT(
    auto results__ = torch::_native_batch_norm_legit(tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, running_mean_local, running_var_local, (bool)training, momentum, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__native_batch_norm_legit_functional(raw_tensor *out__, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, int training, double momentum, double eps) {
  PROTECT(
    auto results__ = torch::_native_batch_norm_legit_functional(tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, tensor_from_ocaml(running_mean), tensor_from_ocaml(running_var), (bool)training, momentum, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
    out__[4] = tensor_to_ocaml(std::get<4>(results__));
  )
}

void atg__native_batch_norm_legit_no_stats(raw_tensor *out__, gc_tensor input, gc_tensor weight, gc_tensor bias, int training, double momentum, double eps) {
  PROTECT(
    auto results__ = torch::_native_batch_norm_legit(tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, (bool)training, momentum, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__native_batch_norm_legit_no_stats_out(raw_tensor *out__, gc_tensor out, gc_tensor save_mean, gc_tensor save_invstd, gc_tensor input, gc_tensor weight, gc_tensor bias, int training, double momentum, double eps) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  torch::Tensor save_mean_local = tensor_from_ocaml(save_mean);
  torch::Tensor save_invstd_local = tensor_from_ocaml(save_invstd);
  PROTECT(
    auto results__ = torch::_native_batch_norm_legit_out(out_local, save_mean_local, save_invstd_local, tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, (bool)training, momentum, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__native_batch_norm_legit_no_training(raw_tensor *out__, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, double momentum, double eps) {
  PROTECT(
    auto results__ = torch::_native_batch_norm_legit_no_training(tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, tensor_from_ocaml(running_mean), tensor_from_ocaml(running_var), momentum, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__native_batch_norm_legit_no_training_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, double momentum, double eps) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  PROTECT(
    auto results__ = torch::_native_batch_norm_legit_no_training_out(out0_local, out1_local, out2_local, tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, tensor_from_ocaml(running_mean), tensor_from_ocaml(running_var), momentum, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__native_batch_norm_legit_out(raw_tensor *out__, gc_tensor out, gc_tensor save_mean, gc_tensor save_invstd, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, int training, double momentum, double eps) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  torch::Tensor save_mean_local = tensor_from_ocaml(save_mean);
  torch::Tensor save_invstd_local = tensor_from_ocaml(save_invstd);
  torch::Tensor running_mean_local = tensor_from_ocaml(running_mean);
  torch::Tensor running_var_local = tensor_from_ocaml(running_var);
  PROTECT(
    auto results__ = torch::_native_batch_norm_legit_out(out_local, save_mean_local, save_invstd_local, tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, running_mean_local, running_var_local, (bool)training, momentum, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__native_multi_head_attention(raw_tensor *out__, gc_tensor query, gc_tensor key, gc_tensor value, int64_t embed_dim, int64_t num_head, gc_tensor qkv_weight, gc_tensor qkv_bias, gc_tensor proj_weight, gc_tensor proj_bias, gc_tensor mask, int need_weights, int average_attn_weights, int64_t mask_type_v, int mask_type_null) {
  PROTECT(
    auto results__ = torch::_native_multi_head_attention(tensor_from_ocaml(query), tensor_from_ocaml(key), tensor_from_ocaml(value), embed_dim, num_head, tensor_from_ocaml(qkv_weight), tensor_from_ocaml(qkv_bias), tensor_from_ocaml(proj_weight), tensor_from_ocaml(proj_bias), mask ? std::make_optional(tensor_from_ocaml(mask)) : std::nullopt, (bool)need_weights, (bool)average_attn_weights, mask_type_null ? c10::nullopt : c10::optional<int64_t>(mask_type_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__native_multi_head_attention_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor query, gc_tensor key, gc_tensor value, int64_t embed_dim, int64_t num_head, gc_tensor qkv_weight, gc_tensor qkv_bias, gc_tensor proj_weight, gc_tensor proj_bias, gc_tensor mask, int need_weights, int average_attn_weights, int64_t mask_type_v, int mask_type_null) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::_native_multi_head_attention_out(out0_local, out1_local, tensor_from_ocaml(query), tensor_from_ocaml(key), tensor_from_ocaml(value), embed_dim, num_head, tensor_from_ocaml(qkv_weight), tensor_from_ocaml(qkv_bias), tensor_from_ocaml(proj_weight), tensor_from_ocaml(proj_bias), mask ? std::make_optional(tensor_from_ocaml(mask)) : std::nullopt, (bool)need_weights, (bool)average_attn_weights, mask_type_null ? c10::nullopt : c10::optional<int64_t>(mask_type_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg__neg_view(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_neg_view(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__neg_view_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_neg_view_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__neg_view_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_neg_view_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

void atg__nested_compute_contiguous_strides_offsets(raw_tensor *out__, gc_tensor nested_size) {
  PROTECT(
    auto results__ = torch::_nested_compute_contiguous_strides_offsets(tensor_from_ocaml(nested_size));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg__nested_from_padded(gc_tensor padded, gc_tensor cpu_nested_shape_example, int fuse_transform_0213) {
  PROTECT(
    torch::Tensor results__ = torch::_nested_from_padded(tensor_from_ocaml(padded), tensor_from_ocaml(cpu_nested_shape_example), (bool)fuse_transform_0213);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_from_padded_and_nested_example(gc_tensor padded, gc_tensor nt_example) {
  PROTECT(
    torch::Tensor results__ = torch::_nested_from_padded_and_nested_example(tensor_from_ocaml(padded), tensor_from_ocaml(nt_example));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_from_padded_and_nested_example_out(gc_tensor out, gc_tensor padded, gc_tensor nt_example) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_nested_from_padded_and_nested_example_out(out_local, tensor_from_ocaml(padded), tensor_from_ocaml(nt_example));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_from_padded_out(gc_tensor out, gc_tensor padded, gc_tensor cpu_nested_shape_example, int fuse_transform_0213) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_nested_from_padded_out(out_local, tensor_from_ocaml(padded), tensor_from_ocaml(cpu_nested_shape_example), (bool)fuse_transform_0213);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_from_padded_tensor(gc_tensor padded, gc_tensor offsets, gc_tensor dummy, int64_t ragged_idx, gc_tensor min_seqlen, gc_tensor max_seqlen, int64_t sum_S_v, int sum_S_null) {
  PROTECT(
    torch::Tensor results__ = torch::_nested_from_padded_tensor(tensor_from_ocaml(padded), tensor_from_ocaml(offsets), tensor_from_ocaml(dummy), ragged_idx, min_seqlen ? std::make_optional(tensor_from_ocaml(min_seqlen)) : std::nullopt, max_seqlen ? std::make_optional(tensor_from_ocaml(max_seqlen)) : std::nullopt, sum_S_null ? c10::nullopt : c10::optional<int64_t>(sum_S_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_get_jagged_dummy(gc_tensor any) {
  PROTECT(
    torch::Tensor results__ = torch::_nested_get_jagged_dummy(tensor_from_ocaml(any));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_get_lengths(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_nested_get_lengths(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_get_max_seqlen(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_nested_get_max_seqlen(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_get_min_seqlen(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_nested_get_min_seqlen(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_get_offsets(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_nested_get_offsets(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

int64_t atg__nested_get_ragged_idx(gc_tensor self) {
  PROTECT(
    return torch::_nested_get_ragged_idx(tensor_from_ocaml(self));
  )
  return 0;
}

raw_tensor atg__nested_get_values(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_nested_get_values(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_get_values_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_nested_get_values_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_get_values_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_nested_get_values_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_select_backward(gc_tensor grad_output, gc_tensor self, int64_t dim, int64_t index) {
  PROTECT(
    torch::Tensor results__ = torch::_nested_select_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), dim, index);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_sum_backward(gc_tensor grad, gc_tensor self, int64_t *dim_data, int dim_len, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::_nested_sum_backward(tensor_from_ocaml(grad), tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_view_from_buffer(gc_tensor self, gc_tensor nested_size, gc_tensor nested_strides, gc_tensor offsets) {
  PROTECT(
    torch::Tensor results__ = torch::_nested_view_from_buffer(tensor_from_ocaml(self), tensor_from_ocaml(nested_size), tensor_from_ocaml(nested_strides), tensor_from_ocaml(offsets));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_view_from_buffer_copy(gc_tensor self, gc_tensor nested_size, gc_tensor nested_strides, gc_tensor offsets) {
  PROTECT(
    torch::Tensor results__ = torch::_nested_view_from_buffer_copy(tensor_from_ocaml(self), tensor_from_ocaml(nested_size), tensor_from_ocaml(nested_strides), tensor_from_ocaml(offsets));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_view_from_buffer_copy_out(gc_tensor out, gc_tensor self, gc_tensor nested_size, gc_tensor nested_strides, gc_tensor offsets) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_nested_view_from_buffer_copy_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(nested_size), tensor_from_ocaml(nested_strides), tensor_from_ocaml(offsets));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_view_from_jagged(gc_tensor self, gc_tensor offsets, gc_tensor dummy, gc_tensor lengths, int64_t ragged_idx, gc_tensor min_seqlen, gc_tensor max_seqlen) {
  PROTECT(
    torch::Tensor results__ = torch::_nested_view_from_jagged(tensor_from_ocaml(self), tensor_from_ocaml(offsets), tensor_from_ocaml(dummy), lengths ? std::make_optional(tensor_from_ocaml(lengths)) : std::nullopt, ragged_idx, min_seqlen ? std::make_optional(tensor_from_ocaml(min_seqlen)) : std::nullopt, max_seqlen ? std::make_optional(tensor_from_ocaml(max_seqlen)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_view_from_jagged_copy(gc_tensor self, gc_tensor offsets, gc_tensor dummy, gc_tensor lengths, int64_t ragged_idx, gc_tensor min_seqlen, gc_tensor max_seqlen) {
  PROTECT(
    torch::Tensor results__ = torch::_nested_view_from_jagged_copy(tensor_from_ocaml(self), tensor_from_ocaml(offsets), tensor_from_ocaml(dummy), lengths ? std::make_optional(tensor_from_ocaml(lengths)) : std::nullopt, ragged_idx, min_seqlen ? std::make_optional(tensor_from_ocaml(min_seqlen)) : std::nullopt, max_seqlen ? std::make_optional(tensor_from_ocaml(max_seqlen)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nested_view_from_jagged_copy_out(gc_tensor out, gc_tensor self, gc_tensor offsets, gc_tensor dummy, gc_tensor lengths, int64_t ragged_idx, gc_tensor min_seqlen, gc_tensor max_seqlen) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_nested_view_from_jagged_copy_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(offsets), tensor_from_ocaml(dummy), lengths ? std::make_optional(tensor_from_ocaml(lengths)) : std::nullopt, ragged_idx, min_seqlen ? std::make_optional(tensor_from_ocaml(min_seqlen)) : std::nullopt, max_seqlen ? std::make_optional(tensor_from_ocaml(max_seqlen)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__new_zeros_with_same_feature_meta(gc_tensor self, gc_tensor other, int64_t self_num_batch_dims) {
  PROTECT(
    torch::Tensor results__ = torch::_new_zeros_with_same_feature_meta(tensor_from_ocaml(self), tensor_from_ocaml(other), self_num_batch_dims);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__new_zeros_with_same_feature_meta_out(gc_tensor out, gc_tensor self, gc_tensor other, int64_t self_num_batch_dims) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_new_zeros_with_same_feature_meta_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other), self_num_batch_dims);
    return tensor_to_ocaml(results__);
  )
}

int atg__nnpack_available() {
  PROTECT(
    return torch::_nnpack_available();
  )
  return 0;
}

raw_tensor atg__nnpack_spatial_convolution(gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len) {
  PROTECT(
    torch::Tensor results__ = torch::_nnpack_spatial_convolution(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__nnpack_spatial_convolution_out(gc_tensor out, gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_nnpack_spatial_convolution_out(out_local, tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len));
    return tensor_to_ocaml(results__);
  )
}

int64_t atg__nnz(gc_tensor self) {
  PROTECT(
    return tensor_from_ocaml(self)._nnz();
  )
  return 0;
}

void atg__pack_padded_sequence(raw_tensor *out__, gc_tensor input, gc_tensor lengths, int batch_first) {
  PROTECT(
    auto results__ = torch::_pack_padded_sequence(tensor_from_ocaml(input), tensor_from_ocaml(lengths), (bool)batch_first);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg__pack_padded_sequence_backward(gc_tensor grad, int64_t *input_size_data, int input_size_len, gc_tensor batch_sizes, int batch_first) {
  PROTECT(
    torch::Tensor results__ = torch::_pack_padded_sequence_backward(tensor_from_ocaml(grad), torch::IntArrayRef(input_size_data, input_size_len), tensor_from_ocaml(batch_sizes), (bool)batch_first);
    return tensor_to_ocaml(results__);
  )
}

void atg__pack_padded_sequence_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor input, gc_tensor lengths, int batch_first) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::_pack_padded_sequence_out(out0_local, out1_local, tensor_from_ocaml(input), tensor_from_ocaml(lengths), (bool)batch_first);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg__pad_circular(gc_tensor self, int64_t *pad_data, int pad_len) {
  PROTECT(
    torch::Tensor results__ = torch::_pad_circular(tensor_from_ocaml(self), torch::IntArrayRef(pad_data, pad_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__pad_enum(gc_tensor self, int64_t *pad_data, int pad_len, int64_t mode, double value_v, int value_null) {
  PROTECT(
    torch::Tensor results__ = torch::_pad_enum(tensor_from_ocaml(self), torch::IntArrayRef(pad_data, pad_len), mode, value_null ? c10::nullopt : c10::optional<double>(value_v));
    return tensor_to_ocaml(results__);
  )
}

void atg__pad_packed_sequence(raw_tensor *out__, gc_tensor data, gc_tensor batch_sizes, int batch_first, scalar padding_value, int64_t total_length) {
  PROTECT(
    auto results__ = torch::_pad_packed_sequence(tensor_from_ocaml(data), tensor_from_ocaml(batch_sizes), (bool)batch_first, *padding_value, total_length);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg__pdist_backward(gc_tensor grad, gc_tensor self, double p, gc_tensor pdist) {
  PROTECT(
    torch::Tensor results__ = torch::_pdist_backward(tensor_from_ocaml(grad), tensor_from_ocaml(self), p, tensor_from_ocaml(pdist));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__pdist_backward_out(gc_tensor out, gc_tensor grad, gc_tensor self, double p, gc_tensor pdist) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_pdist_backward_out(out_local, tensor_from_ocaml(grad), tensor_from_ocaml(self), p, tensor_from_ocaml(pdist));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__pin_memory(gc_tensor self, int device) {
  PROTECT(
    torch::Tensor results__ = torch::_pin_memory(tensor_from_ocaml(self), optional_device_of_int(device));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__pin_memory_out(gc_tensor out, gc_tensor self, int device) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_pin_memory_out(out_local, tensor_from_ocaml(self), optional_device_of_int(device));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__prelu_kernel(gc_tensor self, gc_tensor weight) {
  PROTECT(
    torch::Tensor results__ = torch::_prelu_kernel(tensor_from_ocaml(self), tensor_from_ocaml(weight));
    return tensor_to_ocaml(results__);
  )
}

void atg__prelu_kernel_backward(raw_tensor *out__, gc_tensor grad_output, gc_tensor self, gc_tensor weight) {
  PROTECT(
    auto results__ = torch::_prelu_kernel_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(weight));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__print(char * s) {
  PROTECT(
    torch::_print(std::string(s));
  )
}

void atg__propagate_xla_data(gc_tensor input, gc_tensor output) {
  PROTECT(
    torch::_propagate_xla_data(tensor_from_ocaml(input), tensor_from_ocaml(output));
  )
}

raw_tensor atg__remove_batch_dim(gc_tensor self, int64_t level, int64_t batch_size, int64_t out_dim) {
  PROTECT(
    torch::Tensor results__ = torch::_remove_batch_dim(tensor_from_ocaml(self), level, batch_size, out_dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__reshape_alias(gc_tensor self, int64_t *size_data, int size_len, int64_t *stride_data, int stride_len) {
  PROTECT(
    torch::Tensor results__ = torch::_reshape_alias(tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), torch::IntArrayRef(stride_data, stride_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__reshape_alias_copy(gc_tensor self, int64_t *size_data, int size_len, int64_t *stride_data, int stride_len) {
  PROTECT(
    torch::Tensor results__ = torch::_reshape_alias_copy(tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), torch::IntArrayRef(stride_data, stride_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__reshape_alias_copy_out(gc_tensor out, gc_tensor self, int64_t *size_data, int size_len, int64_t *stride_data, int stride_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_reshape_alias_copy_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), torch::IntArrayRef(stride_data, stride_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__reshape_copy(gc_tensor self, int64_t *size_data, int size_len) {
  PROTECT(
    torch::Tensor results__ = torch::_reshape_copy(tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__reshape_from_tensor(gc_tensor self, gc_tensor shape) {
  PROTECT(
    torch::Tensor results__ = torch::_reshape_from_tensor(tensor_from_ocaml(self), tensor_from_ocaml(shape));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__resize_output(gc_tensor self, int64_t *size_data, int size_len, int device) {
  PROTECT(
    torch::Tensor results__ = torch::_resize_output(tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), device_of_int(device));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__resize_output_(gc_tensor self, int64_t *size_data, int size_len, int device) {
  PROTECT(
    torch::Tensor results__ = torch::_resize_output_(tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), device_of_int(device));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__resize_output_out(gc_tensor out, gc_tensor self, int64_t *size_data, int size_len, int device) {
  PROTECT(
    torch::Tensor results__ = torch::_resize_output_out(tensor_from_ocaml(out), tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), device_of_int(device));
    return tensor_to_ocaml(results__);
  )
}

void atg__rowwise_prune(raw_tensor *out__, gc_tensor weight, gc_tensor mask, int compressed_indices_dtype) {
  PROTECT(
    auto results__ = torch::_rowwise_prune(tensor_from_ocaml(weight), tensor_from_ocaml(mask), torch::ScalarType(compressed_indices_dtype));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg__safe_softmax(gc_tensor self, int64_t dim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::_safe_softmax(tensor_from_ocaml(self), dim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sample_dirichlet(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_sample_dirichlet(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sample_dirichlet_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_sample_dirichlet_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__saturate_weight_to_fp16(gc_tensor weight) {
  PROTECT(
    torch::Tensor results__ = torch::_saturate_weight_to_fp16(tensor_from_ocaml(weight));
    return tensor_to_ocaml(results__);
  )
}

void atg__scaled_dot_product_attention_math(raw_tensor *out__, gc_tensor query, gc_tensor key, gc_tensor value, gc_tensor attn_mask, double dropout_p, int is_causal, gc_tensor dropout_mask, double scale_v, int scale_null, int enable_gqa) {
  PROTECT(
    auto results__ = torch::_scaled_dot_product_attention_math(tensor_from_ocaml(query), tensor_from_ocaml(key), tensor_from_ocaml(value), attn_mask ? std::make_optional(tensor_from_ocaml(attn_mask)) : std::nullopt, dropout_p, (bool)is_causal, dropout_mask ? std::make_optional(tensor_from_ocaml(dropout_mask)) : std::nullopt, scale_null ? c10::nullopt : c10::optional<double>(scale_v), (bool)enable_gqa);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__scaled_dot_product_attention_math_for_mps(raw_tensor *out__, gc_tensor query, gc_tensor key, gc_tensor value, gc_tensor attn_mask, double dropout_p, int is_causal, gc_tensor dropout_mask, double scale_v, int scale_null) {
  PROTECT(
    auto results__ = torch::_scaled_dot_product_attention_math_for_mps(tensor_from_ocaml(query), tensor_from_ocaml(key), tensor_from_ocaml(value), attn_mask ? std::make_optional(tensor_from_ocaml(attn_mask)) : std::nullopt, dropout_p, (bool)is_causal, dropout_mask ? std::make_optional(tensor_from_ocaml(dropout_mask)) : std::nullopt, scale_null ? c10::nullopt : c10::optional<double>(scale_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__scaled_dot_product_cudnn_attention_backward(raw_tensor *out__, gc_tensor grad_out, gc_tensor query, gc_tensor key, gc_tensor value, gc_tensor out, gc_tensor logsumexp, gc_tensor philox_seed, gc_tensor philox_offset, gc_tensor attn_bias, gc_tensor cum_seq_q, gc_tensor cum_seq_k, int64_t max_q, int64_t max_k, double dropout_p, int is_causal, double scale_v, int scale_null) {
  PROTECT(
    auto results__ = torch::_scaled_dot_product_cudnn_attention_backward(tensor_from_ocaml(grad_out), tensor_from_ocaml(query), tensor_from_ocaml(key), tensor_from_ocaml(value), tensor_from_ocaml(out), tensor_from_ocaml(logsumexp), tensor_from_ocaml(philox_seed), tensor_from_ocaml(philox_offset), tensor_from_ocaml(attn_bias), tensor_from_ocaml(cum_seq_q), tensor_from_ocaml(cum_seq_k), max_q, max_k, dropout_p, (bool)is_causal, scale_null ? c10::nullopt : c10::optional<double>(scale_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__scaled_dot_product_efficient_attention(raw_tensor *out__, gc_tensor query, gc_tensor key, gc_tensor value, gc_tensor attn_bias, int compute_log_sumexp, double dropout_p, int is_causal, double scale_v, int scale_null) {
  PROTECT(
    auto results__ = torch::_scaled_dot_product_efficient_attention(tensor_from_ocaml(query), tensor_from_ocaml(key), tensor_from_ocaml(value), attn_bias ? std::make_optional(tensor_from_ocaml(attn_bias)) : std::nullopt, (bool)compute_log_sumexp, dropout_p, (bool)is_causal, scale_null ? c10::nullopt : c10::optional<double>(scale_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

void atg__scaled_dot_product_flash_attention_backward(raw_tensor *out__, gc_tensor grad_out, gc_tensor query, gc_tensor key, gc_tensor value, gc_tensor out, gc_tensor logsumexp, gc_tensor cum_seq_q, gc_tensor cum_seq_k, int64_t max_q, int64_t max_k, double dropout_p, int is_causal, gc_tensor philox_seed, gc_tensor philox_offset, double scale_v, int scale_null) {
  PROTECT(
    auto results__ = torch::_scaled_dot_product_flash_attention_backward(tensor_from_ocaml(grad_out), tensor_from_ocaml(query), tensor_from_ocaml(key), tensor_from_ocaml(value), tensor_from_ocaml(out), tensor_from_ocaml(logsumexp), tensor_from_ocaml(cum_seq_q), tensor_from_ocaml(cum_seq_k), max_q, max_k, dropout_p, (bool)is_causal, tensor_from_ocaml(philox_seed), tensor_from_ocaml(philox_offset), scale_null ? c10::nullopt : c10::optional<double>(scale_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__scaled_dot_product_flash_attention_for_cpu(raw_tensor *out__, gc_tensor query, gc_tensor key, gc_tensor value, double dropout_p, int is_causal, gc_tensor attn_mask, double scale_v, int scale_null) {
  PROTECT(
    auto results__ = torch::_scaled_dot_product_flash_attention_for_cpu(tensor_from_ocaml(query), tensor_from_ocaml(key), tensor_from_ocaml(value), dropout_p, (bool)is_causal, attn_mask ? std::make_optional(tensor_from_ocaml(attn_mask)) : std::nullopt, scale_null ? c10::nullopt : c10::optional<double>(scale_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__scaled_dot_product_flash_attention_for_cpu_backward(raw_tensor *out__, gc_tensor grad_out, gc_tensor query, gc_tensor key, gc_tensor value, gc_tensor out, gc_tensor logsumexp, double dropout_p, int is_causal, gc_tensor attn_mask, double scale_v, int scale_null) {
  PROTECT(
    auto results__ = torch::_scaled_dot_product_flash_attention_for_cpu_backward(tensor_from_ocaml(grad_out), tensor_from_ocaml(query), tensor_from_ocaml(key), tensor_from_ocaml(value), tensor_from_ocaml(out), tensor_from_ocaml(logsumexp), dropout_p, (bool)is_causal, attn_mask ? std::make_optional(tensor_from_ocaml(attn_mask)) : std::nullopt, scale_null ? c10::nullopt : c10::optional<double>(scale_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

raw_tensor atg__scaled_grouped_mm(gc_tensor self, gc_tensor mat2, gc_tensor scale_a, gc_tensor scale_b, gc_tensor offs, gc_tensor bias, gc_tensor scale_result, int out_dtype, int use_fast_accum) {
  PROTECT(
    torch::Tensor results__ = torch::_scaled_grouped_mm(tensor_from_ocaml(self), tensor_from_ocaml(mat2), tensor_from_ocaml(scale_a), tensor_from_ocaml(scale_b), offs ? std::make_optional(tensor_from_ocaml(offs)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, scale_result ? std::make_optional(tensor_from_ocaml(scale_result)) : std::nullopt, torch::ScalarType(out_dtype), (bool)use_fast_accum);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__scaled_mm(gc_tensor self, gc_tensor mat2, gc_tensor scale_a, gc_tensor scale_b, gc_tensor bias, gc_tensor scale_result, int out_dtype, int use_fast_accum) {
  PROTECT(
    torch::Tensor results__ = torch::_scaled_mm(tensor_from_ocaml(self), tensor_from_ocaml(mat2), tensor_from_ocaml(scale_a), tensor_from_ocaml(scale_b), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, scale_result ? std::make_optional(tensor_from_ocaml(scale_result)) : std::nullopt, torch::ScalarType(out_dtype), (bool)use_fast_accum);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__scaled_mm_out(gc_tensor out, gc_tensor self, gc_tensor mat2, gc_tensor scale_a, gc_tensor scale_b, gc_tensor bias, gc_tensor scale_result, int out_dtype, int use_fast_accum) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_scaled_mm_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mat2), tensor_from_ocaml(scale_a), tensor_from_ocaml(scale_b), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, scale_result ? std::make_optional(tensor_from_ocaml(scale_result)) : std::nullopt, torch::ScalarType(out_dtype), (bool)use_fast_accum);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__scatter_reduce(gc_tensor self, int64_t dim, gc_tensor index, gc_tensor src, char * reduce, int include_self) {
  PROTECT(
    torch::Tensor results__ = torch::scatter_reduce(tensor_from_ocaml(self), dim, tensor_from_ocaml(index), tensor_from_ocaml(src), std::string(reduce), (bool)include_self);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__scatter_reduce_(gc_tensor self, int64_t dim, gc_tensor index, gc_tensor src, char * reduce, int include_self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).scatter_reduce_(dim, tensor_from_ocaml(index), tensor_from_ocaml(src), std::string(reduce), (bool)include_self);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__scatter_reduce_two_out(gc_tensor out, gc_tensor self, int64_t dim, gc_tensor index, gc_tensor src, char * reduce, int include_self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::scatter_reduce_out(out_local, tensor_from_ocaml(self), dim, tensor_from_ocaml(index), tensor_from_ocaml(src), std::string(reduce), (bool)include_self);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__segment_reduce_backward(gc_tensor grad, gc_tensor output, gc_tensor data, char * reduce, gc_tensor lengths, gc_tensor offsets, int64_t axis, scalar initial) {
  PROTECT(
    torch::Tensor results__ = torch::_segment_reduce_backward(tensor_from_ocaml(grad), tensor_from_ocaml(output), tensor_from_ocaml(data), std::string(reduce), lengths ? std::make_optional(tensor_from_ocaml(lengths)) : std::nullopt, offsets ? std::make_optional(tensor_from_ocaml(offsets)) : std::nullopt, axis, *initial);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__segment_reduce_backward_out(gc_tensor out, gc_tensor grad, gc_tensor output, gc_tensor data, char * reduce, gc_tensor lengths, gc_tensor offsets, int64_t axis, scalar initial) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_segment_reduce_backward_out(out_local, tensor_from_ocaml(grad), tensor_from_ocaml(output), tensor_from_ocaml(data), std::string(reduce), lengths ? std::make_optional(tensor_from_ocaml(lengths)) : std::nullopt, offsets ? std::make_optional(tensor_from_ocaml(offsets)) : std::nullopt, axis, *initial);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__shape_as_tensor(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_shape_as_tensor(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

void atg__slow_conv2d_backward(raw_tensor *out__, gc_tensor grad_input, gc_tensor grad_weight, gc_tensor grad_bias, gc_tensor grad_output, gc_tensor self, gc_tensor weight, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  torch::Tensor grad_weight_local = tensor_from_ocaml(grad_weight);
  torch::Tensor grad_bias_local = tensor_from_ocaml(grad_bias);
  PROTECT(
    auto results__ = torch::_slow_conv2d_backward_out(grad_input_local, grad_weight_local, grad_bias_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__sobol_engine_draw(raw_tensor *out__, gc_tensor quasi, int64_t n, gc_tensor sobolstate, int64_t dimension, int64_t num_generated, int dtype) {
  PROTECT(
    auto results__ = torch::_sobol_engine_draw(tensor_from_ocaml(quasi), n, tensor_from_ocaml(sobolstate), dimension, num_generated, torch::ScalarType(dtype));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg__sobol_engine_ff_(gc_tensor self, int64_t n, gc_tensor sobolstate, int64_t dimension, int64_t num_generated) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::_sobol_engine_ff_(self_local, n, tensor_from_ocaml(sobolstate), dimension, num_generated);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sobol_engine_initialize_state_(gc_tensor self, int64_t dimension) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::_sobol_engine_initialize_state_(self_local, dimension);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sobol_engine_scramble_(gc_tensor self, gc_tensor ltm, int64_t dimension) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::_sobol_engine_scramble_(self_local, tensor_from_ocaml(ltm), dimension);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__softmax(gc_tensor self, int64_t dim, int half_to_float) {
  PROTECT(
    torch::Tensor results__ = torch::_softmax(tensor_from_ocaml(self), dim, (bool)half_to_float);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__softmax_backward_data(gc_tensor grad_output, gc_tensor output, int64_t dim, int input_dtype) {
  PROTECT(
    torch::Tensor results__ = torch::_softmax_backward_data(tensor_from_ocaml(grad_output), tensor_from_ocaml(output), dim, torch::ScalarType(input_dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__softmax_backward_data_out(gc_tensor grad_input, gc_tensor grad_output, gc_tensor output, int64_t dim, int input_dtype) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::_softmax_backward_data_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(output), dim, torch::ScalarType(input_dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__softmax_out(gc_tensor out, gc_tensor self, int64_t dim, int half_to_float) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_softmax_out(out_local, tensor_from_ocaml(self), dim, (bool)half_to_float);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_addmm(gc_tensor self, gc_tensor mat1, gc_tensor mat2, scalar beta, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_addmm(tensor_from_ocaml(self), tensor_from_ocaml(mat1), tensor_from_ocaml(mat2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_addmm_out(gc_tensor out, gc_tensor self, gc_tensor mat1, gc_tensor mat2, scalar beta, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_sparse_addmm_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mat1), tensor_from_ocaml(mat2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_broadcast_to(gc_tensor self, int64_t *size_data, int size_len) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_broadcast_to(tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_broadcast_to_copy(gc_tensor self, int64_t *size_data, int size_len) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_broadcast_to_copy(tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_broadcast_to_copy_out(gc_tensor out, gc_tensor self, int64_t *size_data, int size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_sparse_broadcast_to_copy_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_bsc_tensor_unsafe(gc_tensor ccol_indices, gc_tensor row_indices, gc_tensor values, int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_bsc_tensor_unsafe(tensor_from_ocaml(ccol_indices), tensor_from_ocaml(row_indices), tensor_from_ocaml(values), torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_bsr_tensor_unsafe(gc_tensor crow_indices, gc_tensor col_indices, gc_tensor values, int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_bsr_tensor_unsafe(tensor_from_ocaml(crow_indices), tensor_from_ocaml(col_indices), tensor_from_ocaml(values), torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_compressed_tensor_unsafe(gc_tensor compressed_indices, gc_tensor plain_indices, gc_tensor values, int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_compressed_tensor_unsafe(tensor_from_ocaml(compressed_indices), tensor_from_ocaml(plain_indices), tensor_from_ocaml(values), torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_compressed_tensor_with_dims(int64_t nnz, int64_t dense_dim, int64_t *size_data, int size_len, int64_t *blocksize_data, int blocksize_len, int index_dtype, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_compressed_tensor_with_dims(nnz, dense_dim, torch::IntArrayRef(size_data, size_len), torch::IntArrayRef(blocksize_data, blocksize_len), torch::ScalarType(index_dtype), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_coo_tensor_unsafe(gc_tensor indices, gc_tensor values, int64_t *size_data, int size_len, int options_kind, int options_device, int is_coalesced) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_coo_tensor_unsafe(tensor_from_ocaml(indices), tensor_from_ocaml(values), torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)), (bool)is_coalesced);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_coo_tensor_with_dims(sparse_dim, dense_dim, torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, int64_t *size_data, int size_len, gc_tensor indices, gc_tensor values, int options_kind, int options_device, int is_coalesced) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_coo_tensor_with_dims_and_tensors(sparse_dim, dense_dim, torch::IntArrayRef(size_data, size_len), tensor_from_ocaml(indices), tensor_from_ocaml(values), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)), (bool)is_coalesced);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_coo_tensor_with_dims_and_tensors_out(gc_tensor out, int64_t sparse_dim, int64_t dense_dim, int64_t *size_data, int size_len, gc_tensor indices, gc_tensor values, int is_coalesced) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_sparse_coo_tensor_with_dims_and_tensors_out(out_local, sparse_dim, dense_dim, torch::IntArrayRef(size_data, size_len), tensor_from_ocaml(indices), tensor_from_ocaml(values), (bool)is_coalesced);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_coo_tensor_with_dims_out(gc_tensor out, int64_t sparse_dim, int64_t dense_dim, int64_t *size_data, int size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_sparse_coo_tensor_with_dims_out(out_local, sparse_dim, dense_dim, torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_csc_tensor_unsafe(gc_tensor ccol_indices, gc_tensor row_indices, gc_tensor values, int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_csc_tensor_unsafe(tensor_from_ocaml(ccol_indices), tensor_from_ocaml(row_indices), tensor_from_ocaml(values), torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_csr_prod(gc_tensor self, int64_t *dim_data, int dim_len, int keepdim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_csr_prod(tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_csr_prod_dim_dtype_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int keepdim, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_sparse_csr_prod_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_csr_sum(gc_tensor self, int64_t *dim_data, int dim_len, int keepdim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_csr_sum(tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_csr_sum_dim_dtype_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int keepdim, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_sparse_csr_sum_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_csr_tensor_unsafe(gc_tensor crow_indices, gc_tensor col_indices, gc_tensor values, int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_csr_tensor_unsafe(tensor_from_ocaml(crow_indices), tensor_from_ocaml(col_indices), tensor_from_ocaml(values), torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_log_softmax(gc_tensor self, int64_t dim, int half_to_float) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_log_softmax(tensor_from_ocaml(self), dim, (bool)half_to_float);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_log_softmax_backward_data(gc_tensor grad_output, gc_tensor output, int64_t dim, gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_log_softmax_backward_data(tensor_from_ocaml(grad_output), tensor_from_ocaml(output), dim, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_log_softmax_backward_data_out(gc_tensor out, gc_tensor grad_output, gc_tensor output, int64_t dim, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_sparse_log_softmax_backward_data_out(out_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(output), dim, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_log_softmax_int(gc_tensor self, int64_t dim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_log_softmax(tensor_from_ocaml(self), dim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_log_softmax_out(gc_tensor out, gc_tensor self, int64_t dim, int half_to_float) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_sparse_log_softmax_out(out_local, tensor_from_ocaml(self), dim, (bool)half_to_float);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_mask_projection(gc_tensor self, gc_tensor mask, int accumulate_matches) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self)._sparse_mask_projection(tensor_from_ocaml(mask), (bool)accumulate_matches);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_mask_projection_out(gc_tensor out, gc_tensor self, gc_tensor mask, int accumulate_matches) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_sparse_mask_projection_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mask), (bool)accumulate_matches);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_mm(gc_tensor sparse, gc_tensor dense) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_mm(tensor_from_ocaml(sparse), tensor_from_ocaml(dense));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_mm_reduce(gc_tensor sparse, gc_tensor dense, char * reduce) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_mm(tensor_from_ocaml(sparse), tensor_from_ocaml(dense), std::string(reduce));
    return tensor_to_ocaml(results__);
  )
}

void atg__sparse_mm_reduce_impl(raw_tensor *out__, gc_tensor self, gc_tensor other, char * reduce) {
  PROTECT(
    auto results__ = torch::_sparse_mm_reduce_impl(tensor_from_ocaml(self), tensor_from_ocaml(other), std::string(reduce));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__sparse_semi_structured_apply(raw_tensor *out__, gc_tensor input, gc_tensor thread_masks) {
  PROTECT(
    auto results__ = torch::_sparse_semi_structured_apply(tensor_from_ocaml(input), tensor_from_ocaml(thread_masks));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg__sparse_semi_structured_apply_dense(gc_tensor input, gc_tensor thread_masks) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_semi_structured_apply_dense(tensor_from_ocaml(input), tensor_from_ocaml(thread_masks));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_semi_structured_linear(gc_tensor input, gc_tensor weight, gc_tensor meta, gc_tensor bias, char * activation_v, int activation_null, int out_dtype) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_semi_structured_linear(tensor_from_ocaml(input), tensor_from_ocaml(weight), tensor_from_ocaml(meta), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, activation_null ? c10::nullopt : c10::optional<c10::string_view>(activation_v), torch::ScalarType(out_dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_semi_structured_mm(gc_tensor mat1, gc_tensor mat1_meta, gc_tensor mat2, int out_dtype) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_semi_structured_mm(tensor_from_ocaml(mat1), tensor_from_ocaml(mat1_meta), tensor_from_ocaml(mat2), torch::ScalarType(out_dtype));
    return tensor_to_ocaml(results__);
  )
}

void atg__sparse_semi_structured_tile(raw_tensor *out__, gc_tensor input, char * algorithm, int use_cutlass) {
  PROTECT(
    auto results__ = torch::_sparse_semi_structured_tile(tensor_from_ocaml(input), std::string(algorithm), (bool)use_cutlass);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
    out__[4] = tensor_to_ocaml(std::get<4>(results__));
  )
}

raw_tensor atg__sparse_softmax(gc_tensor self, int64_t dim, int half_to_float) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_softmax(tensor_from_ocaml(self), dim, (bool)half_to_float);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_softmax_backward_data(gc_tensor grad_output, gc_tensor output, int64_t dim, gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_softmax_backward_data(tensor_from_ocaml(grad_output), tensor_from_ocaml(output), dim, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_softmax_backward_data_out(gc_tensor out, gc_tensor grad_output, gc_tensor output, int64_t dim, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_sparse_softmax_backward_data_out(out_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(output), dim, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_softmax_int(gc_tensor self, int64_t dim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_softmax(tensor_from_ocaml(self), dim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_softmax_out(gc_tensor out, gc_tensor self, int64_t dim, int half_to_float) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_sparse_softmax_out(out_local, tensor_from_ocaml(self), dim, (bool)half_to_float);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_sparse_matmul(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_sparse_matmul(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_sparse_matmul_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_sparse_sparse_matmul_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_sum(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_sum(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_sum_backward(gc_tensor grad, gc_tensor self, int64_t *dim_data, int dim_len) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_sum_backward(tensor_from_ocaml(grad), tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_sum_backward_out(gc_tensor out, gc_tensor grad, gc_tensor self, int64_t *dim_data, int dim_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_sparse_sum_backward_out(out_local, tensor_from_ocaml(grad), tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_sum_dim(gc_tensor self, int64_t *dim_data, int dim_len) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_sum(tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_sum_dim_dtype(gc_tensor self, int64_t *dim_data, int dim_len, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_sum(tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_sum_dim_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_sparse_sum_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__sparse_sum_dtype(gc_tensor self, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::_sparse_sum(tensor_from_ocaml(self), torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__spdiags(gc_tensor diagonals, gc_tensor offsets, int64_t *shape_data, int shape_len) {
  PROTECT(
    torch::Tensor results__ = torch::_spdiags(tensor_from_ocaml(diagonals), tensor_from_ocaml(offsets), torch::IntArrayRef(shape_data, shape_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__spdiags_out(gc_tensor out, gc_tensor diagonals, gc_tensor offsets, int64_t *shape_data, int shape_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_spdiags_out(out_local, tensor_from_ocaml(diagonals), tensor_from_ocaml(offsets), torch::IntArrayRef(shape_data, shape_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__spsolve(gc_tensor A, gc_tensor B, int left) {
  PROTECT(
    torch::Tensor results__ = torch::_spsolve(tensor_from_ocaml(A), tensor_from_ocaml(B), (bool)left);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__stack(gc_tensor *tensors_data, int tensors_len, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::_stack(of_carray_tensor(tensors_data, tensors_len), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__stack_out(gc_tensor out, gc_tensor *tensors_data, int tensors_len, int64_t dim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_stack_out(out_local, of_carray_tensor(tensors_data, tensors_len), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__standard_gamma(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_standard_gamma(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__standard_gamma_grad(gc_tensor self, gc_tensor output) {
  PROTECT(
    torch::Tensor results__ = torch::_standard_gamma_grad(tensor_from_ocaml(self), tensor_from_ocaml(output));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__standard_gamma_grad_out(gc_tensor out, gc_tensor self, gc_tensor output) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_standard_gamma_grad_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(output));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__standard_gamma_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_standard_gamma_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_ambiguous_defaults(gc_tensor dummy, int64_t a, int64_t b) {
  PROTECT(
    torch::Tensor results__ = torch::_test_ambiguous_defaults(tensor_from_ocaml(dummy), a, b);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_ambiguous_defaults_b(gc_tensor dummy, int64_t a, char * b) {
  PROTECT(
    torch::Tensor results__ = torch::_test_ambiguous_defaults(tensor_from_ocaml(dummy), a, std::string(b));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_autograd_multiple_dispatch(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_test_autograd_multiple_dispatch(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_autograd_multiple_dispatch_fullcoverage_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_test_autograd_multiple_dispatch_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_autograd_multiple_dispatch_ntonly(gc_tensor self, int b) {
  PROTECT(
    torch::Tensor results__ = torch::_test_autograd_multiple_dispatch(tensor_from_ocaml(self), (bool)b);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_autograd_multiple_dispatch_view(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_test_autograd_multiple_dispatch_view(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_autograd_multiple_dispatch_view_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_test_autograd_multiple_dispatch_view_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_autograd_multiple_dispatch_view_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_test_autograd_multiple_dispatch_view_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_check_tensor(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_test_check_tensor(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_functorch_fallback(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::_test_functorch_fallback(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_functorch_fallback_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_test_functorch_fallback_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_optional_filled_intlist(gc_tensor values, int64_t *addends_data, int addends_len) {
  PROTECT(
    torch::Tensor results__ = torch::_test_optional_filled_intlist(tensor_from_ocaml(values), addends_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(addends_data, addends_len)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_optional_filled_intlist_out(gc_tensor out, gc_tensor values, int64_t *addends_data, int addends_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_test_optional_filled_intlist_out(out_local, tensor_from_ocaml(values), addends_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(addends_data, addends_len)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_optional_floatlist(gc_tensor values, double *addends_data, int addends_len) {
  PROTECT(
    torch::Tensor results__ = torch::_test_optional_floatlist(tensor_from_ocaml(values), at::ArrayRef<double>(addends_data, addends_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_optional_floatlist_out(gc_tensor out, gc_tensor values, double *addends_data, int addends_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_test_optional_floatlist_out(out_local, tensor_from_ocaml(values), at::ArrayRef<double>(addends_data, addends_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_optional_intlist(gc_tensor values, int64_t *addends_data, int addends_len) {
  PROTECT(
    torch::Tensor results__ = torch::_test_optional_intlist(tensor_from_ocaml(values), addends_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(addends_data, addends_len)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_optional_intlist_out(gc_tensor out, gc_tensor values, int64_t *addends_data, int addends_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_test_optional_intlist_out(out_local, tensor_from_ocaml(values), addends_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(addends_data, addends_len)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_parallel_materialize(gc_tensor self, int64_t num_parallel, int skip_first) {
  PROTECT(
    torch::Tensor results__ = torch::_test_parallel_materialize(tensor_from_ocaml(self), num_parallel, (bool)skip_first);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_serialization_subcmul(gc_tensor self, gc_tensor other, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::_test_serialization_subcmul(tensor_from_ocaml(self), tensor_from_ocaml(other), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_string_default(gc_tensor dummy, char * a, char * b) {
  PROTECT(
    torch::Tensor results__ = torch::_test_string_default(tensor_from_ocaml(dummy), std::string(a), std::string(b));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_warn_in_autograd(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_test_warn_in_autograd(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__test_warn_in_autograd_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_test_warn_in_autograd_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

void atg__thnn_differentiable_gru_cell_backward(raw_tensor *out__, gc_tensor grad_hy, gc_tensor input_gates, gc_tensor hidden_gates, gc_tensor hx, gc_tensor input_bias, gc_tensor hidden_bias) {
  PROTECT(
    auto results__ = torch::_thnn_differentiable_gru_cell_backward(tensor_from_ocaml(grad_hy), tensor_from_ocaml(input_gates), tensor_from_ocaml(hidden_gates), tensor_from_ocaml(hx), input_bias ? std::make_optional(tensor_from_ocaml(input_bias)) : std::nullopt, hidden_bias ? std::make_optional(tensor_from_ocaml(hidden_bias)) : std::nullopt);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
    out__[4] = tensor_to_ocaml(std::get<4>(results__));
  )
}

void atg__thnn_differentiable_lstm_cell_backward(raw_tensor *out__, gc_tensor grad_hy, gc_tensor grad_cy, gc_tensor input_gates, gc_tensor hidden_gates, gc_tensor input_bias, gc_tensor hidden_bias, gc_tensor cx, gc_tensor cy) {
  PROTECT(
    auto results__ = torch::_thnn_differentiable_lstm_cell_backward(grad_hy ? std::make_optional(tensor_from_ocaml(grad_hy)) : std::nullopt, grad_cy ? std::make_optional(tensor_from_ocaml(grad_cy)) : std::nullopt, tensor_from_ocaml(input_gates), tensor_from_ocaml(hidden_gates), input_bias ? std::make_optional(tensor_from_ocaml(input_bias)) : std::nullopt, hidden_bias ? std::make_optional(tensor_from_ocaml(hidden_bias)) : std::nullopt, tensor_from_ocaml(cx), tensor_from_ocaml(cy));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
    out__[4] = tensor_to_ocaml(std::get<4>(results__));
  )
}

void atg__thnn_fused_gru_cell(raw_tensor *out__, gc_tensor input_gates, gc_tensor hidden_gates, gc_tensor hx, gc_tensor input_bias, gc_tensor hidden_bias) {
  PROTECT(
    auto results__ = torch::_thnn_fused_gru_cell(tensor_from_ocaml(input_gates), tensor_from_ocaml(hidden_gates), tensor_from_ocaml(hx), input_bias ? std::make_optional(tensor_from_ocaml(input_bias)) : std::nullopt, hidden_bias ? std::make_optional(tensor_from_ocaml(hidden_bias)) : std::nullopt);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__thnn_fused_gru_cell_backward(raw_tensor *out__, gc_tensor grad_hy, gc_tensor workspace, int has_bias) {
  PROTECT(
    auto results__ = torch::_thnn_fused_gru_cell_backward(tensor_from_ocaml(grad_hy), tensor_from_ocaml(workspace), (bool)has_bias);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
    out__[4] = tensor_to_ocaml(std::get<4>(results__));
  )
}

void atg__thnn_fused_gru_cell_backward_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor out3, gc_tensor out4, gc_tensor grad_hy, gc_tensor workspace, int has_bias) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  torch::Tensor out3_local = tensor_from_ocaml(out3);
  torch::Tensor out4_local = tensor_from_ocaml(out4);
  PROTECT(
    auto results__ = torch::_thnn_fused_gru_cell_backward_out(out0_local, out1_local, out2_local, out3_local, out4_local, tensor_from_ocaml(grad_hy), tensor_from_ocaml(workspace), (bool)has_bias);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
    out__[4] = tensor_to_ocaml(std::get<4>(results__));
  )
}

void atg__thnn_fused_gru_cell_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor input_gates, gc_tensor hidden_gates, gc_tensor hx, gc_tensor input_bias, gc_tensor hidden_bias) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::_thnn_fused_gru_cell_out(out0_local, out1_local, tensor_from_ocaml(input_gates), tensor_from_ocaml(hidden_gates), tensor_from_ocaml(hx), input_bias ? std::make_optional(tensor_from_ocaml(input_bias)) : std::nullopt, hidden_bias ? std::make_optional(tensor_from_ocaml(hidden_bias)) : std::nullopt);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__thnn_fused_lstm_cell(raw_tensor *out__, gc_tensor input_gates, gc_tensor hidden_gates, gc_tensor cx, gc_tensor input_bias, gc_tensor hidden_bias) {
  PROTECT(
    auto results__ = torch::_thnn_fused_lstm_cell(tensor_from_ocaml(input_gates), tensor_from_ocaml(hidden_gates), tensor_from_ocaml(cx), input_bias ? std::make_optional(tensor_from_ocaml(input_bias)) : std::nullopt, hidden_bias ? std::make_optional(tensor_from_ocaml(hidden_bias)) : std::nullopt);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__thnn_fused_lstm_cell_backward(raw_tensor *out__, gc_tensor grad_hy, gc_tensor grad_cy, gc_tensor cx, gc_tensor cy, gc_tensor workspace, int has_bias) {
  PROTECT(
    auto results__ = torch::_thnn_fused_lstm_cell_backward(grad_hy ? std::make_optional(tensor_from_ocaml(grad_hy)) : std::nullopt, grad_cy ? std::make_optional(tensor_from_ocaml(grad_cy)) : std::nullopt, tensor_from_ocaml(cx), tensor_from_ocaml(cy), tensor_from_ocaml(workspace), (bool)has_bias);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
    out__[4] = tensor_to_ocaml(std::get<4>(results__));
  )
}

void atg__thnn_fused_lstm_cell_backward_impl(raw_tensor *out__, gc_tensor grad_hy, gc_tensor grad_cy, gc_tensor cx, gc_tensor cy, gc_tensor workspace, int has_bias) {
  PROTECT(
    auto results__ = torch::_thnn_fused_lstm_cell_backward_impl(grad_hy ? std::make_optional(tensor_from_ocaml(grad_hy)) : std::nullopt, grad_cy ? std::make_optional(tensor_from_ocaml(grad_cy)) : std::nullopt, tensor_from_ocaml(cx), tensor_from_ocaml(cy), tensor_from_ocaml(workspace), (bool)has_bias);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__thnn_fused_lstm_cell_backward_impl_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor grad_hy, gc_tensor grad_cy, gc_tensor cx, gc_tensor cy, gc_tensor workspace, int has_bias) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  PROTECT(
    auto results__ = torch::_thnn_fused_lstm_cell_backward_impl_out(out0_local, out1_local, out2_local, grad_hy ? std::make_optional(tensor_from_ocaml(grad_hy)) : std::nullopt, grad_cy ? std::make_optional(tensor_from_ocaml(grad_cy)) : std::nullopt, tensor_from_ocaml(cx), tensor_from_ocaml(cy), tensor_from_ocaml(workspace), (bool)has_bias);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__thnn_fused_lstm_cell_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor input_gates, gc_tensor hidden_gates, gc_tensor cx, gc_tensor input_bias, gc_tensor hidden_bias) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  PROTECT(
    auto results__ = torch::_thnn_fused_lstm_cell_out(out0_local, out1_local, out2_local, tensor_from_ocaml(input_gates), tensor_from_ocaml(hidden_gates), tensor_from_ocaml(cx), input_bias ? std::make_optional(tensor_from_ocaml(input_bias)) : std::nullopt, hidden_bias ? std::make_optional(tensor_from_ocaml(hidden_bias)) : std::nullopt);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

raw_tensor atg__to_copy(gc_tensor self, int options_kind, int options_device, int non_blocking) {
  PROTECT(
    torch::Tensor results__ = torch::_to_copy(tensor_from_ocaml(self), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)), (bool)non_blocking);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__to_copy_out(gc_tensor out, gc_tensor self, int non_blocking) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_to_copy_out(out_local, tensor_from_ocaml(self), (bool)non_blocking);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg__to_cpu(gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    auto results__ = torch::_to_cpu(of_carray_tensor(tensors_data, tensors_len));
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor atg__to_dense(gc_tensor self, int dtype, int masked_grad) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self)._to_dense(torch::ScalarType(dtype), (bool)masked_grad);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__to_dense_out(gc_tensor out, gc_tensor self, int dtype, int masked_grad) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_to_dense_out(out_local, tensor_from_ocaml(self), torch::ScalarType(dtype), (bool)masked_grad);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__to_sparse_bsc(gc_tensor self, int64_t *blocksize_data, int blocksize_len, int64_t dense_dim_v, int dense_dim_null) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self)._to_sparse_bsc(torch::IntArrayRef(blocksize_data, blocksize_len), dense_dim_null ? c10::nullopt : c10::optional<int64_t>(dense_dim_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__to_sparse_bsc_out(gc_tensor out, gc_tensor self, int64_t *blocksize_data, int blocksize_len, int64_t dense_dim_v, int dense_dim_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_to_sparse_bsc_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(blocksize_data, blocksize_len), dense_dim_null ? c10::nullopt : c10::optional<int64_t>(dense_dim_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__to_sparse_bsr(gc_tensor self, int64_t *blocksize_data, int blocksize_len, int64_t dense_dim_v, int dense_dim_null) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self)._to_sparse_bsr(torch::IntArrayRef(blocksize_data, blocksize_len), dense_dim_null ? c10::nullopt : c10::optional<int64_t>(dense_dim_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__to_sparse_bsr_out(gc_tensor out, gc_tensor self, int64_t *blocksize_data, int blocksize_len, int64_t dense_dim_v, int dense_dim_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_to_sparse_bsr_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(blocksize_data, blocksize_len), dense_dim_null ? c10::nullopt : c10::optional<int64_t>(dense_dim_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__to_sparse_csc(gc_tensor self, int64_t dense_dim_v, int dense_dim_null) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self)._to_sparse_csc(dense_dim_null ? c10::nullopt : c10::optional<int64_t>(dense_dim_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__to_sparse_csc_out(gc_tensor out, gc_tensor self, int64_t dense_dim_v, int dense_dim_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_to_sparse_csc_out(out_local, tensor_from_ocaml(self), dense_dim_null ? c10::nullopt : c10::optional<int64_t>(dense_dim_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__to_sparse_csr(gc_tensor self, int64_t dense_dim_v, int dense_dim_null) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self)._to_sparse_csr(dense_dim_null ? c10::nullopt : c10::optional<int64_t>(dense_dim_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__to_sparse_csr_out(gc_tensor out, gc_tensor self, int64_t dense_dim_v, int dense_dim_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_to_sparse_csr_out(out_local, tensor_from_ocaml(self), dense_dim_null ? c10::nullopt : c10::optional<int64_t>(dense_dim_v));
    return tensor_to_ocaml(results__);
  )
}

void atg__to_sparse_semi_structured(raw_tensor *out__, gc_tensor dense) {
  PROTECT(
    auto results__ = torch::_to_sparse_semi_structured(tensor_from_ocaml(dense));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__transform_bias_rescale_qkv(raw_tensor *out__, gc_tensor qkv, gc_tensor qkv_bias, int64_t num_heads) {
  PROTECT(
    auto results__ = torch::_transform_bias_rescale_qkv(tensor_from_ocaml(qkv), tensor_from_ocaml(qkv_bias), num_heads);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__transform_bias_rescale_qkv_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor qkv, gc_tensor qkv_bias, int64_t num_heads) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  PROTECT(
    auto results__ = torch::_transform_bias_rescale_qkv_out(out0_local, out1_local, out2_local, tensor_from_ocaml(qkv), tensor_from_ocaml(qkv_bias), num_heads);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

raw_tensor atg__transformer_encoder_layer_fwd(gc_tensor src, int64_t embed_dim, int64_t num_heads, gc_tensor qkv_weight, gc_tensor qkv_bias, gc_tensor proj_weight, gc_tensor proj_bias, int use_gelu, int norm_first, double eps, gc_tensor norm_weight_1, gc_tensor norm_bias_1, gc_tensor norm_weight_2, gc_tensor norm_bias_2, gc_tensor ffn_weight_1, gc_tensor ffn_bias_1, gc_tensor ffn_weight_2, gc_tensor ffn_bias_2, gc_tensor mask, int64_t mask_type_v, int mask_type_null) {
  PROTECT(
    torch::Tensor results__ = torch::_transformer_encoder_layer_fwd(tensor_from_ocaml(src), embed_dim, num_heads, tensor_from_ocaml(qkv_weight), tensor_from_ocaml(qkv_bias), tensor_from_ocaml(proj_weight), tensor_from_ocaml(proj_bias), (bool)use_gelu, (bool)norm_first, eps, tensor_from_ocaml(norm_weight_1), tensor_from_ocaml(norm_bias_1), tensor_from_ocaml(norm_weight_2), tensor_from_ocaml(norm_bias_2), tensor_from_ocaml(ffn_weight_1), tensor_from_ocaml(ffn_bias_1), tensor_from_ocaml(ffn_weight_2), tensor_from_ocaml(ffn_bias_2), mask ? std::make_optional(tensor_from_ocaml(mask)) : std::nullopt, mask_type_null ? c10::nullopt : c10::optional<int64_t>(mask_type_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__transformer_encoder_layer_fwd_out(gc_tensor out, gc_tensor src, int64_t embed_dim, int64_t num_heads, gc_tensor qkv_weight, gc_tensor qkv_bias, gc_tensor proj_weight, gc_tensor proj_bias, int use_gelu, int norm_first, double eps, gc_tensor norm_weight_1, gc_tensor norm_bias_1, gc_tensor norm_weight_2, gc_tensor norm_bias_2, gc_tensor ffn_weight_1, gc_tensor ffn_bias_1, gc_tensor ffn_weight_2, gc_tensor ffn_bias_2, gc_tensor mask, int64_t mask_type_v, int mask_type_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_transformer_encoder_layer_fwd_out(out_local, tensor_from_ocaml(src), embed_dim, num_heads, tensor_from_ocaml(qkv_weight), tensor_from_ocaml(qkv_bias), tensor_from_ocaml(proj_weight), tensor_from_ocaml(proj_bias), (bool)use_gelu, (bool)norm_first, eps, tensor_from_ocaml(norm_weight_1), tensor_from_ocaml(norm_bias_1), tensor_from_ocaml(norm_weight_2), tensor_from_ocaml(norm_bias_2), tensor_from_ocaml(ffn_weight_1), tensor_from_ocaml(ffn_bias_1), tensor_from_ocaml(ffn_weight_2), tensor_from_ocaml(ffn_bias_2), mask ? std::make_optional(tensor_from_ocaml(mask)) : std::nullopt, mask_type_null ? c10::nullopt : c10::optional<int64_t>(mask_type_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__trilinear(gc_tensor i1, gc_tensor i2, gc_tensor i3, int64_t *expand1_data, int expand1_len, int64_t *expand2_data, int expand2_len, int64_t *expand3_data, int expand3_len, int64_t *sumdim_data, int sumdim_len, int64_t unroll_dim) {
  PROTECT(
    torch::Tensor results__ = torch::_trilinear(tensor_from_ocaml(i1), tensor_from_ocaml(i2), tensor_from_ocaml(i3), torch::IntArrayRef(expand1_data, expand1_len), torch::IntArrayRef(expand2_data, expand2_len), torch::IntArrayRef(expand3_data, expand3_len), torch::IntArrayRef(sumdim_data, sumdim_len), unroll_dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__trilinear_out(gc_tensor out, gc_tensor i1, gc_tensor i2, gc_tensor i3, int64_t *expand1_data, int expand1_len, int64_t *expand2_data, int expand2_len, int64_t *expand3_data, int expand3_len, int64_t *sumdim_data, int sumdim_len, int64_t unroll_dim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_trilinear_out(out_local, tensor_from_ocaml(i1), tensor_from_ocaml(i2), tensor_from_ocaml(i3), torch::IntArrayRef(expand1_data, expand1_len), torch::IntArrayRef(expand2_data, expand2_len), torch::IntArrayRef(expand3_data, expand3_len), torch::IntArrayRef(sumdim_data, sumdim_len), unroll_dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__triton_multi_head_attention(gc_tensor query, gc_tensor key, gc_tensor value, int64_t embed_dim, int64_t num_head, gc_tensor qkv_weight, gc_tensor qkv_bias, gc_tensor proj_weight, gc_tensor proj_bias, gc_tensor mask) {
  PROTECT(
    torch::Tensor results__ = torch::_triton_multi_head_attention(tensor_from_ocaml(query), tensor_from_ocaml(key), tensor_from_ocaml(value), embed_dim, num_head, tensor_from_ocaml(qkv_weight), tensor_from_ocaml(qkv_bias), tensor_from_ocaml(proj_weight), tensor_from_ocaml(proj_bias), mask ? std::make_optional(tensor_from_ocaml(mask)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__triton_multi_head_attention_out(gc_tensor out, gc_tensor query, gc_tensor key, gc_tensor value, int64_t embed_dim, int64_t num_head, gc_tensor qkv_weight, gc_tensor qkv_bias, gc_tensor proj_weight, gc_tensor proj_bias, gc_tensor mask) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_triton_multi_head_attention_out(out_local, tensor_from_ocaml(query), tensor_from_ocaml(key), tensor_from_ocaml(value), embed_dim, num_head, tensor_from_ocaml(qkv_weight), tensor_from_ocaml(qkv_bias), tensor_from_ocaml(proj_weight), tensor_from_ocaml(proj_bias), mask ? std::make_optional(tensor_from_ocaml(mask)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__triton_scaled_dot_attention(gc_tensor q, gc_tensor k, gc_tensor v, double dropout_p) {
  PROTECT(
    torch::Tensor results__ = torch::_triton_scaled_dot_attention(tensor_from_ocaml(q), tensor_from_ocaml(k), tensor_from_ocaml(v), dropout_p);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__triton_scaled_dot_attention_out(gc_tensor out, gc_tensor q, gc_tensor k, gc_tensor v, double dropout_p) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_triton_scaled_dot_attention_out(out_local, tensor_from_ocaml(q), tensor_from_ocaml(k), tensor_from_ocaml(v), dropout_p);
    return tensor_to_ocaml(results__);
  )
}

void atg__unique(raw_tensor *out__, gc_tensor self, int sorted, int return_inverse) {
  PROTECT(
    auto results__ = torch::_unique(tensor_from_ocaml(self), (bool)sorted, (bool)return_inverse);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__unique2(raw_tensor *out__, gc_tensor self, int sorted, int return_inverse, int return_counts) {
  PROTECT(
    auto results__ = torch::_unique2(tensor_from_ocaml(self), (bool)sorted, (bool)return_inverse, (bool)return_counts);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__unique2_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor self, int sorted, int return_inverse, int return_counts) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  PROTECT(
    auto results__ = torch::_unique2_out(out0_local, out1_local, out2_local, tensor_from_ocaml(self), (bool)sorted, (bool)return_inverse, (bool)return_counts);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg__unique_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor self, int sorted, int return_inverse) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::_unique_out(out0_local, out1_local, tensor_from_ocaml(self), (bool)sorted, (bool)return_inverse);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__unpack_dual(raw_tensor *out__, gc_tensor dual, int64_t level) {
  PROTECT(
    auto results__ = torch::_unpack_dual(tensor_from_ocaml(dual), level);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg__unsafe_view(gc_tensor self, int64_t *size_data, int size_len) {
  PROTECT(
    torch::Tensor results__ = torch::_unsafe_view(tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__unsafe_view_out(gc_tensor out, gc_tensor self, int64_t *size_data, int size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_unsafe_view_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_bicubic2d_aa(gc_tensor self, int64_t *output_size_data, int output_size_len, int align_corners, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::_upsample_bicubic2d_aa(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), (bool)align_corners, scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_bicubic2d_aa_backward(gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, int align_corners, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::_upsample_bicubic2d_aa_backward(tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), (bool)align_corners, scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_bicubic2d_aa_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, int align_corners, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::_upsample_bicubic2d_aa_backward_out(grad_input_local, tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), (bool)align_corners, scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_bicubic2d_aa_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len, int align_corners, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_upsample_bicubic2d_aa_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), (bool)align_corners, scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_bicubic2d_aa_vec(gc_tensor input, int64_t *output_size_data, int output_size_len, int align_corners, double *scale_factors_data, int scale_factors_len) {
  PROTECT(
    torch::Tensor results__ = torch::_upsample_bicubic2d_aa(tensor_from_ocaml(input), output_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(output_size_data, output_size_len)), (bool)align_corners, at::ArrayRef<double>(scale_factors_data, scale_factors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_bilinear2d_aa(gc_tensor self, int64_t *output_size_data, int output_size_len, int align_corners, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::_upsample_bilinear2d_aa(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), (bool)align_corners, scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_bilinear2d_aa_backward(gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, int align_corners, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::_upsample_bilinear2d_aa_backward(tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), (bool)align_corners, scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_bilinear2d_aa_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, int align_corners, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::_upsample_bilinear2d_aa_backward_out(grad_input_local, tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), (bool)align_corners, scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_bilinear2d_aa_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len, int align_corners, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_upsample_bilinear2d_aa_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), (bool)align_corners, scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_bilinear2d_aa_vec(gc_tensor input, int64_t *output_size_data, int output_size_len, int align_corners, double *scale_factors_data, int scale_factors_len) {
  PROTECT(
    torch::Tensor results__ = torch::_upsample_bilinear2d_aa(tensor_from_ocaml(input), output_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(output_size_data, output_size_len)), (bool)align_corners, at::ArrayRef<double>(scale_factors_data, scale_factors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_nearest_exact1d(gc_tensor self, int64_t *output_size_data, int output_size_len, double scales_v, int scales_null) {
  PROTECT(
    torch::Tensor results__ = torch::_upsample_nearest_exact1d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), scales_null ? c10::nullopt : c10::optional<double>(scales_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_nearest_exact1d_backward(gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, double scales_v, int scales_null) {
  PROTECT(
    torch::Tensor results__ = torch::_upsample_nearest_exact1d_backward(tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), scales_null ? c10::nullopt : c10::optional<double>(scales_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_nearest_exact1d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, double scales_v, int scales_null) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::_upsample_nearest_exact1d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), scales_null ? c10::nullopt : c10::optional<double>(scales_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_nearest_exact1d_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len, double scales_v, int scales_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_upsample_nearest_exact1d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), scales_null ? c10::nullopt : c10::optional<double>(scales_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_nearest_exact1d_vec(gc_tensor input, int64_t *output_size_data, int output_size_len, double *scale_factors_data, int scale_factors_len) {
  PROTECT(
    torch::Tensor results__ = torch::_upsample_nearest_exact1d(tensor_from_ocaml(input), output_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(output_size_data, output_size_len)), at::ArrayRef<double>(scale_factors_data, scale_factors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_nearest_exact2d(gc_tensor self, int64_t *output_size_data, int output_size_len, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::_upsample_nearest_exact2d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_nearest_exact2d_backward(gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::_upsample_nearest_exact2d_backward(tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_nearest_exact2d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::_upsample_nearest_exact2d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_nearest_exact2d_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_upsample_nearest_exact2d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_nearest_exact2d_vec(gc_tensor input, int64_t *output_size_data, int output_size_len, double *scale_factors_data, int scale_factors_len) {
  PROTECT(
    torch::Tensor results__ = torch::_upsample_nearest_exact2d(tensor_from_ocaml(input), output_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(output_size_data, output_size_len)), at::ArrayRef<double>(scale_factors_data, scale_factors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_nearest_exact3d(gc_tensor self, int64_t *output_size_data, int output_size_len, double scales_d_v, int scales_d_null, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::_upsample_nearest_exact3d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), scales_d_null ? c10::nullopt : c10::optional<double>(scales_d_v), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_nearest_exact3d_backward(gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, double scales_d_v, int scales_d_null, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::_upsample_nearest_exact3d_backward(tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), scales_d_null ? c10::nullopt : c10::optional<double>(scales_d_v), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_nearest_exact3d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, double scales_d_v, int scales_d_null, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::_upsample_nearest_exact3d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), scales_d_null ? c10::nullopt : c10::optional<double>(scales_d_v), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_nearest_exact3d_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len, double scales_d_v, int scales_d_null, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_upsample_nearest_exact3d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), scales_d_null ? c10::nullopt : c10::optional<double>(scales_d_v), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__upsample_nearest_exact3d_vec(gc_tensor input, int64_t *output_size_data, int output_size_len, double *scale_factors_data, int scale_factors_len) {
  PROTECT(
    torch::Tensor results__ = torch::_upsample_nearest_exact3d(tensor_from_ocaml(input), output_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(output_size_data, output_size_len)), at::ArrayRef<double>(scale_factors_data, scale_factors_len));
    return tensor_to_ocaml(results__);
  )
}

int atg__use_cudnn_ctc_loss(gc_tensor log_probs, gc_tensor targets, int64_t *input_lengths_data, int input_lengths_len, int64_t *target_lengths_data, int target_lengths_len, int64_t blank) {
  PROTECT(
    return torch::_use_cudnn_ctc_loss(tensor_from_ocaml(log_probs), tensor_from_ocaml(targets), torch::IntArrayRef(input_lengths_data, input_lengths_len), torch::IntArrayRef(target_lengths_data, target_lengths_len), blank);
  )
  return 0;
}

int atg__use_cudnn_ctc_loss_tensor(gc_tensor log_probs, gc_tensor targets, gc_tensor input_lengths, gc_tensor target_lengths, int64_t blank) {
  PROTECT(
    return torch::_use_cudnn_ctc_loss(tensor_from_ocaml(log_probs), tensor_from_ocaml(targets), tensor_from_ocaml(input_lengths), tensor_from_ocaml(target_lengths), blank);
  )
  return 0;
}

int atg__use_cudnn_rnn_flatten_weight() {
  PROTECT(
    return torch::_use_cudnn_rnn_flatten_weight();
  )
  return 0;
}

void atg__validate_compressed_sparse_indices(int is_crow, gc_tensor compressed_idx, gc_tensor plain_idx, int64_t cdim, int64_t dim, int64_t nnz) {
  PROTECT(
    torch::_validate_compressed_sparse_indices((bool)is_crow, tensor_from_ocaml(compressed_idx), tensor_from_ocaml(plain_idx), cdim, dim, nnz);
  )
}

void atg__validate_sparse_bsc_tensor_args(gc_tensor ccol_indices, gc_tensor row_indices, gc_tensor values, int64_t *size_data, int size_len) {
  PROTECT(
    torch::_validate_sparse_bsc_tensor_args(tensor_from_ocaml(ccol_indices), tensor_from_ocaml(row_indices), tensor_from_ocaml(values), torch::IntArrayRef(size_data, size_len));
  )
}

void atg__validate_sparse_bsr_tensor_args(gc_tensor crow_indices, gc_tensor col_indices, gc_tensor values, int64_t *size_data, int size_len) {
  PROTECT(
    torch::_validate_sparse_bsr_tensor_args(tensor_from_ocaml(crow_indices), tensor_from_ocaml(col_indices), tensor_from_ocaml(values), torch::IntArrayRef(size_data, size_len));
  )
}

void atg__validate_sparse_csc_tensor_args(gc_tensor ccol_indices, gc_tensor row_indices, gc_tensor values, int64_t *size_data, int size_len) {
  PROTECT(
    torch::_validate_sparse_csc_tensor_args(tensor_from_ocaml(ccol_indices), tensor_from_ocaml(row_indices), tensor_from_ocaml(values), torch::IntArrayRef(size_data, size_len));
  )
}

raw_tensor atg__values(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self)._values();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__values_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::_values_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__values_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::_values_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

int64_t atg__version(gc_tensor self) {
  PROTECT(
    return tensor_from_ocaml(self)._version();
  )
  return 0;
}

raw_tensor atg__weight_int4pack_mm(gc_tensor self, gc_tensor mat2, int64_t qGroupSize, gc_tensor qScaleAndZeros) {
  PROTECT(
    torch::Tensor results__ = torch::_weight_int4pack_mm(tensor_from_ocaml(self), tensor_from_ocaml(mat2), qGroupSize, tensor_from_ocaml(qScaleAndZeros));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__weight_int4pack_mm_for_cpu(gc_tensor self, gc_tensor mat2, int64_t qGroupSize, gc_tensor qScaleAndZeros) {
  PROTECT(
    torch::Tensor results__ = torch::_weight_int4pack_mm_for_cpu(tensor_from_ocaml(self), tensor_from_ocaml(mat2), qGroupSize, tensor_from_ocaml(qScaleAndZeros));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__weight_int8pack_mm(gc_tensor self, gc_tensor mat2, gc_tensor scales) {
  PROTECT(
    torch::Tensor results__ = torch::_weight_int8pack_mm(tensor_from_ocaml(self), tensor_from_ocaml(mat2), tensor_from_ocaml(scales));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__weight_norm(gc_tensor v, gc_tensor g, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::_weight_norm(tensor_from_ocaml(v), tensor_from_ocaml(g), dim);
    return tensor_to_ocaml(results__);
  )
}

void atg__weight_norm_differentiable_backward(raw_tensor *out__, gc_tensor grad_w, gc_tensor saved_v, gc_tensor saved_g, gc_tensor saved_norms, int64_t dim) {
  PROTECT(
    auto results__ = torch::_weight_norm_differentiable_backward(tensor_from_ocaml(grad_w), tensor_from_ocaml(saved_v), tensor_from_ocaml(saved_g), tensor_from_ocaml(saved_norms), dim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__weight_norm_interface(raw_tensor *out__, gc_tensor v, gc_tensor g, int64_t dim) {
  PROTECT(
    auto results__ = torch::_weight_norm_interface(tensor_from_ocaml(v), tensor_from_ocaml(g), dim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__weight_norm_interface_backward(raw_tensor *out__, gc_tensor grad_w, gc_tensor saved_v, gc_tensor saved_g, gc_tensor saved_norms, int64_t dim) {
  PROTECT(
    auto results__ = torch::_weight_norm_interface_backward(tensor_from_ocaml(grad_w), tensor_from_ocaml(saved_v), tensor_from_ocaml(saved_g), tensor_from_ocaml(saved_norms), dim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__weight_norm_interface_backward_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor grad_w, gc_tensor saved_v, gc_tensor saved_g, gc_tensor saved_norms, int64_t dim) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::_weight_norm_interface_backward_out(out0_local, out1_local, tensor_from_ocaml(grad_w), tensor_from_ocaml(saved_v), tensor_from_ocaml(saved_g), tensor_from_ocaml(saved_norms), dim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg__weight_norm_interface_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor v, gc_tensor g, int64_t dim) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::_weight_norm_interface_out(out0_local, out1_local, tensor_from_ocaml(v), tensor_from_ocaml(g), dim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg__wrapped_linear_prepack(gc_tensor weight, gc_tensor weight_scale, gc_tensor weight_zero_point, gc_tensor bias) {
  PROTECT(
    torch::Tensor results__ = torch::_wrapped_linear_prepack(tensor_from_ocaml(weight), tensor_from_ocaml(weight_scale), tensor_from_ocaml(weight_zero_point), tensor_from_ocaml(bias));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg__wrapped_quantized_linear_prepacked(gc_tensor input, gc_tensor input_scale, gc_tensor input_zero_point, gc_tensor packed_weight, gc_tensor output_scale, gc_tensor output_zero_point, int64_t out_channel) {
  PROTECT(
    torch::Tensor results__ = torch::_wrapped_quantized_linear_prepacked(tensor_from_ocaml(input), tensor_from_ocaml(input_scale), tensor_from_ocaml(input_zero_point), tensor_from_ocaml(packed_weight), tensor_from_ocaml(output_scale), tensor_from_ocaml(output_zero_point), out_channel);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_abs(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::abs(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_abs_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::abs_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_abs_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::abs_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_absolute(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::absolute(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_absolute_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).absolute_();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_absolute_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::absolute_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_acos(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::acos(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_acos_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::acos_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_acos_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::acos_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_acosh(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::acosh(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_acosh_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::acosh_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_acosh_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::acosh_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_adaptive_avg_pool1d(gc_tensor self, int64_t *output_size_data, int output_size_len) {
  PROTECT(
    torch::Tensor results__ = torch::adaptive_avg_pool1d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_adaptive_avg_pool1d_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::adaptive_avg_pool1d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_adaptive_avg_pool2d(gc_tensor self, int64_t *output_size_data, int output_size_len) {
  PROTECT(
    torch::Tensor results__ = torch::adaptive_avg_pool2d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_adaptive_avg_pool2d_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::adaptive_avg_pool2d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_adaptive_avg_pool3d(gc_tensor self, int64_t *output_size_data, int output_size_len) {
  PROTECT(
    torch::Tensor results__ = torch::adaptive_avg_pool3d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_adaptive_avg_pool3d_backward(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::adaptive_avg_pool3d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_adaptive_avg_pool3d_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::adaptive_avg_pool3d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len));
    return tensor_to_ocaml(results__);
  )
}

void atg_adaptive_max_pool1d(raw_tensor *out__, gc_tensor self, int64_t *output_size_data, int output_size_len) {
  PROTECT(
    auto results__ = torch::adaptive_max_pool1d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_adaptive_max_pool2d(raw_tensor *out__, gc_tensor self, int64_t *output_size_data, int output_size_len) {
  PROTECT(
    auto results__ = torch::adaptive_max_pool2d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_adaptive_max_pool2d_backward(gc_tensor grad_output, gc_tensor self, gc_tensor indices) {
  PROTECT(
    torch::Tensor results__ = torch::adaptive_max_pool2d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(indices));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_adaptive_max_pool2d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, gc_tensor indices) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::adaptive_max_pool2d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(indices));
    return tensor_to_ocaml(results__);
  )
}

void atg_adaptive_max_pool2d_out(raw_tensor *out__, gc_tensor out, gc_tensor indices, gc_tensor self, int64_t *output_size_data, int output_size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  torch::Tensor indices_local = tensor_from_ocaml(indices);
  PROTECT(
    auto results__ = torch::adaptive_max_pool2d_out(out_local, indices_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_adaptive_max_pool3d(raw_tensor *out__, gc_tensor self, int64_t *output_size_data, int output_size_len) {
  PROTECT(
    auto results__ = torch::adaptive_max_pool3d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_adaptive_max_pool3d_backward(gc_tensor grad_output, gc_tensor self, gc_tensor indices) {
  PROTECT(
    torch::Tensor results__ = torch::adaptive_max_pool3d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(indices));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_adaptive_max_pool3d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, gc_tensor indices) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::adaptive_max_pool3d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(indices));
    return tensor_to_ocaml(results__);
  )
}

void atg_adaptive_max_pool3d_out(raw_tensor *out__, gc_tensor out, gc_tensor indices, gc_tensor self, int64_t *output_size_data, int output_size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  torch::Tensor indices_local = tensor_from_ocaml(indices);
  PROTECT(
    auto results__ = torch::adaptive_max_pool3d_out(out_local, indices_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_add(gc_tensor self, gc_tensor other, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::add(tensor_from_ocaml(self), tensor_from_ocaml(other), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_add_(gc_tensor self, gc_tensor other, scalar alpha) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).add_(tensor_from_ocaml(other), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_add_out(gc_tensor out, gc_tensor self, gc_tensor other, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::add_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_add_scalar(gc_tensor self, scalar other, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::add(tensor_from_ocaml(self), *other, alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_add_scalar_(gc_tensor self, scalar other, scalar alpha) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).add_(*other, alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_add_scalar_out(gc_tensor out, gc_tensor self, scalar other, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::add_out(out_local, tensor_from_ocaml(self), *other, alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addbmm(gc_tensor self, gc_tensor batch1, gc_tensor batch2, scalar beta, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::addbmm(tensor_from_ocaml(self), tensor_from_ocaml(batch1), tensor_from_ocaml(batch2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addbmm_(gc_tensor self, gc_tensor batch1, gc_tensor batch2, scalar beta, scalar alpha) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).addbmm_(tensor_from_ocaml(batch1), tensor_from_ocaml(batch2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addbmm_out(gc_tensor out, gc_tensor self, gc_tensor batch1, gc_tensor batch2, scalar beta, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::addbmm_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(batch1), tensor_from_ocaml(batch2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addcdiv(gc_tensor self, gc_tensor tensor1, gc_tensor tensor2, scalar value) {
  PROTECT(
    torch::Tensor results__ = torch::addcdiv(tensor_from_ocaml(self), tensor_from_ocaml(tensor1), tensor_from_ocaml(tensor2), value ? *value : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addcdiv_(gc_tensor self, gc_tensor tensor1, gc_tensor tensor2, scalar value) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).addcdiv_(tensor_from_ocaml(tensor1), tensor_from_ocaml(tensor2), value ? *value : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addcdiv_out(gc_tensor out, gc_tensor self, gc_tensor tensor1, gc_tensor tensor2, scalar value) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::addcdiv_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(tensor1), tensor_from_ocaml(tensor2), value ? *value : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addcmul(gc_tensor self, gc_tensor tensor1, gc_tensor tensor2, scalar value) {
  PROTECT(
    torch::Tensor results__ = torch::addcmul(tensor_from_ocaml(self), tensor_from_ocaml(tensor1), tensor_from_ocaml(tensor2), value ? *value : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addcmul_(gc_tensor self, gc_tensor tensor1, gc_tensor tensor2, scalar value) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).addcmul_(tensor_from_ocaml(tensor1), tensor_from_ocaml(tensor2), value ? *value : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addcmul_out(gc_tensor out, gc_tensor self, gc_tensor tensor1, gc_tensor tensor2, scalar value) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::addcmul_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(tensor1), tensor_from_ocaml(tensor2), value ? *value : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addmm(gc_tensor self, gc_tensor mat1, gc_tensor mat2, scalar beta, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::addmm(tensor_from_ocaml(self), tensor_from_ocaml(mat1), tensor_from_ocaml(mat2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addmm_(gc_tensor self, gc_tensor mat1, gc_tensor mat2, scalar beta, scalar alpha) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).addmm_(tensor_from_ocaml(mat1), tensor_from_ocaml(mat2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addmm_out(gc_tensor out, gc_tensor self, gc_tensor mat1, gc_tensor mat2, scalar beta, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::addmm_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mat1), tensor_from_ocaml(mat2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addmv(gc_tensor self, gc_tensor mat, gc_tensor vec, scalar beta, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::addmv(tensor_from_ocaml(self), tensor_from_ocaml(mat), tensor_from_ocaml(vec), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addmv_(gc_tensor self, gc_tensor mat, gc_tensor vec, scalar beta, scalar alpha) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::addmv_(self_local, tensor_from_ocaml(mat), tensor_from_ocaml(vec), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addmv_out(gc_tensor out, gc_tensor self, gc_tensor mat, gc_tensor vec, scalar beta, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::addmv_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mat), tensor_from_ocaml(vec), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addr(gc_tensor self, gc_tensor vec1, gc_tensor vec2, scalar beta, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::addr(tensor_from_ocaml(self), tensor_from_ocaml(vec1), tensor_from_ocaml(vec2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addr_(gc_tensor self, gc_tensor vec1, gc_tensor vec2, scalar beta, scalar alpha) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).addr_(tensor_from_ocaml(vec1), tensor_from_ocaml(vec2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_addr_out(gc_tensor out, gc_tensor self, gc_tensor vec1, gc_tensor vec2, scalar beta, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::addr_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(vec1), tensor_from_ocaml(vec2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_adjoint(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::adjoint(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_affine_grid_generator(gc_tensor theta, int64_t *size_data, int size_len, int align_corners) {
  PROTECT(
    torch::Tensor results__ = torch::affine_grid_generator(tensor_from_ocaml(theta), torch::IntArrayRef(size_data, size_len), (bool)align_corners);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_affine_grid_generator_backward(gc_tensor grad, int64_t *size_data, int size_len, int align_corners) {
  PROTECT(
    torch::Tensor results__ = torch::affine_grid_generator_backward(tensor_from_ocaml(grad), torch::IntArrayRef(size_data, size_len), (bool)align_corners);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_affine_grid_generator_out(gc_tensor out, gc_tensor theta, int64_t *size_data, int size_len, int align_corners) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::affine_grid_generator_out(out_local, tensor_from_ocaml(theta), torch::IntArrayRef(size_data, size_len), (bool)align_corners);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_alias(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::alias(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_alias_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::alias_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_alias_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::alias_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_align_as(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).align_as(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg_align_tensors(gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    auto results__ = torch::align_tensors(of_carray_tensor(tensors_data, tensors_len));
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor atg_all(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::all(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_all_all_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::all_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_all_dim(gc_tensor self, int64_t dim, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::all(tensor_from_ocaml(self), dim, (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_all_dims(gc_tensor self, int64_t *dim_data, int dim_len, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::all(tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_all_dims_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::all_out(out_local, tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_all_out(gc_tensor out, gc_tensor self, int64_t dim, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::all_out(out_local, tensor_from_ocaml(self), dim, (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

int atg_allclose(gc_tensor self, gc_tensor other, double rtol, double atol, int equal_nan) {
  PROTECT(
    return torch::allclose(tensor_from_ocaml(self), tensor_from_ocaml(other), rtol, atol, (bool)equal_nan);
  )
  return 0;
}

raw_tensor atg_alpha_dropout(gc_tensor input, double p, int train) {
  PROTECT(
    torch::Tensor results__ = torch::alpha_dropout(tensor_from_ocaml(input), p, (bool)train);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_alpha_dropout_(gc_tensor self, double p, int train) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::alpha_dropout_(self_local, p, (bool)train);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_amax(gc_tensor self, int64_t *dim_data, int dim_len, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::amax(tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_amax_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::amax_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_amin(gc_tensor self, int64_t *dim_data, int dim_len, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::amin(tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_amin_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::amin_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

void atg_aminmax(raw_tensor *out__, gc_tensor self, int64_t dim_v, int dim_null, int keepdim) {
  PROTECT(
    auto results__ = torch::aminmax(tensor_from_ocaml(self), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_aminmax_out(raw_tensor *out__, gc_tensor min, gc_tensor max, gc_tensor self, int64_t dim_v, int dim_null, int keepdim) {
  torch::Tensor min_local = tensor_from_ocaml(min);
  torch::Tensor max_local = tensor_from_ocaml(max);
  PROTECT(
    auto results__ = torch::aminmax_out(min_local, max_local, tensor_from_ocaml(self), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_angle(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::angle(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_angle_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::angle_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_any(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::any(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_any_all_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::any_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_any_dim(gc_tensor self, int64_t dim, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::any(tensor_from_ocaml(self), dim, (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_any_dims(gc_tensor self, int64_t *dim_data, int dim_len, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::any(tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_any_dims_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::any_out(out_local, tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_any_out(gc_tensor out, gc_tensor self, int64_t dim, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::any_out(out_local, tensor_from_ocaml(self), dim, (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arange(scalar end, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::arange(*end, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arange_start(scalar start, scalar end, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::arange(*start, *end, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arange_start_step(scalar start, scalar end, scalar step, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::arange(*start, *end, step ? *step : c10::Scalar{1} , at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arccos(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::arccos(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arccos_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::arccos_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arccos_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::arccos_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arccosh(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::arccosh(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arccosh_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::arccosh_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arccosh_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::arccosh_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arcsin(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::arcsin(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arcsin_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::arcsin_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arcsin_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::arcsin_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arcsinh(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::arcsinh(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arcsinh_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::arcsinh_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arcsinh_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::arcsinh_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arctan(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::arctan(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arctan2(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::arctan2(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arctan2_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).arctan2_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arctan2_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::arctan2_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arctan_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::arctan_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arctan_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::arctan_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arctanh(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::arctanh(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arctanh_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::arctanh_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_arctanh_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::arctanh_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_argmax(gc_tensor self, int64_t dim_v, int dim_null, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::argmax(tensor_from_ocaml(self), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_argmax_out(gc_tensor out, gc_tensor self, int64_t dim_v, int dim_null, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::argmax_out(out_local, tensor_from_ocaml(self), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_argmin(gc_tensor self, int64_t dim_v, int dim_null, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::argmin(tensor_from_ocaml(self), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_argmin_out(gc_tensor out, gc_tensor self, int64_t dim_v, int dim_null, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::argmin_out(out_local, tensor_from_ocaml(self), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_argsort(gc_tensor self, int64_t dim, int descending) {
  PROTECT(
    torch::Tensor results__ = torch::argsort(tensor_from_ocaml(self), dim, (bool)descending);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_argsort_stable(gc_tensor self, int stable, int64_t dim, int descending) {
  PROTECT(
    torch::Tensor results__ = torch::argsort(tensor_from_ocaml(self), (bool)stable, dim, (bool)descending);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_argsort_stable_out(gc_tensor out, gc_tensor self, int stable, int64_t dim, int descending) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::argsort_out(out_local, tensor_from_ocaml(self), (bool)stable, dim, (bool)descending);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_argwhere(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::argwhere(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_as_strided(gc_tensor self, int64_t *size_data, int size_len, int64_t *stride_data, int stride_len, int64_t storage_offset_v, int storage_offset_null) {
  PROTECT(
    torch::Tensor results__ = torch::as_strided(tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), torch::IntArrayRef(stride_data, stride_len), storage_offset_null ? c10::nullopt : c10::optional<int64_t>(storage_offset_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_as_strided_(gc_tensor self, int64_t *size_data, int size_len, int64_t *stride_data, int stride_len, int64_t storage_offset_v, int storage_offset_null) {
  PROTECT(
    torch::Tensor results__ = torch::as_strided_(tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), torch::IntArrayRef(stride_data, stride_len), storage_offset_null ? c10::nullopt : c10::optional<int64_t>(storage_offset_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_as_strided_copy(gc_tensor self, int64_t *size_data, int size_len, int64_t *stride_data, int stride_len, int64_t storage_offset_v, int storage_offset_null) {
  PROTECT(
    torch::Tensor results__ = torch::as_strided_copy(tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), torch::IntArrayRef(stride_data, stride_len), storage_offset_null ? c10::nullopt : c10::optional<int64_t>(storage_offset_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_as_strided_copy_out(gc_tensor out, gc_tensor self, int64_t *size_data, int size_len, int64_t *stride_data, int stride_len, int64_t storage_offset_v, int storage_offset_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::as_strided_copy_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), torch::IntArrayRef(stride_data, stride_len), storage_offset_null ? c10::nullopt : c10::optional<int64_t>(storage_offset_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_as_strided_scatter(gc_tensor self, gc_tensor src, int64_t *size_data, int size_len, int64_t *stride_data, int stride_len, int64_t storage_offset_v, int storage_offset_null) {
  PROTECT(
    torch::Tensor results__ = torch::as_strided_scatter(tensor_from_ocaml(self), tensor_from_ocaml(src), torch::IntArrayRef(size_data, size_len), torch::IntArrayRef(stride_data, stride_len), storage_offset_null ? c10::nullopt : c10::optional<int64_t>(storage_offset_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_as_strided_scatter_out(gc_tensor out, gc_tensor self, gc_tensor src, int64_t *size_data, int size_len, int64_t *stride_data, int stride_len, int64_t storage_offset_v, int storage_offset_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::as_strided_scatter_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(src), torch::IntArrayRef(size_data, size_len), torch::IntArrayRef(stride_data, stride_len), storage_offset_null ? c10::nullopt : c10::optional<int64_t>(storage_offset_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_asin(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::asin(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_asin_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::asin_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_asin_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::asin_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_asinh(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::asinh(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_asinh_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::asinh_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_asinh_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::asinh_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_atan(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::atan(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_atan2(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::atan2(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_atan2_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).atan2_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_atan2_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::atan2_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_atan_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::atan_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_atan_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::atan_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_atanh(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::atanh(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_atanh_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::atanh_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_atanh_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::atanh_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_atleast_1d(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::atleast_1d(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg_atleast_1d_sequence(gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    auto results__ = torch::atleast_1d(of_carray_tensor(tensors_data, tensors_len));
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor atg_atleast_2d(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::atleast_2d(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg_atleast_2d_sequence(gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    auto results__ = torch::atleast_2d(of_carray_tensor(tensors_data, tensors_len));
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor atg_atleast_3d(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::atleast_3d(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg_atleast_3d_sequence(gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    auto results__ = torch::atleast_3d(of_carray_tensor(tensors_data, tensors_len));
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor atg_avg_pool1d(gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int ceil_mode, int count_include_pad) {
  PROTECT(
    torch::Tensor results__ = torch::avg_pool1d(tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), (bool)ceil_mode, (bool)count_include_pad);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_avg_pool1d_out(gc_tensor out, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int ceil_mode, int count_include_pad) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::avg_pool1d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), (bool)ceil_mode, (bool)count_include_pad);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_avg_pool2d(gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int ceil_mode, int count_include_pad, int64_t divisor_override_v, int divisor_override_null) {
  PROTECT(
    torch::Tensor results__ = torch::avg_pool2d(tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), (bool)ceil_mode, (bool)count_include_pad, divisor_override_null ? c10::nullopt : c10::optional<int64_t>(divisor_override_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_avg_pool2d_backward(gc_tensor grad_output, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int ceil_mode, int count_include_pad, int64_t divisor_override_v, int divisor_override_null) {
  PROTECT(
    torch::Tensor results__ = torch::avg_pool2d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), (bool)ceil_mode, (bool)count_include_pad, divisor_override_null ? c10::nullopt : c10::optional<int64_t>(divisor_override_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_avg_pool2d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int ceil_mode, int count_include_pad, int64_t divisor_override_v, int divisor_override_null) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::avg_pool2d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), (bool)ceil_mode, (bool)count_include_pad, divisor_override_null ? c10::nullopt : c10::optional<int64_t>(divisor_override_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_avg_pool2d_out(gc_tensor out, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int ceil_mode, int count_include_pad, int64_t divisor_override_v, int divisor_override_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::avg_pool2d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), (bool)ceil_mode, (bool)count_include_pad, divisor_override_null ? c10::nullopt : c10::optional<int64_t>(divisor_override_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_avg_pool3d(gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int ceil_mode, int count_include_pad, int64_t divisor_override_v, int divisor_override_null) {
  PROTECT(
    torch::Tensor results__ = torch::avg_pool3d(tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), (bool)ceil_mode, (bool)count_include_pad, divisor_override_null ? c10::nullopt : c10::optional<int64_t>(divisor_override_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_avg_pool3d_backward(gc_tensor grad_output, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int ceil_mode, int count_include_pad, int64_t divisor_override_v, int divisor_override_null) {
  PROTECT(
    torch::Tensor results__ = torch::avg_pool3d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), (bool)ceil_mode, (bool)count_include_pad, divisor_override_null ? c10::nullopt : c10::optional<int64_t>(divisor_override_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_avg_pool3d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int ceil_mode, int count_include_pad, int64_t divisor_override_v, int divisor_override_null) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::avg_pool3d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), (bool)ceil_mode, (bool)count_include_pad, divisor_override_null ? c10::nullopt : c10::optional<int64_t>(divisor_override_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_avg_pool3d_out(gc_tensor out, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int ceil_mode, int count_include_pad, int64_t divisor_override_v, int divisor_override_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::avg_pool3d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), (bool)ceil_mode, (bool)count_include_pad, divisor_override_null ? c10::nullopt : c10::optional<int64_t>(divisor_override_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_baddbmm(gc_tensor self, gc_tensor batch1, gc_tensor batch2, scalar beta, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::baddbmm(tensor_from_ocaml(self), tensor_from_ocaml(batch1), tensor_from_ocaml(batch2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_baddbmm_(gc_tensor self, gc_tensor batch1, gc_tensor batch2, scalar beta, scalar alpha) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).baddbmm_(tensor_from_ocaml(batch1), tensor_from_ocaml(batch2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_baddbmm_out(gc_tensor out, gc_tensor self, gc_tensor batch1, gc_tensor batch2, scalar beta, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::baddbmm_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(batch1), tensor_from_ocaml(batch2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bartlett_window(int64_t window_length, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::bartlett_window(window_length, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bartlett_window_out(gc_tensor out, int64_t window_length) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bartlett_window_out(out_local, window_length);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bartlett_window_periodic(int64_t window_length, int periodic, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::bartlett_window(window_length, (bool)periodic, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bartlett_window_periodic_out(gc_tensor out, int64_t window_length, int periodic) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bartlett_window_out(out_local, window_length, (bool)periodic);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_batch_norm(gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, int training, double momentum, double eps, int cudnn_enabled) {
  PROTECT(
    torch::Tensor results__ = torch::batch_norm(tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, (bool)training, momentum, eps, (bool)cudnn_enabled);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_batch_norm_backward_elemt(gc_tensor grad_out, gc_tensor input, gc_tensor mean, gc_tensor invstd, gc_tensor weight, gc_tensor sum_dy, gc_tensor sum_dy_xmu, gc_tensor count) {
  PROTECT(
    torch::Tensor results__ = torch::batch_norm_backward_elemt(tensor_from_ocaml(grad_out), tensor_from_ocaml(input), tensor_from_ocaml(mean), tensor_from_ocaml(invstd), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, tensor_from_ocaml(sum_dy), tensor_from_ocaml(sum_dy_xmu), tensor_from_ocaml(count));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_batch_norm_backward_elemt_out(gc_tensor out, gc_tensor grad_out, gc_tensor input, gc_tensor mean, gc_tensor invstd, gc_tensor weight, gc_tensor sum_dy, gc_tensor sum_dy_xmu, gc_tensor count) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::batch_norm_backward_elemt_out(out_local, tensor_from_ocaml(grad_out), tensor_from_ocaml(input), tensor_from_ocaml(mean), tensor_from_ocaml(invstd), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, tensor_from_ocaml(sum_dy), tensor_from_ocaml(sum_dy_xmu), tensor_from_ocaml(count));
    return tensor_to_ocaml(results__);
  )
}

void atg_batch_norm_backward_reduce(raw_tensor *out__, gc_tensor grad_out, gc_tensor input, gc_tensor mean, gc_tensor invstd, gc_tensor weight, int input_g, int weight_g, int bias_g) {
  PROTECT(
    auto results__ = torch::batch_norm_backward_reduce(tensor_from_ocaml(grad_out), tensor_from_ocaml(input), tensor_from_ocaml(mean), tensor_from_ocaml(invstd), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, (bool)input_g, (bool)weight_g, (bool)bias_g);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

void atg_batch_norm_backward_reduce_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor out3, gc_tensor grad_out, gc_tensor input, gc_tensor mean, gc_tensor invstd, gc_tensor weight, int input_g, int weight_g, int bias_g) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  torch::Tensor out3_local = tensor_from_ocaml(out3);
  PROTECT(
    auto results__ = torch::batch_norm_backward_reduce_out(out0_local, out1_local, out2_local, out3_local, tensor_from_ocaml(grad_out), tensor_from_ocaml(input), tensor_from_ocaml(mean), tensor_from_ocaml(invstd), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, (bool)input_g, (bool)weight_g, (bool)bias_g);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

raw_tensor atg_batch_norm_elemt(gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor mean, gc_tensor invstd, double eps) {
  PROTECT(
    torch::Tensor results__ = torch::batch_norm_elemt(tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, tensor_from_ocaml(mean), tensor_from_ocaml(invstd), eps);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_batch_norm_elemt_out(gc_tensor out, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor mean, gc_tensor invstd, double eps) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::batch_norm_elemt_out(out_local, tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, tensor_from_ocaml(mean), tensor_from_ocaml(invstd), eps);
    return tensor_to_ocaml(results__);
  )
}

void atg_batch_norm_gather_stats(raw_tensor *out__, gc_tensor input, gc_tensor mean, gc_tensor invstd, gc_tensor running_mean, gc_tensor running_var, double momentum, double eps, int64_t count) {
  PROTECT(
    auto results__ = torch::batch_norm_gather_stats(tensor_from_ocaml(input), tensor_from_ocaml(mean), tensor_from_ocaml(invstd), running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, momentum, eps, count);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_batch_norm_gather_stats_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor input, gc_tensor mean, gc_tensor invstd, gc_tensor running_mean, gc_tensor running_var, double momentum, double eps, int64_t count) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::batch_norm_gather_stats_out(out0_local, out1_local, tensor_from_ocaml(input), tensor_from_ocaml(mean), tensor_from_ocaml(invstd), running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, momentum, eps, count);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_batch_norm_gather_stats_with_counts(raw_tensor *out__, gc_tensor input, gc_tensor mean, gc_tensor invstd, gc_tensor running_mean, gc_tensor running_var, double momentum, double eps, gc_tensor counts) {
  PROTECT(
    auto results__ = torch::batch_norm_gather_stats_with_counts(tensor_from_ocaml(input), tensor_from_ocaml(mean), tensor_from_ocaml(invstd), running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, momentum, eps, tensor_from_ocaml(counts));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_batch_norm_gather_stats_with_counts_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor input, gc_tensor mean, gc_tensor invstd, gc_tensor running_mean, gc_tensor running_var, double momentum, double eps, gc_tensor counts) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::batch_norm_gather_stats_with_counts_out(out0_local, out1_local, tensor_from_ocaml(input), tensor_from_ocaml(mean), tensor_from_ocaml(invstd), running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, momentum, eps, tensor_from_ocaml(counts));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_batch_norm_stats(raw_tensor *out__, gc_tensor input, double eps) {
  PROTECT(
    auto results__ = torch::batch_norm_stats(tensor_from_ocaml(input), eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_batch_norm_stats_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor input, double eps) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::batch_norm_stats_out(out0_local, out1_local, tensor_from_ocaml(input), eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_batch_norm_update_stats(raw_tensor *out__, gc_tensor input, gc_tensor running_mean, gc_tensor running_var, double momentum) {
  PROTECT(
    auto results__ = torch::batch_norm_update_stats(tensor_from_ocaml(input), running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, momentum);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_batch_norm_update_stats_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor input, gc_tensor running_mean, gc_tensor running_var, double momentum) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::batch_norm_update_stats_out(out0_local, out1_local, tensor_from_ocaml(input), running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, momentum);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_bernoulli(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::bernoulli(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bernoulli_(gc_tensor self, gc_tensor p) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).bernoulli_(tensor_from_ocaml(p));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bernoulli_float_(gc_tensor self, double p) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).bernoulli_(p);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bernoulli_p(gc_tensor self, double p) {
  PROTECT(
    torch::Tensor results__ = torch::bernoulli(tensor_from_ocaml(self), p);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bernoulli_tensor(gc_tensor self, gc_tensor p) {
  PROTECT(
    torch::Tensor results__ = torch::bernoulli(tensor_from_ocaml(self), tensor_from_ocaml(p));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bilinear(gc_tensor input1, gc_tensor input2, gc_tensor weight, gc_tensor bias) {
  PROTECT(
    torch::Tensor results__ = torch::bilinear(tensor_from_ocaml(input1), tensor_from_ocaml(input2), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_binary_cross_entropy(gc_tensor self, gc_tensor target, gc_tensor weight, int64_t reduction) {
  PROTECT(
    torch::Tensor results__ = torch::binary_cross_entropy(tensor_from_ocaml(self), tensor_from_ocaml(target), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_binary_cross_entropy_backward(gc_tensor grad_output, gc_tensor self, gc_tensor target, gc_tensor weight, int64_t reduction) {
  PROTECT(
    torch::Tensor results__ = torch::binary_cross_entropy_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_binary_cross_entropy_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, gc_tensor target, gc_tensor weight, int64_t reduction) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::binary_cross_entropy_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_binary_cross_entropy_out(gc_tensor out, gc_tensor self, gc_tensor target, gc_tensor weight, int64_t reduction) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::binary_cross_entropy_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(target), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_binary_cross_entropy_with_logits(gc_tensor self, gc_tensor target, gc_tensor weight, gc_tensor pos_weight, int64_t reduction) {
  PROTECT(
    torch::Tensor results__ = torch::binary_cross_entropy_with_logits(tensor_from_ocaml(self), tensor_from_ocaml(target), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, pos_weight ? std::make_optional(tensor_from_ocaml(pos_weight)) : std::nullopt, reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_binary_cross_entropy_with_logits_out(gc_tensor out, gc_tensor self, gc_tensor target, gc_tensor weight, gc_tensor pos_weight, int64_t reduction) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::binary_cross_entropy_with_logits_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(target), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, pos_weight ? std::make_optional(tensor_from_ocaml(pos_weight)) : std::nullopt, reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bincount(gc_tensor self, gc_tensor weights, int64_t minlength) {
  PROTECT(
    torch::Tensor results__ = torch::bincount(tensor_from_ocaml(self), weights ? std::make_optional(tensor_from_ocaml(weights)) : std::nullopt, minlength);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bincount_out(gc_tensor out, gc_tensor self, gc_tensor weights, int64_t minlength) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bincount_out(out_local, tensor_from_ocaml(self), weights ? std::make_optional(tensor_from_ocaml(weights)) : std::nullopt, minlength);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_binomial(gc_tensor count, gc_tensor prob) {
  PROTECT(
    torch::Tensor results__ = torch::binomial(tensor_from_ocaml(count), tensor_from_ocaml(prob));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_binomial_out(gc_tensor out, gc_tensor count, gc_tensor prob) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::binomial_out(out_local, tensor_from_ocaml(count), tensor_from_ocaml(prob));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_and(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::bitwise_and(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_and_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).bitwise_and_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_and_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bitwise_and_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_and_scalar_tensor(scalar self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::bitwise_and(*self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_and_scalar_tensor_out(gc_tensor out, scalar self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bitwise_and_out(out_local, *self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_and_tensor(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::bitwise_and(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_and_tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).bitwise_and_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_and_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bitwise_and_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_left_shift(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::bitwise_left_shift(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_left_shift_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).bitwise_left_shift_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_left_shift_scalar_tensor(scalar self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::bitwise_left_shift(*self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_left_shift_scalar_tensor_out(gc_tensor out, scalar self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bitwise_left_shift_out(out_local, *self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_left_shift_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bitwise_left_shift_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_left_shift_tensor_scalar(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::bitwise_left_shift(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_left_shift_tensor_scalar_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).bitwise_left_shift_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_left_shift_tensor_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bitwise_left_shift_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_not(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::bitwise_not(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_not_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).bitwise_not_();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_not_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bitwise_not_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_or(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::bitwise_or(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_or_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).bitwise_or_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_or_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bitwise_or_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_or_scalar_tensor(scalar self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::bitwise_or(*self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_or_scalar_tensor_out(gc_tensor out, scalar self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bitwise_or_out(out_local, *self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_or_tensor(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::bitwise_or(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_or_tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).bitwise_or_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_or_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bitwise_or_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_right_shift(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::bitwise_right_shift(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_right_shift_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).bitwise_right_shift_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_right_shift_scalar_tensor(scalar self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::bitwise_right_shift(*self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_right_shift_scalar_tensor_out(gc_tensor out, scalar self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bitwise_right_shift_out(out_local, *self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_right_shift_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bitwise_right_shift_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_right_shift_tensor_scalar(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::bitwise_right_shift(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_right_shift_tensor_scalar_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).bitwise_right_shift_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_right_shift_tensor_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bitwise_right_shift_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_xor(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::bitwise_xor(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_xor_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).bitwise_xor_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_xor_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bitwise_xor_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_xor_scalar_tensor(scalar self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::bitwise_xor(*self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_xor_scalar_tensor_out(gc_tensor out, scalar self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bitwise_xor_out(out_local, *self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_xor_tensor(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::bitwise_xor(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_xor_tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).bitwise_xor_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bitwise_xor_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bitwise_xor_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_blackman_window(int64_t window_length, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::blackman_window(window_length, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_blackman_window_out(gc_tensor out, int64_t window_length) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::blackman_window_out(out_local, window_length);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_blackman_window_periodic(int64_t window_length, int periodic, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::blackman_window(window_length, (bool)periodic, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_blackman_window_periodic_out(gc_tensor out, int64_t window_length, int periodic) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::blackman_window_out(out_local, window_length, (bool)periodic);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_block_diag(gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    torch::Tensor results__ = torch::block_diag(of_carray_tensor(tensors_data, tensors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_block_diag_out(gc_tensor out, gc_tensor *tensors_data, int tensors_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::block_diag_out(out_local, of_carray_tensor(tensors_data, tensors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bmm(gc_tensor self, gc_tensor mat2) {
  PROTECT(
    torch::Tensor results__ = torch::bmm(tensor_from_ocaml(self), tensor_from_ocaml(mat2));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bmm_out(gc_tensor out, gc_tensor self, gc_tensor mat2) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bmm_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mat2));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg_broadcast_tensors(gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    auto results__ = torch::broadcast_tensors(of_carray_tensor(tensors_data, tensors_len));
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor atg_broadcast_to(gc_tensor self, int64_t *size_data, int size_len) {
  PROTECT(
    torch::Tensor results__ = torch::broadcast_to(tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bucketize(gc_tensor self, gc_tensor boundaries, int out_int32, int right) {
  PROTECT(
    torch::Tensor results__ = torch::bucketize(tensor_from_ocaml(self), tensor_from_ocaml(boundaries), (bool)out_int32, (bool)right);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bucketize_scalar(scalar self, gc_tensor boundaries, int out_int32, int right) {
  PROTECT(
    torch::Tensor results__ = torch::bucketize(*self, tensor_from_ocaml(boundaries), (bool)out_int32, (bool)right);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bucketize_scalar_out(gc_tensor out, scalar self, gc_tensor boundaries, int out_int32, int right) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bucketize_out(out_local, *self, tensor_from_ocaml(boundaries), (bool)out_int32, (bool)right);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_bucketize_tensor_out(gc_tensor out, gc_tensor self, gc_tensor boundaries, int out_int32, int right) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::bucketize_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(boundaries), (bool)out_int32, (bool)right);
    return tensor_to_ocaml(results__);
  )
}

int atg_can_cast(int from_, int to) {
  PROTECT(
    return torch::can_cast(torch::ScalarType(from_), torch::ScalarType(to));
  )
  return 0;
}

raw_tensor atg_cartesian_prod(gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    torch::Tensor results__ = torch::cartesian_prod(of_carray_tensor(tensors_data, tensors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cat(gc_tensor *tensors_data, int tensors_len, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::cat(of_carray_tensor(tensors_data, tensors_len), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cat_out(gc_tensor out, gc_tensor *tensors_data, int tensors_len, int64_t dim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::cat_out(out_local, of_carray_tensor(tensors_data, tensors_len), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cauchy(gc_tensor self, double median, double sigma) {
  PROTECT(
    torch::Tensor results__ = torch::cauchy(tensor_from_ocaml(self), median, sigma);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cauchy_(gc_tensor self, double median, double sigma) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).cauchy_(median, sigma);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cauchy_out(gc_tensor out, gc_tensor self, double median, double sigma) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::cauchy_out(out_local, tensor_from_ocaml(self), median, sigma);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ccol_indices(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).ccol_indices();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ccol_indices_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::ccol_indices_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ccol_indices_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::ccol_indices_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cdist(gc_tensor x1, gc_tensor x2, double p, int64_t compute_mode_v, int compute_mode_null) {
  PROTECT(
    torch::Tensor results__ = torch::cdist(tensor_from_ocaml(x1), tensor_from_ocaml(x2), p, compute_mode_null ? c10::nullopt : c10::optional<int64_t>(compute_mode_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ceil(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::ceil(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ceil_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::ceil_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ceil_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::ceil_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_celu(gc_tensor self, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::celu(tensor_from_ocaml(self), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_celu_(gc_tensor self, scalar alpha) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::celu_(self_local, alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_celu_out(gc_tensor out, gc_tensor self, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::celu_out(out_local, tensor_from_ocaml(self), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_chain_matmul(gc_tensor *matrices_data, int matrices_len) {
  PROTECT(
    torch::Tensor results__ = torch::chain_matmul(of_carray_tensor(matrices_data, matrices_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_chain_matmul_out(gc_tensor out, gc_tensor *matrices_data, int matrices_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::chain_matmul_out(out_local, of_carray_tensor(matrices_data, matrices_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_chalf(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).chalf();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_channel_shuffle(gc_tensor self, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::channel_shuffle(tensor_from_ocaml(self), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_channel_shuffle_out(gc_tensor out, gc_tensor self, int64_t groups) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::channel_shuffle_out(out_local, tensor_from_ocaml(self), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cholesky(gc_tensor self, int upper) {
  PROTECT(
    torch::Tensor results__ = torch::cholesky(tensor_from_ocaml(self), (bool)upper);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cholesky_inverse(gc_tensor self, int upper) {
  PROTECT(
    torch::Tensor results__ = torch::cholesky_inverse(tensor_from_ocaml(self), (bool)upper);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cholesky_inverse_out(gc_tensor out, gc_tensor self, int upper) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::cholesky_inverse_out(out_local, tensor_from_ocaml(self), (bool)upper);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cholesky_out(gc_tensor out, gc_tensor self, int upper) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::cholesky_out(out_local, tensor_from_ocaml(self), (bool)upper);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cholesky_solve(gc_tensor self, gc_tensor input2, int upper) {
  PROTECT(
    torch::Tensor results__ = torch::cholesky_solve(tensor_from_ocaml(self), tensor_from_ocaml(input2), (bool)upper);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cholesky_solve_out(gc_tensor out, gc_tensor self, gc_tensor input2, int upper) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::cholesky_solve_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(input2), (bool)upper);
    return tensor_to_ocaml(results__);
  )
}

void atg_choose_qparams_optimized(raw_tensor *out__, gc_tensor input, int64_t numel, int64_t n_bins, double ratio, int64_t bit_width) {
  PROTECT(
    auto results__ = torch::choose_qparams_optimized(tensor_from_ocaml(input), numel, n_bins, ratio, bit_width);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor *atg_chunk(gc_tensor self, int64_t chunks, int64_t dim) {
  PROTECT(
    auto results__ = torch::chunk(tensor_from_ocaml(self), chunks, dim);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor atg_clamp(gc_tensor self, scalar min, scalar max) {
  PROTECT(
    torch::Tensor results__ = torch::clamp(tensor_from_ocaml(self), *min, *max);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clamp_(gc_tensor self, scalar min, scalar max) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::clamp_(self_local, *min, *max);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clamp_max(gc_tensor self, scalar max) {
  PROTECT(
    torch::Tensor results__ = torch::clamp_max(tensor_from_ocaml(self), *max);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clamp_max_(gc_tensor self, scalar max) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::clamp_max_(self_local, *max);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clamp_max_out(gc_tensor out, gc_tensor self, scalar max) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::clamp_max_out(out_local, tensor_from_ocaml(self), *max);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clamp_max_tensor(gc_tensor self, gc_tensor max) {
  PROTECT(
    torch::Tensor results__ = torch::clamp_max(tensor_from_ocaml(self), tensor_from_ocaml(max));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clamp_max_tensor_(gc_tensor self, gc_tensor max) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::clamp_max_(self_local, tensor_from_ocaml(max));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clamp_max_tensor_out(gc_tensor out, gc_tensor self, gc_tensor max) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::clamp_max_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(max));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clamp_min(gc_tensor self, scalar min) {
  PROTECT(
    torch::Tensor results__ = torch::clamp_min(tensor_from_ocaml(self), *min);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clamp_min_(gc_tensor self, scalar min) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::clamp_min_(self_local, *min);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clamp_min_out(gc_tensor out, gc_tensor self, scalar min) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::clamp_min_out(out_local, tensor_from_ocaml(self), *min);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clamp_min_tensor(gc_tensor self, gc_tensor min) {
  PROTECT(
    torch::Tensor results__ = torch::clamp_min(tensor_from_ocaml(self), tensor_from_ocaml(min));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clamp_min_tensor_(gc_tensor self, gc_tensor min) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::clamp_min_(self_local, tensor_from_ocaml(min));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clamp_min_tensor_out(gc_tensor out, gc_tensor self, gc_tensor min) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::clamp_min_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(min));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clamp_out(gc_tensor out, gc_tensor self, scalar min, scalar max) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::clamp_out(out_local, tensor_from_ocaml(self), *min, *max);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clamp_tensor(gc_tensor self, gc_tensor min, gc_tensor max) {
  PROTECT(
    torch::Tensor results__ = torch::clamp(tensor_from_ocaml(self), min ? std::make_optional(tensor_from_ocaml(min)) : std::nullopt, max ? std::make_optional(tensor_from_ocaml(max)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clamp_tensor_(gc_tensor self, gc_tensor min, gc_tensor max) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::clamp_(self_local, min ? std::make_optional(tensor_from_ocaml(min)) : std::nullopt, max ? std::make_optional(tensor_from_ocaml(max)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clamp_tensor_out(gc_tensor out, gc_tensor self, gc_tensor min, gc_tensor max) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::clamp_out(out_local, tensor_from_ocaml(self), min ? std::make_optional(tensor_from_ocaml(min)) : std::nullopt, max ? std::make_optional(tensor_from_ocaml(max)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clip(gc_tensor self, scalar min, scalar max) {
  PROTECT(
    torch::Tensor results__ = torch::clip(tensor_from_ocaml(self), *min, *max);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clip_(gc_tensor self, scalar min, scalar max) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::clip_(self_local, *min, *max);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clip_out(gc_tensor out, gc_tensor self, scalar min, scalar max) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::clip_out(out_local, tensor_from_ocaml(self), *min, *max);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clip_tensor(gc_tensor self, gc_tensor min, gc_tensor max) {
  PROTECT(
    torch::Tensor results__ = torch::clip(tensor_from_ocaml(self), min ? std::make_optional(tensor_from_ocaml(min)) : std::nullopt, max ? std::make_optional(tensor_from_ocaml(max)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clip_tensor_(gc_tensor self, gc_tensor min, gc_tensor max) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::clip_(self_local, min ? std::make_optional(tensor_from_ocaml(min)) : std::nullopt, max ? std::make_optional(tensor_from_ocaml(max)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clip_tensor_out(gc_tensor out, gc_tensor self, gc_tensor min, gc_tensor max) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::clip_out(out_local, tensor_from_ocaml(self), min ? std::make_optional(tensor_from_ocaml(min)) : std::nullopt, max ? std::make_optional(tensor_from_ocaml(max)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clone(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::clone(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_clone_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::clone_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_coalesce(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).coalesce();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_col2im(gc_tensor self, int64_t *output_size_data, int output_size_len, int64_t *kernel_size_data, int kernel_size_len, int64_t *dilation_data, int dilation_len, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len) {
  PROTECT(
    torch::Tensor results__ = torch::col2im(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(dilation_data, dilation_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_col2im_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len, int64_t *kernel_size_data, int kernel_size_len, int64_t *dilation_data, int dilation_len, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::col2im_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(dilation_data, dilation_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_col_indices(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).col_indices();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_col_indices_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::col_indices_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_col_indices_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::col_indices_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_column_stack(gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    torch::Tensor results__ = torch::column_stack(of_carray_tensor(tensors_data, tensors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_column_stack_out(gc_tensor out, gc_tensor *tensors_data, int tensors_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::column_stack_out(out_local, of_carray_tensor(tensors_data, tensors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_combinations(gc_tensor self, int64_t r, int with_replacement) {
  PROTECT(
    torch::Tensor results__ = torch::combinations(tensor_from_ocaml(self), r, (bool)with_replacement);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_complex(gc_tensor real, gc_tensor imag) {
  PROTECT(
    torch::Tensor results__ = torch::complex(tensor_from_ocaml(real), tensor_from_ocaml(imag));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_complex_out(gc_tensor out, gc_tensor real, gc_tensor imag) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::complex_out(out_local, tensor_from_ocaml(real), tensor_from_ocaml(imag));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_concat(gc_tensor *tensors_data, int tensors_len, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::concat(of_carray_tensor(tensors_data, tensors_len), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_concat_out(gc_tensor out, gc_tensor *tensors_data, int tensors_len, int64_t dim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::concat_out(out_local, of_carray_tensor(tensors_data, tensors_len), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_concatenate(gc_tensor *tensors_data, int tensors_len, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::concatenate(of_carray_tensor(tensors_data, tensors_len), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_concatenate_out(gc_tensor out, gc_tensor *tensors_data, int tensors_len, int64_t dim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::concatenate_out(out_local, of_carray_tensor(tensors_data, tensors_len), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_conj(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::conj(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_conj_physical(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::conj_physical(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_conj_physical_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::conj_physical_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_conj_physical_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::conj_physical_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_constant_pad_nd(gc_tensor self, int64_t *pad_data, int pad_len, scalar value) {
  PROTECT(
    torch::Tensor results__ = torch::constant_pad_nd(tensor_from_ocaml(self), torch::IntArrayRef(pad_data, pad_len), value ? *value : c10::Scalar{0} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_constant_pad_nd_out(gc_tensor out, gc_tensor self, int64_t *pad_data, int pad_len, scalar value) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::constant_pad_nd_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(pad_data, pad_len), value ? *value : c10::Scalar{0} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_contiguous(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).contiguous();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_conv1d(gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::conv1d(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_conv1d_padding(gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, char * padding, int64_t *dilation_data, int dilation_len, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::conv1d(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), std::string(padding), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_conv2d(gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::conv2d(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_conv2d_padding(gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, char * padding, int64_t *dilation_data, int dilation_len, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::conv2d(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), std::string(padding), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_conv3d(gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::conv3d(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_conv3d_padding(gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, char * padding, int64_t *dilation_data, int dilation_len, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::conv3d(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), std::string(padding), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_conv_depthwise3d(gc_tensor self, gc_tensor weight, int64_t *kernel_size_data, int kernel_size_len, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len) {
  PROTECT(
    torch::Tensor results__ = torch::conv_depthwise3d(tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(kernel_size_data, kernel_size_len), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_conv_depthwise3d_out(gc_tensor out, gc_tensor self, gc_tensor weight, int64_t *kernel_size_data, int kernel_size_len, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::conv_depthwise3d_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(kernel_size_data, kernel_size_len), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_conv_tbc(gc_tensor self, gc_tensor weight, gc_tensor bias, int64_t pad) {
  PROTECT(
    torch::Tensor results__ = torch::conv_tbc(tensor_from_ocaml(self), tensor_from_ocaml(weight), tensor_from_ocaml(bias), pad);
    return tensor_to_ocaml(results__);
  )
}

void atg_conv_tbc_backward(raw_tensor *out__, gc_tensor self, gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t pad) {
  PROTECT(
    auto results__ = torch::conv_tbc_backward(tensor_from_ocaml(self), tensor_from_ocaml(input), tensor_from_ocaml(weight), tensor_from_ocaml(bias), pad);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

raw_tensor atg_conv_tbc_out(gc_tensor out, gc_tensor self, gc_tensor weight, gc_tensor bias, int64_t pad) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::conv_tbc_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), tensor_from_ocaml(bias), pad);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_conv_transpose1d(gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *output_padding_data, int output_padding_len, int64_t groups, int64_t *dilation_data, int dilation_len) {
  PROTECT(
    torch::Tensor results__ = torch::conv_transpose1d(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(output_padding_data, output_padding_len), groups, torch::IntArrayRef(dilation_data, dilation_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_conv_transpose2d(gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *output_padding_data, int output_padding_len, int64_t groups, int64_t *dilation_data, int dilation_len) {
  PROTECT(
    torch::Tensor results__ = torch::conv_transpose2d(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(output_padding_data, output_padding_len), groups, torch::IntArrayRef(dilation_data, dilation_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_conv_transpose3d(gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *output_padding_data, int output_padding_len, int64_t groups, int64_t *dilation_data, int dilation_len) {
  PROTECT(
    torch::Tensor results__ = torch::conv_transpose3d(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(output_padding_data, output_padding_len), groups, torch::IntArrayRef(dilation_data, dilation_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_convolution(gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int transposed, int64_t *output_padding_data, int output_padding_len, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::convolution(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)transposed, torch::IntArrayRef(output_padding_data, output_padding_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_convolution_out(gc_tensor out, gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int transposed, int64_t *output_padding_data, int output_padding_len, int64_t groups) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::convolution_out(out_local, tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)transposed, torch::IntArrayRef(output_padding_data, output_padding_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_convolution_overrideable(gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int transposed, int64_t *output_padding_data, int output_padding_len, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::convolution_overrideable(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)transposed, torch::IntArrayRef(output_padding_data, output_padding_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_convolution_overrideable_out(gc_tensor out, gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int transposed, int64_t *output_padding_data, int output_padding_len, int64_t groups) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::convolution_overrideable_out(out_local, tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)transposed, torch::IntArrayRef(output_padding_data, output_padding_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_copy(gc_tensor self, gc_tensor src, int non_blocking) {
  PROTECT(
    torch::Tensor results__ = torch::copy(tensor_from_ocaml(self), tensor_from_ocaml(src), (bool)non_blocking);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_copy_out(gc_tensor out, gc_tensor self, gc_tensor src, int non_blocking) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::copy_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(src), (bool)non_blocking);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_copy_sparse_to_sparse(gc_tensor self, gc_tensor src, int non_blocking) {
  PROTECT(
    torch::Tensor results__ = torch::copy_sparse_to_sparse(tensor_from_ocaml(self), tensor_from_ocaml(src), (bool)non_blocking);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_copy_sparse_to_sparse_(gc_tensor self, gc_tensor src, int non_blocking) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::copy_sparse_to_sparse_(self_local, tensor_from_ocaml(src), (bool)non_blocking);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_copy_sparse_to_sparse_out(gc_tensor out, gc_tensor self, gc_tensor src, int non_blocking) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::copy_sparse_to_sparse_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(src), (bool)non_blocking);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_copysign(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::copysign(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_copysign_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).copysign_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_copysign_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::copysign_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_copysign_scalar(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::copysign(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_copysign_scalar_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).copysign_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_copysign_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::copysign_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_corrcoef(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::corrcoef(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cos(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::cos(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cos_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::cos_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cos_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::cos_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cosh(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::cosh(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cosh_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::cosh_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cosh_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::cosh_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cosine_embedding_loss(gc_tensor input1, gc_tensor input2, gc_tensor target, double margin, int64_t reduction) {
  PROTECT(
    torch::Tensor results__ = torch::cosine_embedding_loss(tensor_from_ocaml(input1), tensor_from_ocaml(input2), tensor_from_ocaml(target), margin, reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cosine_similarity(gc_tensor x1, gc_tensor x2, int64_t dim, double eps) {
  PROTECT(
    torch::Tensor results__ = torch::cosine_similarity(tensor_from_ocaml(x1), tensor_from_ocaml(x2), dim, eps);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_count_nonzero(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::count_nonzero_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_count_nonzero_out(gc_tensor out, gc_tensor self, int64_t dim_v, int dim_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::count_nonzero_out(out_local, tensor_from_ocaml(self), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cov(gc_tensor self, int64_t correction, gc_tensor fweights, gc_tensor aweights) {
  PROTECT(
    torch::Tensor results__ = torch::cov(tensor_from_ocaml(self), correction, fweights ? std::make_optional(tensor_from_ocaml(fweights)) : std::nullopt, aweights ? std::make_optional(tensor_from_ocaml(aweights)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cross(gc_tensor self, gc_tensor other, int64_t dim_v, int dim_null) {
  PROTECT(
    torch::Tensor results__ = torch::cross(tensor_from_ocaml(self), tensor_from_ocaml(other), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cross_entropy_loss(gc_tensor self, gc_tensor target, gc_tensor weight, int64_t reduction, int64_t ignore_index, double label_smoothing) {
  PROTECT(
    torch::Tensor results__ = torch::cross_entropy_loss(tensor_from_ocaml(self), tensor_from_ocaml(target), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, reduction, ignore_index, label_smoothing);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cross_out(gc_tensor out, gc_tensor self, gc_tensor other, int64_t dim_v, int dim_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::cross_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_crow_indices(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).crow_indices();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_crow_indices_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::crow_indices_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_crow_indices_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::crow_indices_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ctc_loss(gc_tensor log_probs, gc_tensor targets, int64_t *input_lengths_data, int input_lengths_len, int64_t *target_lengths_data, int target_lengths_len, int64_t blank, int64_t reduction, int zero_infinity) {
  PROTECT(
    torch::Tensor results__ = torch::ctc_loss(tensor_from_ocaml(log_probs), tensor_from_ocaml(targets), torch::IntArrayRef(input_lengths_data, input_lengths_len), torch::IntArrayRef(target_lengths_data, target_lengths_len), blank, reduction, (bool)zero_infinity);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ctc_loss_tensor(gc_tensor log_probs, gc_tensor targets, gc_tensor input_lengths, gc_tensor target_lengths, int64_t blank, int64_t reduction, int zero_infinity) {
  PROTECT(
    torch::Tensor results__ = torch::ctc_loss(tensor_from_ocaml(log_probs), tensor_from_ocaml(targets), tensor_from_ocaml(input_lengths), tensor_from_ocaml(target_lengths), blank, reduction, (bool)zero_infinity);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cudnn_affine_grid_generator(gc_tensor theta, int64_t n, int64_t C, int64_t H, int64_t W) {
  PROTECT(
    torch::Tensor results__ = torch::cudnn_affine_grid_generator(tensor_from_ocaml(theta), n, C, H, W);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cudnn_affine_grid_generator_backward(gc_tensor grad, int64_t n, int64_t C, int64_t H, int64_t W) {
  PROTECT(
    torch::Tensor results__ = torch::cudnn_affine_grid_generator_backward(tensor_from_ocaml(grad), n, C, H, W);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cudnn_affine_grid_generator_backward_out(gc_tensor out, gc_tensor grad, int64_t n, int64_t C, int64_t H, int64_t W) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::cudnn_affine_grid_generator_backward_out(out_local, tensor_from_ocaml(grad), n, C, H, W);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cudnn_affine_grid_generator_out(gc_tensor out, gc_tensor theta, int64_t n, int64_t C, int64_t H, int64_t W) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::cudnn_affine_grid_generator_out(out_local, tensor_from_ocaml(theta), n, C, H, W);
    return tensor_to_ocaml(results__);
  )
}

void atg_cudnn_batch_norm(raw_tensor *out__, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, int training, double exponential_average_factor, double epsilon) {
  PROTECT(
    auto results__ = torch::cudnn_batch_norm(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, (bool)training, exponential_average_factor, epsilon);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

void atg_cudnn_batch_norm_backward(raw_tensor *out__, gc_tensor input, gc_tensor grad_output, gc_tensor weight, gc_tensor running_mean, gc_tensor running_var, gc_tensor save_mean, gc_tensor save_var, double epsilon, gc_tensor reserveSpace) {
  PROTECT(
    auto results__ = torch::cudnn_batch_norm_backward(tensor_from_ocaml(input), tensor_from_ocaml(grad_output), tensor_from_ocaml(weight), running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, save_mean ? std::make_optional(tensor_from_ocaml(save_mean)) : std::nullopt, save_var ? std::make_optional(tensor_from_ocaml(save_var)) : std::nullopt, epsilon, tensor_from_ocaml(reserveSpace));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_cudnn_batch_norm_backward_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor input, gc_tensor grad_output, gc_tensor weight, gc_tensor running_mean, gc_tensor running_var, gc_tensor save_mean, gc_tensor save_var, double epsilon, gc_tensor reserveSpace) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  PROTECT(
    auto results__ = torch::cudnn_batch_norm_backward_out(out0_local, out1_local, out2_local, tensor_from_ocaml(input), tensor_from_ocaml(grad_output), tensor_from_ocaml(weight), running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, save_mean ? std::make_optional(tensor_from_ocaml(save_mean)) : std::nullopt, save_var ? std::make_optional(tensor_from_ocaml(save_var)) : std::nullopt, epsilon, tensor_from_ocaml(reserveSpace));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_cudnn_batch_norm_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor out3, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, int training, double exponential_average_factor, double epsilon) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  torch::Tensor out3_local = tensor_from_ocaml(out3);
  PROTECT(
    auto results__ = torch::cudnn_batch_norm_out(out0_local, out1_local, out2_local, out3_local, tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, (bool)training, exponential_average_factor, epsilon);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

raw_tensor atg_cudnn_convolution(gc_tensor self, gc_tensor weight, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups, int benchmark, int deterministic, int allow_tf32) {
  PROTECT(
    torch::Tensor results__ = torch::cudnn_convolution(tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups, (bool)benchmark, (bool)deterministic, (bool)allow_tf32);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cudnn_convolution_add_relu(gc_tensor self, gc_tensor weight, gc_tensor z, scalar alpha, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::cudnn_convolution_add_relu(tensor_from_ocaml(self), tensor_from_ocaml(weight), tensor_from_ocaml(z), *alpha, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cudnn_convolution_add_relu_out(gc_tensor out, gc_tensor self, gc_tensor weight, gc_tensor z, scalar alpha, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int64_t groups) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::cudnn_convolution_add_relu_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), tensor_from_ocaml(z), *alpha, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cudnn_convolution_out(gc_tensor out, gc_tensor self, gc_tensor weight, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups, int benchmark, int deterministic, int allow_tf32) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::cudnn_convolution_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups, (bool)benchmark, (bool)deterministic, (bool)allow_tf32);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cudnn_convolution_relu(gc_tensor self, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::cudnn_convolution_relu(tensor_from_ocaml(self), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cudnn_convolution_relu_out(gc_tensor out, gc_tensor self, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int64_t groups) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::cudnn_convolution_relu_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cudnn_convolution_transpose(gc_tensor self, gc_tensor weight, int64_t *padding_data, int padding_len, int64_t *output_padding_data, int output_padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups, int benchmark, int deterministic, int allow_tf32) {
  PROTECT(
    torch::Tensor results__ = torch::cudnn_convolution_transpose(tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(output_padding_data, output_padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups, (bool)benchmark, (bool)deterministic, (bool)allow_tf32);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cudnn_convolution_transpose_out(gc_tensor out, gc_tensor self, gc_tensor weight, int64_t *padding_data, int padding_len, int64_t *output_padding_data, int output_padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups, int benchmark, int deterministic, int allow_tf32) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::cudnn_convolution_transpose_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(output_padding_data, output_padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups, (bool)benchmark, (bool)deterministic, (bool)allow_tf32);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cudnn_grid_sampler(gc_tensor self, gc_tensor grid) {
  PROTECT(
    torch::Tensor results__ = torch::cudnn_grid_sampler(tensor_from_ocaml(self), tensor_from_ocaml(grid));
    return tensor_to_ocaml(results__);
  )
}

void atg_cudnn_grid_sampler_backward(raw_tensor *out__, gc_tensor self, gc_tensor grid, gc_tensor grad_output) {
  PROTECT(
    auto results__ = torch::cudnn_grid_sampler_backward(tensor_from_ocaml(self), tensor_from_ocaml(grid), tensor_from_ocaml(grad_output));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_cudnn_grid_sampler_backward_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor self, gc_tensor grid, gc_tensor grad_output) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::cudnn_grid_sampler_backward_out(out0_local, out1_local, tensor_from_ocaml(self), tensor_from_ocaml(grid), tensor_from_ocaml(grad_output));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_cudnn_grid_sampler_out(gc_tensor out, gc_tensor self, gc_tensor grid) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::cudnn_grid_sampler_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(grid));
    return tensor_to_ocaml(results__);
  )
}

int atg_cudnn_is_acceptable(gc_tensor self) {
  PROTECT(
    return torch::cudnn_is_acceptable(tensor_from_ocaml(self));
  )
  return 0;
}

void atg_cummax(raw_tensor *out__, gc_tensor self, int64_t dim) {
  PROTECT(
    auto results__ = torch::cummax(tensor_from_ocaml(self), dim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_cummax_out(raw_tensor *out__, gc_tensor values, gc_tensor indices, gc_tensor self, int64_t dim) {
  torch::Tensor values_local = tensor_from_ocaml(values);
  torch::Tensor indices_local = tensor_from_ocaml(indices);
  PROTECT(
    auto results__ = torch::cummax_out(values_local, indices_local, tensor_from_ocaml(self), dim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_cummaxmin_backward(gc_tensor grad, gc_tensor input, gc_tensor indices, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::cummaxmin_backward(tensor_from_ocaml(grad), tensor_from_ocaml(input), tensor_from_ocaml(indices), dim);
    return tensor_to_ocaml(results__);
  )
}

void atg_cummin(raw_tensor *out__, gc_tensor self, int64_t dim) {
  PROTECT(
    auto results__ = torch::cummin(tensor_from_ocaml(self), dim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_cummin_out(raw_tensor *out__, gc_tensor values, gc_tensor indices, gc_tensor self, int64_t dim) {
  torch::Tensor values_local = tensor_from_ocaml(values);
  torch::Tensor indices_local = tensor_from_ocaml(indices);
  PROTECT(
    auto results__ = torch::cummin_out(values_local, indices_local, tensor_from_ocaml(self), dim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_cumprod(gc_tensor self, int64_t dim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::cumprod(tensor_from_ocaml(self), dim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cumprod_(gc_tensor self, int64_t dim, int dtype) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).cumprod_(dim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cumprod_backward(gc_tensor grad, gc_tensor input, int64_t dim, gc_tensor output) {
  PROTECT(
    torch::Tensor results__ = torch::cumprod_backward(tensor_from_ocaml(grad), tensor_from_ocaml(input), dim, tensor_from_ocaml(output));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cumprod_out(gc_tensor out, gc_tensor self, int64_t dim, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::cumprod_out(out_local, tensor_from_ocaml(self), dim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cumsum(gc_tensor self, int64_t dim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::cumsum(tensor_from_ocaml(self), dim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cumsum_(gc_tensor self, int64_t dim, int dtype) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).cumsum_(dim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cumsum_out(gc_tensor out, gc_tensor self, int64_t dim, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::cumsum_out(out_local, tensor_from_ocaml(self), dim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cumulative_trapezoid(gc_tensor y, gc_tensor x, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::cumulative_trapezoid(tensor_from_ocaml(y), tensor_from_ocaml(x), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_cumulative_trapezoid_dx(gc_tensor y, scalar dx, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::cumulative_trapezoid(tensor_from_ocaml(y), dx ? *dx : c10::Scalar{1} , dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_data(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).data();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_deg2rad(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::deg2rad(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_deg2rad_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::deg2rad_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_deg2rad_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::deg2rad_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

int64_t atg_dense_dim(gc_tensor self) {
  PROTECT(
    return tensor_from_ocaml(self).dense_dim();
  )
  return 0;
}

raw_tensor atg_dequantize(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::dequantize(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_dequantize_self_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::dequantize_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg_dequantize_tensors(gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    auto results__ = torch::dequantize(of_carray_tensor(tensors_data, tensors_len));
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

void atg_dequantize_tensors_out(gc_tensor *out_data, int out_len, gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    torch::dequantize_out(of_carray_tensor(out_data, out_len), of_carray_tensor(tensors_data, tensors_len));
  )
}

raw_tensor atg_det(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::det(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_detach(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::detach(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_detach_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::detach_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_detach_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::detach_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_detach_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::detach_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_diag(gc_tensor self, int64_t diagonal) {
  PROTECT(
    torch::Tensor results__ = torch::diag(tensor_from_ocaml(self), diagonal);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_diag_embed(gc_tensor self, int64_t offset, int64_t dim1, int64_t dim2) {
  PROTECT(
    torch::Tensor results__ = torch::diag_embed(tensor_from_ocaml(self), offset, dim1, dim2);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_diag_embed_out(gc_tensor out, gc_tensor self, int64_t offset, int64_t dim1, int64_t dim2) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::diag_embed_out(out_local, tensor_from_ocaml(self), offset, dim1, dim2);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_diag_out(gc_tensor out, gc_tensor self, int64_t diagonal) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::diag_out(out_local, tensor_from_ocaml(self), diagonal);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_diagflat(gc_tensor self, int64_t offset) {
  PROTECT(
    torch::Tensor results__ = torch::diagflat(tensor_from_ocaml(self), offset);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_diagonal(gc_tensor self, int64_t offset, int64_t dim1, int64_t dim2) {
  PROTECT(
    torch::Tensor results__ = torch::diagonal(tensor_from_ocaml(self), offset, dim1, dim2);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_diagonal_backward(gc_tensor grad_output, int64_t *input_sizes_data, int input_sizes_len, int64_t offset, int64_t dim1, int64_t dim2) {
  PROTECT(
    torch::Tensor results__ = torch::diagonal_backward(tensor_from_ocaml(grad_output), torch::IntArrayRef(input_sizes_data, input_sizes_len), offset, dim1, dim2);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_diagonal_backward_out(gc_tensor out, gc_tensor grad_output, int64_t *input_sizes_data, int input_sizes_len, int64_t offset, int64_t dim1, int64_t dim2) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::diagonal_backward_out(out_local, tensor_from_ocaml(grad_output), torch::IntArrayRef(input_sizes_data, input_sizes_len), offset, dim1, dim2);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_diagonal_copy(gc_tensor self, int64_t offset, int64_t dim1, int64_t dim2) {
  PROTECT(
    torch::Tensor results__ = torch::diagonal_copy(tensor_from_ocaml(self), offset, dim1, dim2);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_diagonal_copy_out(gc_tensor out, gc_tensor self, int64_t offset, int64_t dim1, int64_t dim2) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::diagonal_copy_out(out_local, tensor_from_ocaml(self), offset, dim1, dim2);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_diagonal_scatter(gc_tensor self, gc_tensor src, int64_t offset, int64_t dim1, int64_t dim2) {
  PROTECT(
    torch::Tensor results__ = torch::diagonal_scatter(tensor_from_ocaml(self), tensor_from_ocaml(src), offset, dim1, dim2);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_diagonal_scatter_out(gc_tensor out, gc_tensor self, gc_tensor src, int64_t offset, int64_t dim1, int64_t dim2) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::diagonal_scatter_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(src), offset, dim1, dim2);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_diff(gc_tensor self, int64_t n, int64_t dim, gc_tensor prepend, gc_tensor append) {
  PROTECT(
    torch::Tensor results__ = torch::diff(tensor_from_ocaml(self), n, dim, prepend ? std::make_optional(tensor_from_ocaml(prepend)) : std::nullopt, append ? std::make_optional(tensor_from_ocaml(append)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_diff_out(gc_tensor out, gc_tensor self, int64_t n, int64_t dim, gc_tensor prepend, gc_tensor append) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::diff_out(out_local, tensor_from_ocaml(self), n, dim, prepend ? std::make_optional(tensor_from_ocaml(prepend)) : std::nullopt, append ? std::make_optional(tensor_from_ocaml(append)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_digamma(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::digamma(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_digamma_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).digamma_();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_digamma_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::digamma_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_dist(gc_tensor self, gc_tensor other, scalar p) {
  PROTECT(
    torch::Tensor results__ = torch::dist(tensor_from_ocaml(self), tensor_from_ocaml(other), p ? *p : c10::Scalar{2} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_dist_out(gc_tensor out, gc_tensor self, gc_tensor other, scalar p) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::dist_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other), p ? *p : c10::Scalar{2} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_div(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::div(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_div_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).div_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_div_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::div_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_div_out_mode(gc_tensor out, gc_tensor self, gc_tensor other, char * rounding_mode_v, int rounding_mode_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::div_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other), rounding_mode_null ? c10::nullopt : c10::optional<c10::string_view>(rounding_mode_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_div_scalar(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::div(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_div_scalar_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).div_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_div_scalar_mode(gc_tensor self, scalar other, char * rounding_mode_v, int rounding_mode_null) {
  PROTECT(
    torch::Tensor results__ = torch::div(tensor_from_ocaml(self), *other, rounding_mode_null ? c10::nullopt : c10::optional<c10::string_view>(rounding_mode_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_div_scalar_mode_(gc_tensor self, scalar other, char * rounding_mode_v, int rounding_mode_null) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).div_(*other, rounding_mode_null ? c10::nullopt : c10::optional<c10::string_view>(rounding_mode_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_div_scalar_mode_out(gc_tensor out, gc_tensor self, scalar other, char * rounding_mode_v, int rounding_mode_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::div_out(out_local, tensor_from_ocaml(self), *other, rounding_mode_null ? c10::nullopt : c10::optional<c10::string_view>(rounding_mode_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_div_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::div_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_div_tensor_mode(gc_tensor self, gc_tensor other, char * rounding_mode_v, int rounding_mode_null) {
  PROTECT(
    torch::Tensor results__ = torch::div(tensor_from_ocaml(self), tensor_from_ocaml(other), rounding_mode_null ? c10::nullopt : c10::optional<c10::string_view>(rounding_mode_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_div_tensor_mode_(gc_tensor self, gc_tensor other, char * rounding_mode_v, int rounding_mode_null) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).div_(tensor_from_ocaml(other), rounding_mode_null ? c10::nullopt : c10::optional<c10::string_view>(rounding_mode_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_divide(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::divide(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_divide_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).divide_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_divide_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::divide_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_divide_out_mode(gc_tensor out, gc_tensor self, gc_tensor other, char * rounding_mode_v, int rounding_mode_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::divide_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other), rounding_mode_null ? c10::nullopt : c10::optional<c10::string_view>(rounding_mode_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_divide_scalar(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::divide(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_divide_scalar_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).divide_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_divide_scalar_mode(gc_tensor self, scalar other, char * rounding_mode_v, int rounding_mode_null) {
  PROTECT(
    torch::Tensor results__ = torch::divide(tensor_from_ocaml(self), *other, rounding_mode_null ? c10::nullopt : c10::optional<c10::string_view>(rounding_mode_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_divide_scalar_mode_(gc_tensor self, scalar other, char * rounding_mode_v, int rounding_mode_null) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).divide_(*other, rounding_mode_null ? c10::nullopt : c10::optional<c10::string_view>(rounding_mode_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_divide_tensor_mode(gc_tensor self, gc_tensor other, char * rounding_mode_v, int rounding_mode_null) {
  PROTECT(
    torch::Tensor results__ = torch::divide(tensor_from_ocaml(self), tensor_from_ocaml(other), rounding_mode_null ? c10::nullopt : c10::optional<c10::string_view>(rounding_mode_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_divide_tensor_mode_(gc_tensor self, gc_tensor other, char * rounding_mode_v, int rounding_mode_null) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).divide_(tensor_from_ocaml(other), rounding_mode_null ? c10::nullopt : c10::optional<c10::string_view>(rounding_mode_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_dot(gc_tensor self, gc_tensor tensor) {
  PROTECT(
    torch::Tensor results__ = torch::dot(tensor_from_ocaml(self), tensor_from_ocaml(tensor));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_dot_out(gc_tensor out, gc_tensor self, gc_tensor tensor) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::dot_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(tensor));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_dropout(gc_tensor input, double p, int train) {
  PROTECT(
    torch::Tensor results__ = torch::dropout(tensor_from_ocaml(input), p, (bool)train);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_dropout_(gc_tensor self, double p, int train) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::dropout_(self_local, p, (bool)train);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg_dsplit(gc_tensor self, int64_t sections) {
  PROTECT(
    auto results__ = torch::dsplit(tensor_from_ocaml(self), sections);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor *atg_dsplit_array(gc_tensor self, int64_t *indices_data, int indices_len) {
  PROTECT(
    auto results__ = torch::dsplit(tensor_from_ocaml(self), torch::IntArrayRef(indices_data, indices_len));
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor atg_dstack(gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    torch::Tensor results__ = torch::dstack(of_carray_tensor(tensors_data, tensors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_dstack_out(gc_tensor out, gc_tensor *tensors_data, int tensors_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::dstack_out(out_local, of_carray_tensor(tensors_data, tensors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_einsum(char * equation, gc_tensor *tensors_data, int tensors_len, int64_t *path_data, int path_len) {
  PROTECT(
    torch::Tensor results__ = torch::einsum(std::string(equation), of_carray_tensor(tensors_data, tensors_len), path_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(path_data, path_len)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_elu(gc_tensor self, scalar alpha, scalar scale, scalar input_scale) {
  PROTECT(
    torch::Tensor results__ = torch::elu(tensor_from_ocaml(self), alpha ? *alpha : c10::Scalar{1} , scale ? *scale : c10::Scalar{1} , input_scale ? *input_scale : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_elu_(gc_tensor self, scalar alpha, scalar scale, scalar input_scale) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::elu_(self_local, alpha ? *alpha : c10::Scalar{1} , scale ? *scale : c10::Scalar{1} , input_scale ? *input_scale : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_elu_backward(gc_tensor grad_output, scalar alpha, scalar scale, scalar input_scale, int is_result, gc_tensor self_or_result) {
  PROTECT(
    torch::Tensor results__ = torch::elu_backward(tensor_from_ocaml(grad_output), *alpha, *scale, *input_scale, (bool)is_result, tensor_from_ocaml(self_or_result));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_elu_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, scalar alpha, scalar scale, scalar input_scale, int is_result, gc_tensor self_or_result) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::elu_backward_out(grad_input_local, tensor_from_ocaml(grad_output), *alpha, *scale, *input_scale, (bool)is_result, tensor_from_ocaml(self_or_result));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_elu_out(gc_tensor out, gc_tensor self, scalar alpha, scalar scale, scalar input_scale) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::elu_out(out_local, tensor_from_ocaml(self), alpha ? *alpha : c10::Scalar{1} , scale ? *scale : c10::Scalar{1} , input_scale ? *input_scale : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_embedding(gc_tensor weight, gc_tensor indices, int64_t padding_idx, int scale_grad_by_freq, int sparse) {
  PROTECT(
    torch::Tensor results__ = torch::embedding(tensor_from_ocaml(weight), tensor_from_ocaml(indices), padding_idx, (bool)scale_grad_by_freq, (bool)sparse);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_embedding_backward(gc_tensor grad, gc_tensor indices, int64_t num_weights, int64_t padding_idx, int scale_grad_by_freq, int sparse) {
  PROTECT(
    torch::Tensor results__ = torch::embedding_backward(tensor_from_ocaml(grad), tensor_from_ocaml(indices), num_weights, padding_idx, (bool)scale_grad_by_freq, (bool)sparse);
    return tensor_to_ocaml(results__);
  )
}

void atg_embedding_bag(raw_tensor *out__, gc_tensor weight, gc_tensor indices, gc_tensor offsets, int scale_grad_by_freq, int64_t mode, int sparse, gc_tensor per_sample_weights, int include_last_offset) {
  PROTECT(
    auto results__ = torch::embedding_bag(tensor_from_ocaml(weight), tensor_from_ocaml(indices), tensor_from_ocaml(offsets), (bool)scale_grad_by_freq, mode, (bool)sparse, per_sample_weights ? std::make_optional(tensor_from_ocaml(per_sample_weights)) : std::nullopt, (bool)include_last_offset);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

void atg_embedding_bag_padding_idx(raw_tensor *out__, gc_tensor weight, gc_tensor indices, gc_tensor offsets, int scale_grad_by_freq, int64_t mode, int sparse, gc_tensor per_sample_weights, int include_last_offset, int64_t padding_idx_v, int padding_idx_null) {
  PROTECT(
    auto results__ = torch::embedding_bag(tensor_from_ocaml(weight), tensor_from_ocaml(indices), tensor_from_ocaml(offsets), (bool)scale_grad_by_freq, mode, (bool)sparse, per_sample_weights ? std::make_optional(tensor_from_ocaml(per_sample_weights)) : std::nullopt, (bool)include_last_offset, padding_idx_null ? c10::nullopt : c10::optional<int64_t>(padding_idx_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

raw_tensor atg_embedding_dense_backward(gc_tensor grad_output, gc_tensor indices, int64_t num_weights, int64_t padding_idx, int scale_grad_by_freq) {
  PROTECT(
    torch::Tensor results__ = torch::embedding_dense_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(indices), num_weights, padding_idx, (bool)scale_grad_by_freq);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_embedding_dense_backward_out(gc_tensor out, gc_tensor grad_output, gc_tensor indices, int64_t num_weights, int64_t padding_idx, int scale_grad_by_freq) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::embedding_dense_backward_out(out_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(indices), num_weights, padding_idx, (bool)scale_grad_by_freq);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_embedding_out(gc_tensor out, gc_tensor weight, gc_tensor indices, int64_t padding_idx, int scale_grad_by_freq, int sparse) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::embedding_out(out_local, tensor_from_ocaml(weight), tensor_from_ocaml(indices), padding_idx, (bool)scale_grad_by_freq, (bool)sparse);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_embedding_renorm(gc_tensor self, gc_tensor indices, double max_norm, double norm_type) {
  PROTECT(
    torch::Tensor results__ = torch::embedding_renorm(tensor_from_ocaml(self), tensor_from_ocaml(indices), max_norm, norm_type);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_embedding_renorm_(gc_tensor self, gc_tensor indices, double max_norm, double norm_type) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::embedding_renorm_(self_local, tensor_from_ocaml(indices), max_norm, norm_type);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_embedding_renorm_out(gc_tensor out, gc_tensor self, gc_tensor indices, double max_norm, double norm_type) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::embedding_renorm_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(indices), max_norm, norm_type);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_embedding_sparse_backward(gc_tensor grad, gc_tensor indices, int64_t num_weights, int64_t padding_idx, int scale_grad_by_freq) {
  PROTECT(
    torch::Tensor results__ = torch::embedding_sparse_backward(tensor_from_ocaml(grad), tensor_from_ocaml(indices), num_weights, padding_idx, (bool)scale_grad_by_freq);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_empty(int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::empty(torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_empty_like(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::empty_like(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_empty_like_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::empty_like_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_empty_out(gc_tensor out, int64_t *size_data, int size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::empty_out(out_local, torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_empty_permuted(int64_t *size_data, int size_len, int64_t *physical_layout_data, int physical_layout_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::empty_permuted(torch::IntArrayRef(size_data, size_len), torch::IntArrayRef(physical_layout_data, physical_layout_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_empty_permuted_out(gc_tensor out, int64_t *size_data, int size_len, int64_t *physical_layout_data, int physical_layout_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::empty_permuted_out(out_local, torch::IntArrayRef(size_data, size_len), torch::IntArrayRef(physical_layout_data, physical_layout_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_empty_quantized(int64_t *size_data, int size_len, gc_tensor qtensor, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::empty_quantized(torch::IntArrayRef(size_data, size_len), tensor_from_ocaml(qtensor), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_empty_quantized_out(gc_tensor out, int64_t *size_data, int size_len, gc_tensor qtensor) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::empty_quantized_out(out_local, torch::IntArrayRef(size_data, size_len), tensor_from_ocaml(qtensor));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_empty_strided(int64_t *size_data, int size_len, int64_t *stride_data, int stride_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::empty_strided(torch::IntArrayRef(size_data, size_len), torch::IntArrayRef(stride_data, stride_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_empty_strided_out(gc_tensor out, int64_t *size_data, int size_len, int64_t *stride_data, int stride_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::empty_strided_out(out_local, torch::IntArrayRef(size_data, size_len), torch::IntArrayRef(stride_data, stride_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_eq(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::eq(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_eq_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).eq_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_eq_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::eq_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_eq_tensor(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::eq(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_eq_tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).eq_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_eq_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::eq_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

int atg_equal(gc_tensor self, gc_tensor other) {
  PROTECT(
    return torch::equal(tensor_from_ocaml(self), tensor_from_ocaml(other));
  )
  return 0;
}

raw_tensor atg_erf(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::erf(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_erf_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::erf_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_erf_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::erf_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_erfc(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::erfc(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_erfc_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::erfc_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_erfc_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::erfc_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_erfinv(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::erfinv(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_erfinv_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).erfinv_();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_erfinv_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::erfinv_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_exp(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::exp(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_exp2(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::exp2(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_exp2_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::exp2_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_exp2_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::exp2_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_exp_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::exp_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_exp_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::exp_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_expand(gc_tensor self, int64_t *size_data, int size_len, int implicit) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).expand(torch::IntArrayRef(size_data, size_len), (bool)implicit);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_expand_as(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).expand_as(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_expand_copy(gc_tensor self, int64_t *size_data, int size_len, int implicit) {
  PROTECT(
    torch::Tensor results__ = torch::expand_copy(tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), (bool)implicit);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_expand_copy_out(gc_tensor out, gc_tensor self, int64_t *size_data, int size_len, int implicit) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::expand_copy_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), (bool)implicit);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_expm1(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::expm1(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_expm1_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::expm1_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_expm1_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::expm1_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_exponential(gc_tensor self, double lambd) {
  PROTECT(
    torch::Tensor results__ = torch::exponential(tensor_from_ocaml(self), lambd);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_exponential_(gc_tensor self, double lambd) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).exponential_(lambd);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_exponential_out(gc_tensor out, gc_tensor self, double lambd) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::exponential_out(out_local, tensor_from_ocaml(self), lambd);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_eye(int64_t n, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::eye(n, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_eye_m(int64_t n, int64_t m, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::eye(n, m, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_eye_m_out(gc_tensor out, int64_t n, int64_t m) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::eye_out(out_local, n, m);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_eye_out(gc_tensor out, int64_t n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::eye_out(out_local, n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fake_quantize_per_channel_affine(gc_tensor self, gc_tensor scale, gc_tensor zero_point, int64_t axis, int64_t quant_min, int64_t quant_max) {
  PROTECT(
    torch::Tensor results__ = torch::fake_quantize_per_channel_affine(tensor_from_ocaml(self), tensor_from_ocaml(scale), tensor_from_ocaml(zero_point), axis, quant_min, quant_max);
    return tensor_to_ocaml(results__);
  )
}

void atg_fake_quantize_per_channel_affine_cachemask(raw_tensor *out__, gc_tensor self, gc_tensor scale, gc_tensor zero_point, int64_t axis, int64_t quant_min, int64_t quant_max) {
  PROTECT(
    auto results__ = torch::fake_quantize_per_channel_affine_cachemask(tensor_from_ocaml(self), tensor_from_ocaml(scale), tensor_from_ocaml(zero_point), axis, quant_min, quant_max);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_fake_quantize_per_channel_affine_cachemask_backward(gc_tensor grad, gc_tensor mask) {
  PROTECT(
    torch::Tensor results__ = torch::fake_quantize_per_channel_affine_cachemask_backward(tensor_from_ocaml(grad), tensor_from_ocaml(mask));
    return tensor_to_ocaml(results__);
  )
}

void atg_fake_quantize_per_channel_affine_cachemask_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor self, gc_tensor scale, gc_tensor zero_point, int64_t axis, int64_t quant_min, int64_t quant_max) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::fake_quantize_per_channel_affine_cachemask_out(out0_local, out1_local, tensor_from_ocaml(self), tensor_from_ocaml(scale), tensor_from_ocaml(zero_point), axis, quant_min, quant_max);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_fake_quantize_per_tensor_affine(gc_tensor self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max) {
  PROTECT(
    torch::Tensor results__ = torch::fake_quantize_per_tensor_affine(tensor_from_ocaml(self), scale, zero_point, quant_min, quant_max);
    return tensor_to_ocaml(results__);
  )
}

void atg_fake_quantize_per_tensor_affine_cachemask(raw_tensor *out__, gc_tensor self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max) {
  PROTECT(
    auto results__ = torch::fake_quantize_per_tensor_affine_cachemask(tensor_from_ocaml(self), scale, zero_point, quant_min, quant_max);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_fake_quantize_per_tensor_affine_cachemask_backward(gc_tensor grad, gc_tensor mask) {
  PROTECT(
    torch::Tensor results__ = torch::fake_quantize_per_tensor_affine_cachemask_backward(tensor_from_ocaml(grad), tensor_from_ocaml(mask));
    return tensor_to_ocaml(results__);
  )
}

void atg_fake_quantize_per_tensor_affine_cachemask_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::fake_quantize_per_tensor_affine_cachemask_out(out0_local, out1_local, tensor_from_ocaml(self), scale, zero_point, quant_min, quant_max);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_fake_quantize_per_tensor_affine_tensor_qparams(gc_tensor self, gc_tensor scale, gc_tensor zero_point, int64_t quant_min, int64_t quant_max) {
  PROTECT(
    torch::Tensor results__ = torch::fake_quantize_per_tensor_affine(tensor_from_ocaml(self), tensor_from_ocaml(scale), tensor_from_ocaml(zero_point), quant_min, quant_max);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fbgemm_linear_fp16_weight(gc_tensor input, gc_tensor packed_weight, gc_tensor bias) {
  PROTECT(
    torch::Tensor results__ = torch::fbgemm_linear_fp16_weight(tensor_from_ocaml(input), tensor_from_ocaml(packed_weight), tensor_from_ocaml(bias));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fbgemm_linear_fp16_weight_fp32_activation(gc_tensor input, gc_tensor packed_weight, gc_tensor bias) {
  PROTECT(
    torch::Tensor results__ = torch::fbgemm_linear_fp16_weight_fp32_activation(tensor_from_ocaml(input), tensor_from_ocaml(packed_weight), tensor_from_ocaml(bias));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fbgemm_linear_int8_weight(gc_tensor input, gc_tensor weight, gc_tensor packed, gc_tensor col_offsets, scalar weight_scale, scalar weight_zero_point, gc_tensor bias) {
  PROTECT(
    torch::Tensor results__ = torch::fbgemm_linear_int8_weight(tensor_from_ocaml(input), tensor_from_ocaml(weight), tensor_from_ocaml(packed), tensor_from_ocaml(col_offsets), *weight_scale, *weight_zero_point, tensor_from_ocaml(bias));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fbgemm_linear_int8_weight_fp32_activation(gc_tensor input, gc_tensor weight, gc_tensor packed, gc_tensor col_offsets, scalar weight_scale, scalar weight_zero_point, gc_tensor bias) {
  PROTECT(
    torch::Tensor results__ = torch::fbgemm_linear_int8_weight_fp32_activation(tensor_from_ocaml(input), tensor_from_ocaml(weight), tensor_from_ocaml(packed), tensor_from_ocaml(col_offsets), *weight_scale, *weight_zero_point, tensor_from_ocaml(bias));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fbgemm_pack_gemm_matrix_fp16(gc_tensor input) {
  PROTECT(
    torch::Tensor results__ = torch::fbgemm_pack_gemm_matrix_fp16(tensor_from_ocaml(input));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fbgemm_pack_quantized_matrix(gc_tensor input) {
  PROTECT(
    torch::Tensor results__ = torch::fbgemm_pack_quantized_matrix(tensor_from_ocaml(input));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fbgemm_pack_quantized_matrix_kn(gc_tensor input, int64_t K, int64_t n) {
  PROTECT(
    torch::Tensor results__ = torch::fbgemm_pack_quantized_matrix(tensor_from_ocaml(input), K, n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_feature_alpha_dropout(gc_tensor input, double p, int train) {
  PROTECT(
    torch::Tensor results__ = torch::feature_alpha_dropout(tensor_from_ocaml(input), p, (bool)train);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_feature_alpha_dropout_(gc_tensor self, double p, int train) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::feature_alpha_dropout_(self_local, p, (bool)train);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_feature_dropout(gc_tensor input, double p, int train) {
  PROTECT(
    torch::Tensor results__ = torch::feature_dropout(tensor_from_ocaml(input), p, (bool)train);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_feature_dropout_(gc_tensor self, double p, int train) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::feature_dropout_(self_local, p, (bool)train);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_fft(gc_tensor self, int64_t n_v, int n_null, int64_t dim, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_fft(tensor_from_ocaml(self), n_null ? c10::nullopt : c10::optional<int64_t>(n_v), dim, norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_fft2(gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_fft2(tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), torch::IntArrayRef(dim_data, dim_len), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_fft2_out(gc_tensor out, gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_fft2_out(out_local, tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), torch::IntArrayRef(dim_data, dim_len), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_fft_out(gc_tensor out, gc_tensor self, int64_t n_v, int n_null, int64_t dim, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_fft_out(out_local, tensor_from_ocaml(self), n_null ? c10::nullopt : c10::optional<int64_t>(n_v), dim, norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_fftfreq(int64_t n, double d, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::fft_fftfreq(n, d, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_fftfreq_out(gc_tensor out, int64_t n, double d) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_fftfreq_out(out_local, n, d);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_fftn(gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_fftn(tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_fftn_out(gc_tensor out, gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_fftn_out(out_local, tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_fftshift(gc_tensor self, int64_t *dim_data, int dim_len) {
  PROTECT(
    torch::Tensor results__ = torch::fft_fftshift(tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_hfft(gc_tensor self, int64_t n_v, int n_null, int64_t dim, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_hfft(tensor_from_ocaml(self), n_null ? c10::nullopt : c10::optional<int64_t>(n_v), dim, norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_hfft2(gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_hfft2(tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), torch::IntArrayRef(dim_data, dim_len), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_hfft2_out(gc_tensor out, gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_hfft2_out(out_local, tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), torch::IntArrayRef(dim_data, dim_len), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_hfft_out(gc_tensor out, gc_tensor self, int64_t n_v, int n_null, int64_t dim, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_hfft_out(out_local, tensor_from_ocaml(self), n_null ? c10::nullopt : c10::optional<int64_t>(n_v), dim, norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_hfftn(gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_hfftn(tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_hfftn_out(gc_tensor out, gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_hfftn_out(out_local, tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_ifft(gc_tensor self, int64_t n_v, int n_null, int64_t dim, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_ifft(tensor_from_ocaml(self), n_null ? c10::nullopt : c10::optional<int64_t>(n_v), dim, norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_ifft2(gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_ifft2(tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), torch::IntArrayRef(dim_data, dim_len), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_ifft2_out(gc_tensor out, gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_ifft2_out(out_local, tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), torch::IntArrayRef(dim_data, dim_len), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_ifft_out(gc_tensor out, gc_tensor self, int64_t n_v, int n_null, int64_t dim, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_ifft_out(out_local, tensor_from_ocaml(self), n_null ? c10::nullopt : c10::optional<int64_t>(n_v), dim, norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_ifftn(gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_ifftn(tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_ifftn_out(gc_tensor out, gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_ifftn_out(out_local, tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_ifftshift(gc_tensor self, int64_t *dim_data, int dim_len) {
  PROTECT(
    torch::Tensor results__ = torch::fft_ifftshift(tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_ihfft(gc_tensor self, int64_t n_v, int n_null, int64_t dim, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_ihfft(tensor_from_ocaml(self), n_null ? c10::nullopt : c10::optional<int64_t>(n_v), dim, norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_ihfft2(gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_ihfft2(tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), torch::IntArrayRef(dim_data, dim_len), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_ihfft2_out(gc_tensor out, gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_ihfft2_out(out_local, tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), torch::IntArrayRef(dim_data, dim_len), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_ihfft_out(gc_tensor out, gc_tensor self, int64_t n_v, int n_null, int64_t dim, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_ihfft_out(out_local, tensor_from_ocaml(self), n_null ? c10::nullopt : c10::optional<int64_t>(n_v), dim, norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_ihfftn(gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_ihfftn(tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_ihfftn_out(gc_tensor out, gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_ihfftn_out(out_local, tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_irfft(gc_tensor self, int64_t n_v, int n_null, int64_t dim, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_irfft(tensor_from_ocaml(self), n_null ? c10::nullopt : c10::optional<int64_t>(n_v), dim, norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_irfft2(gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_irfft2(tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), torch::IntArrayRef(dim_data, dim_len), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_irfft2_out(gc_tensor out, gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_irfft2_out(out_local, tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), torch::IntArrayRef(dim_data, dim_len), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_irfft_out(gc_tensor out, gc_tensor self, int64_t n_v, int n_null, int64_t dim, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_irfft_out(out_local, tensor_from_ocaml(self), n_null ? c10::nullopt : c10::optional<int64_t>(n_v), dim, norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_irfftn(gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_irfftn(tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_irfftn_out(gc_tensor out, gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_irfftn_out(out_local, tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_rfft(gc_tensor self, int64_t n_v, int n_null, int64_t dim, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_rfft(tensor_from_ocaml(self), n_null ? c10::nullopt : c10::optional<int64_t>(n_v), dim, norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_rfft2(gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_rfft2(tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), torch::IntArrayRef(dim_data, dim_len), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_rfft2_out(gc_tensor out, gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_rfft2_out(out_local, tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), torch::IntArrayRef(dim_data, dim_len), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_rfft_out(gc_tensor out, gc_tensor self, int64_t n_v, int n_null, int64_t dim, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_rfft_out(out_local, tensor_from_ocaml(self), n_null ? c10::nullopt : c10::optional<int64_t>(n_v), dim, norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_rfftfreq(int64_t n, double d, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::fft_rfftfreq(n, d, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_rfftfreq_out(gc_tensor out, int64_t n, double d) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_rfftfreq_out(out_local, n, d);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_rfftn(gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  PROTECT(
    torch::Tensor results__ = torch::fft_rfftn(tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fft_rfftn_out(gc_tensor out, gc_tensor self, int64_t *s_data, int s_len, int64_t *dim_data, int dim_len, char * norm_v, int norm_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fft_rfftn_out(out_local, tensor_from_ocaml(self), s_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(s_data, s_len)), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), norm_null ? c10::nullopt : c10::optional<c10::string_view>(norm_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fill(gc_tensor self, scalar value) {
  PROTECT(
    torch::Tensor results__ = torch::fill(tensor_from_ocaml(self), *value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fill_(gc_tensor self, scalar value) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::fill_(self_local, *value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fill_diagonal_(gc_tensor self, scalar fill_value, int wrap) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).fill_diagonal_(*fill_value, (bool)wrap);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fill_scalar_out(gc_tensor out, gc_tensor self, scalar value) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fill_out(out_local, tensor_from_ocaml(self), *value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fill_tensor(gc_tensor self, gc_tensor value) {
  PROTECT(
    torch::Tensor results__ = torch::fill(tensor_from_ocaml(self), tensor_from_ocaml(value));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fill_tensor_(gc_tensor self, gc_tensor value) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::fill_(self_local, tensor_from_ocaml(value));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fill_tensor_out(gc_tensor out, gc_tensor self, gc_tensor value) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fill_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(value));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fix(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::fix(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fix_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::fix_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fix_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fix_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_flatten(gc_tensor self, int64_t start_dim, int64_t end_dim) {
  PROTECT(
    torch::Tensor results__ = torch::flatten(tensor_from_ocaml(self), start_dim, end_dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_flatten_dense_tensors(gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    torch::Tensor results__ = torch::flatten_dense_tensors(of_carray_tensor(tensors_data, tensors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_flip(gc_tensor self, int64_t *dims_data, int dims_len) {
  PROTECT(
    torch::Tensor results__ = torch::flip(tensor_from_ocaml(self), torch::IntArrayRef(dims_data, dims_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_flip_out(gc_tensor out, gc_tensor self, int64_t *dims_data, int dims_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::flip_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(dims_data, dims_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fliplr(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::fliplr(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_flipud(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::flipud(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_float_power(gc_tensor self, gc_tensor exponent) {
  PROTECT(
    torch::Tensor results__ = torch::float_power(tensor_from_ocaml(self), tensor_from_ocaml(exponent));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_float_power_(gc_tensor self, scalar exponent) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).float_power_(*exponent);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_float_power_scalar(scalar self, gc_tensor exponent) {
  PROTECT(
    torch::Tensor results__ = torch::float_power(*self, tensor_from_ocaml(exponent));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_float_power_scalar_out(gc_tensor out, scalar self, gc_tensor exponent) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::float_power_out(out_local, *self, tensor_from_ocaml(exponent));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_float_power_tensor_(gc_tensor self, gc_tensor exponent) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).float_power_(tensor_from_ocaml(exponent));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_float_power_tensor_scalar(gc_tensor self, scalar exponent) {
  PROTECT(
    torch::Tensor results__ = torch::float_power(tensor_from_ocaml(self), *exponent);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_float_power_tensor_scalar_out(gc_tensor out, gc_tensor self, scalar exponent) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::float_power_out(out_local, tensor_from_ocaml(self), *exponent);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_float_power_tensor_tensor_out(gc_tensor out, gc_tensor self, gc_tensor exponent) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::float_power_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(exponent));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_floor(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::floor(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_floor_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::floor_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_floor_divide(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::floor_divide(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_floor_divide_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).floor_divide_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_floor_divide_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::floor_divide_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_floor_divide_scalar(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::floor_divide(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_floor_divide_scalar_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).floor_divide_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_floor_divide_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::floor_divide_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_floor_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::floor_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fmax(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::fmax(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fmax_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fmax_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fmin(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::fmin(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fmin_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fmin_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fmod(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::fmod(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fmod_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).fmod_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fmod_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fmod_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fmod_tensor(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::fmod(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fmod_tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).fmod_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fmod_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::fmod_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_frac(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::frac(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_frac_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::frac_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_frac_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::frac_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

void atg_fractional_max_pool2d(raw_tensor *out__, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *output_size_data, int output_size_len, gc_tensor random_samples) {
  PROTECT(
    auto results__ = torch::fractional_max_pool2d(tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(output_size_data, output_size_len), tensor_from_ocaml(random_samples));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_fractional_max_pool2d_backward(gc_tensor grad_output, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *output_size_data, int output_size_len, gc_tensor indices) {
  PROTECT(
    torch::Tensor results__ = torch::fractional_max_pool2d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(output_size_data, output_size_len), tensor_from_ocaml(indices));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fractional_max_pool2d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *output_size_data, int output_size_len, gc_tensor indices) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::fractional_max_pool2d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(output_size_data, output_size_len), tensor_from_ocaml(indices));
    return tensor_to_ocaml(results__);
  )
}

void atg_fractional_max_pool2d_output(raw_tensor *out__, gc_tensor output, gc_tensor indices, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *output_size_data, int output_size_len, gc_tensor random_samples) {
  torch::Tensor output_local = tensor_from_ocaml(output);
  torch::Tensor indices_local = tensor_from_ocaml(indices);
  PROTECT(
    auto results__ = torch::fractional_max_pool2d_out(output_local, indices_local, tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(output_size_data, output_size_len), tensor_from_ocaml(random_samples));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_fractional_max_pool3d(raw_tensor *out__, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *output_size_data, int output_size_len, gc_tensor random_samples) {
  PROTECT(
    auto results__ = torch::fractional_max_pool3d(tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(output_size_data, output_size_len), tensor_from_ocaml(random_samples));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_fractional_max_pool3d_backward(gc_tensor grad_output, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *output_size_data, int output_size_len, gc_tensor indices) {
  PROTECT(
    torch::Tensor results__ = torch::fractional_max_pool3d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(output_size_data, output_size_len), tensor_from_ocaml(indices));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fractional_max_pool3d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *output_size_data, int output_size_len, gc_tensor indices) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::fractional_max_pool3d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(output_size_data, output_size_len), tensor_from_ocaml(indices));
    return tensor_to_ocaml(results__);
  )
}

void atg_fractional_max_pool3d_output(raw_tensor *out__, gc_tensor output, gc_tensor indices, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *output_size_data, int output_size_len, gc_tensor random_samples) {
  torch::Tensor output_local = tensor_from_ocaml(output);
  torch::Tensor indices_local = tensor_from_ocaml(indices);
  PROTECT(
    auto results__ = torch::fractional_max_pool3d_out(output_local, indices_local, tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(output_size_data, output_size_len), tensor_from_ocaml(random_samples));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_frexp(raw_tensor *out__, gc_tensor self) {
  PROTECT(
    auto results__ = torch::frexp(tensor_from_ocaml(self));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_frexp_tensor_out(raw_tensor *out__, gc_tensor mantissa, gc_tensor exponent, gc_tensor self) {
  torch::Tensor mantissa_local = tensor_from_ocaml(mantissa);
  torch::Tensor exponent_local = tensor_from_ocaml(exponent);
  PROTECT(
    auto results__ = torch::frexp_out(mantissa_local, exponent_local, tensor_from_ocaml(self));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_frobenius_norm(gc_tensor self, int64_t *dim_data, int dim_len, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::frobenius_norm(tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_frobenius_norm_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::frobenius_norm_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_from_file(char * filename, int shared, int64_t size_v, int size_null, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::from_file(std::string(filename), (bool)shared, size_null ? c10::nullopt : c10::optional<int64_t>(size_v), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_from_file_out(gc_tensor out, char * filename, int shared, int64_t size_v, int size_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::from_file_out(out_local, std::string(filename), (bool)shared, size_null ? c10::nullopt : c10::optional<int64_t>(size_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_full(int64_t *size_data, int size_len, scalar fill_value, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::full(torch::IntArrayRef(size_data, size_len), *fill_value, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_full_like(gc_tensor self, scalar fill_value) {
  PROTECT(
    torch::Tensor results__ = torch::full_like(tensor_from_ocaml(self), *fill_value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_full_like_out(gc_tensor out, gc_tensor self, scalar fill_value) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::full_like_out(out_local, tensor_from_ocaml(self), *fill_value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_full_out(gc_tensor out, int64_t *size_data, int size_len, scalar fill_value) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::full_out(out_local, torch::IntArrayRef(size_data, size_len), *fill_value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_fused_moving_avg_obs_fake_quant(gc_tensor self, gc_tensor observer_on, gc_tensor fake_quant_on, gc_tensor running_min, gc_tensor running_max, gc_tensor scale, gc_tensor zero_point, double averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, int per_row_fake_quant, int symmetric_quant) {
  torch::Tensor running_min_local = tensor_from_ocaml(running_min);
  torch::Tensor running_max_local = tensor_from_ocaml(running_max);
  torch::Tensor scale_local = tensor_from_ocaml(scale);
  torch::Tensor zero_point_local = tensor_from_ocaml(zero_point);
  PROTECT(
    torch::Tensor results__ = torch::fused_moving_avg_obs_fake_quant(tensor_from_ocaml(self), tensor_from_ocaml(observer_on), tensor_from_ocaml(fake_quant_on), running_min_local, running_max_local, scale_local, zero_point_local, averaging_const, quant_min, quant_max, ch_axis, (bool)per_row_fake_quant, (bool)symmetric_quant);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_gather(gc_tensor self, int64_t dim, gc_tensor index, int sparse_grad) {
  PROTECT(
    torch::Tensor results__ = torch::gather(tensor_from_ocaml(self), dim, tensor_from_ocaml(index), (bool)sparse_grad);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_gather_backward(gc_tensor grad, gc_tensor self, int64_t dim, gc_tensor index, int sparse_grad) {
  PROTECT(
    torch::Tensor results__ = torch::gather_backward(tensor_from_ocaml(grad), tensor_from_ocaml(self), dim, tensor_from_ocaml(index), (bool)sparse_grad);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_gather_out(gc_tensor out, gc_tensor self, int64_t dim, gc_tensor index, int sparse_grad) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::gather_out(out_local, tensor_from_ocaml(self), dim, tensor_from_ocaml(index), (bool)sparse_grad);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_gcd(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::gcd(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_gcd_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::gcd_(self_local, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_gcd_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::gcd_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ge(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::ge(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ge_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).ge_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ge_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::ge_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ge_tensor(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::ge(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ge_tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).ge_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ge_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::ge_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_gelu(gc_tensor self, char * approximate) {
  PROTECT(
    torch::Tensor results__ = torch::gelu(tensor_from_ocaml(self), std::string(approximate));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_gelu_(gc_tensor self, char * approximate) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::gelu_(self_local, std::string(approximate));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_gelu_backward(gc_tensor grad_output, gc_tensor self, char * approximate) {
  PROTECT(
    torch::Tensor results__ = torch::gelu_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), std::string(approximate));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_gelu_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, char * approximate) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::gelu_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), std::string(approximate));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_gelu_out(gc_tensor out, gc_tensor self, char * approximate) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::gelu_out(out_local, tensor_from_ocaml(self), std::string(approximate));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_geometric(gc_tensor self, double p) {
  PROTECT(
    torch::Tensor results__ = torch::geometric(tensor_from_ocaml(self), p);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_geometric_(gc_tensor self, double p) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).geometric_(p);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_geometric_out(gc_tensor out, gc_tensor self, double p) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::geometric_out(out_local, tensor_from_ocaml(self), p);
    return tensor_to_ocaml(results__);
  )
}

void atg_geqrf(raw_tensor *out__, gc_tensor self) {
  PROTECT(
    auto results__ = torch::geqrf(tensor_from_ocaml(self));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_geqrf_a(raw_tensor *out__, gc_tensor a, gc_tensor tau, gc_tensor self) {
  torch::Tensor a_local = tensor_from_ocaml(a);
  torch::Tensor tau_local = tensor_from_ocaml(tau);
  PROTECT(
    auto results__ = torch::geqrf_out(a_local, tau_local, tensor_from_ocaml(self));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_ger(gc_tensor self, gc_tensor vec2) {
  PROTECT(
    torch::Tensor results__ = torch::ger(tensor_from_ocaml(self), tensor_from_ocaml(vec2));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ger_out(gc_tensor out, gc_tensor self, gc_tensor vec2) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::ger_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(vec2));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_glu(gc_tensor self, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::glu(tensor_from_ocaml(self), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_glu_backward(gc_tensor grad_output, gc_tensor self, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::glu_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_glu_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, int64_t dim) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::glu_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_glu_backward_jvp(gc_tensor grad_x, gc_tensor grad_glu, gc_tensor x, gc_tensor dgrad_glu, gc_tensor dx, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::glu_backward_jvp(tensor_from_ocaml(grad_x), tensor_from_ocaml(grad_glu), tensor_from_ocaml(x), tensor_from_ocaml(dgrad_glu), tensor_from_ocaml(dx), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_glu_backward_jvp_out(gc_tensor out, gc_tensor grad_x, gc_tensor grad_glu, gc_tensor x, gc_tensor dgrad_glu, gc_tensor dx, int64_t dim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::glu_backward_jvp_out(out_local, tensor_from_ocaml(grad_x), tensor_from_ocaml(grad_glu), tensor_from_ocaml(x), tensor_from_ocaml(dgrad_glu), tensor_from_ocaml(dx), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_glu_jvp(gc_tensor glu, gc_tensor x, gc_tensor dx, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::glu_jvp(tensor_from_ocaml(glu), tensor_from_ocaml(x), tensor_from_ocaml(dx), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_glu_jvp_out(gc_tensor out, gc_tensor glu, gc_tensor x, gc_tensor dx, int64_t dim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::glu_jvp_out(out_local, tensor_from_ocaml(glu), tensor_from_ocaml(x), tensor_from_ocaml(dx), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_glu_out(gc_tensor out, gc_tensor self, int64_t dim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::glu_out(out_local, tensor_from_ocaml(self), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_grad(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).grad();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_greater(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::greater(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_greater_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).greater_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_greater_equal(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::greater_equal(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_greater_equal_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).greater_equal_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_greater_equal_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::greater_equal_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_greater_equal_tensor(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::greater_equal(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_greater_equal_tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).greater_equal_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_greater_equal_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::greater_equal_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_greater_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::greater_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_greater_tensor(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::greater(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_greater_tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).greater_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_greater_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::greater_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_grid_sampler(gc_tensor input, gc_tensor grid, int64_t interpolation_mode, int64_t padding_mode, int align_corners) {
  PROTECT(
    torch::Tensor results__ = torch::grid_sampler(tensor_from_ocaml(input), tensor_from_ocaml(grid), interpolation_mode, padding_mode, (bool)align_corners);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_grid_sampler_2d(gc_tensor input, gc_tensor grid, int64_t interpolation_mode, int64_t padding_mode, int align_corners) {
  PROTECT(
    torch::Tensor results__ = torch::grid_sampler_2d(tensor_from_ocaml(input), tensor_from_ocaml(grid), interpolation_mode, padding_mode, (bool)align_corners);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_grid_sampler_2d_out(gc_tensor out, gc_tensor input, gc_tensor grid, int64_t interpolation_mode, int64_t padding_mode, int align_corners) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::grid_sampler_2d_out(out_local, tensor_from_ocaml(input), tensor_from_ocaml(grid), interpolation_mode, padding_mode, (bool)align_corners);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_grid_sampler_3d(gc_tensor input, gc_tensor grid, int64_t interpolation_mode, int64_t padding_mode, int align_corners) {
  PROTECT(
    torch::Tensor results__ = torch::grid_sampler_3d(tensor_from_ocaml(input), tensor_from_ocaml(grid), interpolation_mode, padding_mode, (bool)align_corners);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_grid_sampler_3d_out(gc_tensor out, gc_tensor input, gc_tensor grid, int64_t interpolation_mode, int64_t padding_mode, int align_corners) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::grid_sampler_3d_out(out_local, tensor_from_ocaml(input), tensor_from_ocaml(grid), interpolation_mode, padding_mode, (bool)align_corners);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_group_norm(gc_tensor input, int64_t num_groups, gc_tensor weight, gc_tensor bias, double eps, int cudnn_enabled) {
  PROTECT(
    torch::Tensor results__ = torch::group_norm(tensor_from_ocaml(input), num_groups, weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, eps, (bool)cudnn_enabled);
    return tensor_to_ocaml(results__);
  )
}

void atg_gru(raw_tensor *out__, gc_tensor input, gc_tensor hx, gc_tensor *params_data, int params_len, int has_biases, int64_t num_layers, double dropout, int train, int bidirectional, int batch_first) {
  PROTECT(
    auto results__ = torch::gru(tensor_from_ocaml(input), tensor_from_ocaml(hx), of_carray_tensor(params_data, params_len), (bool)has_biases, num_layers, dropout, (bool)train, (bool)bidirectional, (bool)batch_first);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_gru_cell(gc_tensor input, gc_tensor hx, gc_tensor w_ih, gc_tensor w_hh, gc_tensor b_ih, gc_tensor b_hh) {
  PROTECT(
    torch::Tensor results__ = torch::gru_cell(tensor_from_ocaml(input), tensor_from_ocaml(hx), tensor_from_ocaml(w_ih), tensor_from_ocaml(w_hh), b_ih ? std::make_optional(tensor_from_ocaml(b_ih)) : std::nullopt, b_hh ? std::make_optional(tensor_from_ocaml(b_hh)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

void atg_gru_data(raw_tensor *out__, gc_tensor data, gc_tensor batch_sizes, gc_tensor hx, gc_tensor *params_data, int params_len, int has_biases, int64_t num_layers, double dropout, int train, int bidirectional) {
  PROTECT(
    auto results__ = torch::gru(tensor_from_ocaml(data), tensor_from_ocaml(batch_sizes), tensor_from_ocaml(hx), of_carray_tensor(params_data, params_len), (bool)has_biases, num_layers, dropout, (bool)train, (bool)bidirectional);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_gt(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::gt(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_gt_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).gt_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_gt_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::gt_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_gt_tensor(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::gt(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_gt_tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).gt_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_gt_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::gt_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hamming_window(int64_t window_length, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::hamming_window(window_length, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hamming_window_out(gc_tensor out, int64_t window_length) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::hamming_window_out(out_local, window_length);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hamming_window_periodic(int64_t window_length, int periodic, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::hamming_window(window_length, (bool)periodic, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hamming_window_periodic_alpha(int64_t window_length, int periodic, double alpha, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::hamming_window(window_length, (bool)periodic, alpha, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hamming_window_periodic_alpha_beta(int64_t window_length, int periodic, double alpha, double beta, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::hamming_window(window_length, (bool)periodic, alpha, beta, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hamming_window_periodic_alpha_beta_out(gc_tensor out, int64_t window_length, int periodic, double alpha, double beta) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::hamming_window_out(out_local, window_length, (bool)periodic, alpha, beta);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hamming_window_periodic_alpha_out(gc_tensor out, int64_t window_length, int periodic, double alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::hamming_window_out(out_local, window_length, (bool)periodic, alpha);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hamming_window_periodic_out(gc_tensor out, int64_t window_length, int periodic) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::hamming_window_out(out_local, window_length, (bool)periodic);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hann_window(int64_t window_length, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::hann_window(window_length, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hann_window_out(gc_tensor out, int64_t window_length) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::hann_window_out(out_local, window_length);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hann_window_periodic(int64_t window_length, int periodic, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::hann_window(window_length, (bool)periodic, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hann_window_periodic_out(gc_tensor out, int64_t window_length, int periodic) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::hann_window_out(out_local, window_length, (bool)periodic);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardshrink(gc_tensor self, scalar lambd) {
  PROTECT(
    torch::Tensor results__ = torch::hardshrink(tensor_from_ocaml(self), lambd ? *lambd : c10::Scalar{0.5} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardshrink_backward(gc_tensor grad_out, gc_tensor self, scalar lambd) {
  PROTECT(
    torch::Tensor results__ = torch::hardshrink_backward(tensor_from_ocaml(grad_out), tensor_from_ocaml(self), *lambd);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardshrink_backward_grad_input(gc_tensor grad_input, gc_tensor grad_out, gc_tensor self, scalar lambd) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::hardshrink_backward_out(grad_input_local, tensor_from_ocaml(grad_out), tensor_from_ocaml(self), *lambd);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardshrink_out(gc_tensor out, gc_tensor self, scalar lambd) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::hardshrink_out(out_local, tensor_from_ocaml(self), lambd ? *lambd : c10::Scalar{0.5} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardsigmoid(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::hardsigmoid(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardsigmoid_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::hardsigmoid_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardsigmoid_backward(gc_tensor grad_output, gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::hardsigmoid_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardsigmoid_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::hardsigmoid_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardsigmoid_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::hardsigmoid_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardswish(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::hardswish(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardswish_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::hardswish_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardswish_backward(gc_tensor grad_output, gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::hardswish_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardswish_backward_out(gc_tensor out, gc_tensor grad_output, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::hardswish_backward_out(out_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardswish_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::hardswish_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardtanh(gc_tensor self, scalar min_val, scalar max_val) {
  PROTECT(
    torch::Tensor results__ = torch::hardtanh(tensor_from_ocaml(self), min_val ? *min_val : c10::Scalar{-1} , max_val ? *max_val : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardtanh_(gc_tensor self, scalar min_val, scalar max_val) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::hardtanh_(self_local, min_val ? *min_val : c10::Scalar{-1} , max_val ? *max_val : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardtanh_backward(gc_tensor grad_output, gc_tensor self, scalar min_val, scalar max_val) {
  PROTECT(
    torch::Tensor results__ = torch::hardtanh_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), *min_val, *max_val);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardtanh_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, scalar min_val, scalar max_val) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::hardtanh_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), *min_val, *max_val);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hardtanh_out(gc_tensor out, gc_tensor self, scalar min_val, scalar max_val) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::hardtanh_out(out_local, tensor_from_ocaml(self), min_val ? *min_val : c10::Scalar{-1} , max_val ? *max_val : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_heaviside(gc_tensor self, gc_tensor values) {
  PROTECT(
    torch::Tensor results__ = torch::heaviside(tensor_from_ocaml(self), tensor_from_ocaml(values));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_heaviside_(gc_tensor self, gc_tensor values) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).heaviside_(tensor_from_ocaml(values));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_heaviside_out(gc_tensor out, gc_tensor self, gc_tensor values) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::heaviside_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(values));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hinge_embedding_loss(gc_tensor self, gc_tensor target, double margin, int64_t reduction) {
  PROTECT(
    torch::Tensor results__ = torch::hinge_embedding_loss(tensor_from_ocaml(self), tensor_from_ocaml(target), margin, reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_histc(gc_tensor self, int64_t bins, scalar min, scalar max) {
  PROTECT(
    torch::Tensor results__ = torch::histc(tensor_from_ocaml(self), bins, min ? *min : c10::Scalar{0} , max ? *max : c10::Scalar{0} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_histc_out(gc_tensor out, gc_tensor self, int64_t bins, scalar min, scalar max) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::histc_out(out_local, tensor_from_ocaml(self), bins, min ? *min : c10::Scalar{0} , max ? *max : c10::Scalar{0} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg_hsplit(gc_tensor self, int64_t sections) {
  PROTECT(
    auto results__ = torch::hsplit(tensor_from_ocaml(self), sections);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor *atg_hsplit_array(gc_tensor self, int64_t *indices_data, int indices_len) {
  PROTECT(
    auto results__ = torch::hsplit(tensor_from_ocaml(self), torch::IntArrayRef(indices_data, indices_len));
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor atg_hspmm(gc_tensor mat1, gc_tensor mat2) {
  PROTECT(
    torch::Tensor results__ = torch::hspmm(tensor_from_ocaml(mat1), tensor_from_ocaml(mat2));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hspmm_out(gc_tensor out, gc_tensor mat1, gc_tensor mat2) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::hspmm_out(out_local, tensor_from_ocaml(mat1), tensor_from_ocaml(mat2));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hstack(gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    torch::Tensor results__ = torch::hstack(of_carray_tensor(tensors_data, tensors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hstack_out(gc_tensor out, gc_tensor *tensors_data, int tensors_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::hstack_out(out_local, of_carray_tensor(tensors_data, tensors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_huber_loss(gc_tensor self, gc_tensor target, int64_t reduction, double delta) {
  PROTECT(
    torch::Tensor results__ = torch::huber_loss(tensor_from_ocaml(self), tensor_from_ocaml(target), reduction, delta);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_huber_loss_backward(gc_tensor grad_output, gc_tensor self, gc_tensor target, int64_t reduction, double delta) {
  PROTECT(
    torch::Tensor results__ = torch::huber_loss_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), reduction, delta);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_huber_loss_backward_out(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, gc_tensor target, int64_t reduction, double delta) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::huber_loss_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), reduction, delta);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_huber_loss_out(gc_tensor out, gc_tensor self, gc_tensor target, int64_t reduction, double delta) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::huber_loss_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(target), reduction, delta);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hypot(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::hypot(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hypot_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).hypot_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_hypot_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::hypot_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_i0(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::i0(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_i0_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::i0_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_i0_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::i0_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_igamma(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::igamma(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_igamma_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).igamma_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_igamma_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::igamma_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_igammac(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::igammac(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_igammac_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).igammac_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_igammac_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::igammac_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_im2col(gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *dilation_data, int dilation_len, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len) {
  PROTECT(
    torch::Tensor results__ = torch::im2col(tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(dilation_data, dilation_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_im2col_out(gc_tensor out, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *dilation_data, int dilation_len, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::im2col_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(dilation_data, dilation_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_imag(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::imag(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_add(gc_tensor self, int64_t dim, gc_tensor index, gc_tensor source, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::index_add(tensor_from_ocaml(self), dim, tensor_from_ocaml(index), tensor_from_ocaml(source), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_add_(gc_tensor self, int64_t dim, gc_tensor index, gc_tensor source, scalar alpha) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).index_add_(dim, tensor_from_ocaml(index), tensor_from_ocaml(source), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_add_out(gc_tensor out, gc_tensor self, int64_t dim, gc_tensor index, gc_tensor source, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::index_add_out(out_local, tensor_from_ocaml(self), dim, tensor_from_ocaml(index), tensor_from_ocaml(source), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_copy(gc_tensor self, int64_t dim, gc_tensor index, gc_tensor source) {
  PROTECT(
    torch::Tensor results__ = torch::index_copy(tensor_from_ocaml(self), dim, tensor_from_ocaml(index), tensor_from_ocaml(source));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_copy_(gc_tensor self, int64_t dim, gc_tensor index, gc_tensor source) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).index_copy_(dim, tensor_from_ocaml(index), tensor_from_ocaml(source));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_copy_out(gc_tensor out, gc_tensor self, int64_t dim, gc_tensor index, gc_tensor source) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::index_copy_out(out_local, tensor_from_ocaml(self), dim, tensor_from_ocaml(index), tensor_from_ocaml(source));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_fill(gc_tensor self, int64_t dim, gc_tensor index, scalar value) {
  PROTECT(
    torch::Tensor results__ = torch::index_fill(tensor_from_ocaml(self), dim, tensor_from_ocaml(index), *value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_fill_(gc_tensor self, int64_t dim, gc_tensor index, scalar value) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).index_fill_(dim, tensor_from_ocaml(index), *value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_fill_int_scalar_out(gc_tensor out, gc_tensor self, int64_t dim, gc_tensor index, scalar value) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::index_fill_out(out_local, tensor_from_ocaml(self), dim, tensor_from_ocaml(index), *value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_fill_int_tensor(gc_tensor self, int64_t dim, gc_tensor index, gc_tensor value) {
  PROTECT(
    torch::Tensor results__ = torch::index_fill(tensor_from_ocaml(self), dim, tensor_from_ocaml(index), tensor_from_ocaml(value));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_fill_int_tensor_(gc_tensor self, int64_t dim, gc_tensor index, gc_tensor value) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).index_fill_(dim, tensor_from_ocaml(index), tensor_from_ocaml(value));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_fill_int_tensor_out(gc_tensor out, gc_tensor self, int64_t dim, gc_tensor index, gc_tensor value) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::index_fill_out(out_local, tensor_from_ocaml(self), dim, tensor_from_ocaml(index), tensor_from_ocaml(value));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_reduce(gc_tensor self, int64_t dim, gc_tensor index, gc_tensor source, char * reduce, int include_self) {
  PROTECT(
    torch::Tensor results__ = torch::index_reduce(tensor_from_ocaml(self), dim, tensor_from_ocaml(index), tensor_from_ocaml(source), std::string(reduce), (bool)include_self);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_reduce_(gc_tensor self, int64_t dim, gc_tensor index, gc_tensor source, char * reduce, int include_self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).index_reduce_(dim, tensor_from_ocaml(index), tensor_from_ocaml(source), std::string(reduce), (bool)include_self);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_reduce_out(gc_tensor out, gc_tensor self, int64_t dim, gc_tensor index, gc_tensor source, char * reduce, int include_self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::index_reduce_out(out_local, tensor_from_ocaml(self), dim, tensor_from_ocaml(index), tensor_from_ocaml(source), std::string(reduce), (bool)include_self);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_select(gc_tensor self, int64_t dim, gc_tensor index) {
  PROTECT(
    torch::Tensor results__ = torch::index_select(tensor_from_ocaml(self), dim, tensor_from_ocaml(index));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_select_backward(gc_tensor grad, int64_t *self_sizes_data, int self_sizes_len, int64_t dim, gc_tensor index) {
  PROTECT(
    torch::Tensor results__ = torch::index_select_backward(tensor_from_ocaml(grad), torch::IntArrayRef(self_sizes_data, self_sizes_len), dim, tensor_from_ocaml(index));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_index_select_out(gc_tensor out, gc_tensor self, int64_t dim, gc_tensor index) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::index_select_out(out_local, tensor_from_ocaml(self), dim, tensor_from_ocaml(index));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_indices(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).indices();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_indices_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::indices_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_indices_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::indices_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_infinitely_differentiable_gelu_backward(gc_tensor grad, gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::infinitely_differentiable_gelu_backward(tensor_from_ocaml(grad), tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_inner(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::inner(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_inner_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::inner_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_instance_norm(gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, int use_input_stats, double momentum, double eps, int cudnn_enabled) {
  PROTECT(
    torch::Tensor results__ = torch::instance_norm(tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, (bool)use_input_stats, momentum, eps, (bool)cudnn_enabled);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_int_repr(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::int_repr(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_int_repr_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::int_repr_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_inverse(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::inverse(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_inverse_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::inverse_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

int atg_is_coalesced(gc_tensor self) {
  PROTECT(
    return tensor_from_ocaml(self).is_coalesced();
  )
  return 0;
}

int atg_is_complex(gc_tensor self) {
  PROTECT(
    return torch::is_complex(tensor_from_ocaml(self));
  )
  return 0;
}

int atg_is_conj(gc_tensor self) {
  PROTECT(
    return torch::is_conj(tensor_from_ocaml(self));
  )
  return 0;
}

int atg_is_distributed(gc_tensor self) {
  PROTECT(
    return torch::is_distributed(tensor_from_ocaml(self));
  )
  return 0;
}

int atg_is_floating_point(gc_tensor self) {
  PROTECT(
    return torch::is_floating_point(tensor_from_ocaml(self));
  )
  return 0;
}

int atg_is_inference(gc_tensor self) {
  PROTECT(
    return torch::is_inference(tensor_from_ocaml(self));
  )
  return 0;
}

int atg_is_leaf(gc_tensor self) {
  PROTECT(
    return tensor_from_ocaml(self).is_leaf();
  )
  return 0;
}

int atg_is_neg(gc_tensor self) {
  PROTECT(
    return torch::is_neg(tensor_from_ocaml(self));
  )
  return 0;
}

int atg_is_nonzero(gc_tensor self) {
  PROTECT(
    return torch::is_nonzero(tensor_from_ocaml(self));
  )
  return 0;
}

int atg_is_pinned(gc_tensor self, int device) {
  PROTECT(
    return tensor_from_ocaml(self).is_pinned(optional_device_of_int(device));
  )
  return 0;
}

int atg_is_same_size(gc_tensor self, gc_tensor other) {
  PROTECT(
    return torch::is_same_size(tensor_from_ocaml(self), tensor_from_ocaml(other));
  )
  return 0;
}

int atg_is_set_to(gc_tensor self, gc_tensor tensor) {
  PROTECT(
    return tensor_from_ocaml(self).is_set_to(tensor_from_ocaml(tensor));
  )
  return 0;
}

int atg_is_signed(gc_tensor self) {
  PROTECT(
    return torch::is_signed(tensor_from_ocaml(self));
  )
  return 0;
}

int atg_is_vulkan_available() {
  PROTECT(
    return torch::is_vulkan_available();
  )
  return 0;
}

raw_tensor atg_isclose(gc_tensor self, gc_tensor other, double rtol, double atol, int equal_nan) {
  PROTECT(
    torch::Tensor results__ = torch::isclose(tensor_from_ocaml(self), tensor_from_ocaml(other), rtol, atol, (bool)equal_nan);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_isfinite(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::isfinite(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_isin(gc_tensor elements, gc_tensor test_elements, int assume_unique, int invert) {
  PROTECT(
    torch::Tensor results__ = torch::isin(tensor_from_ocaml(elements), tensor_from_ocaml(test_elements), (bool)assume_unique, (bool)invert);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_isin_scalar_tensor(scalar element, gc_tensor test_elements, int assume_unique, int invert) {
  PROTECT(
    torch::Tensor results__ = torch::isin(*element, tensor_from_ocaml(test_elements), (bool)assume_unique, (bool)invert);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_isin_scalar_tensor_out(gc_tensor out, scalar element, gc_tensor test_elements, int assume_unique, int invert) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::isin_out(out_local, *element, tensor_from_ocaml(test_elements), (bool)assume_unique, (bool)invert);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_isin_tensor_scalar(gc_tensor elements, scalar test_element, int assume_unique, int invert) {
  PROTECT(
    torch::Tensor results__ = torch::isin(tensor_from_ocaml(elements), *test_element, (bool)assume_unique, (bool)invert);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_isin_tensor_scalar_out(gc_tensor out, gc_tensor elements, scalar test_element, int assume_unique, int invert) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::isin_out(out_local, tensor_from_ocaml(elements), *test_element, (bool)assume_unique, (bool)invert);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_isin_tensor_tensor_out(gc_tensor out, gc_tensor elements, gc_tensor test_elements, int assume_unique, int invert) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::isin_out(out_local, tensor_from_ocaml(elements), tensor_from_ocaml(test_elements), (bool)assume_unique, (bool)invert);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_isinf(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::isinf(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_isinf_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::isinf_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_isnan(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::isnan(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_isnan_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::isnan_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_isneginf(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::isneginf(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_isneginf_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::isneginf_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_isposinf(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::isposinf(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_isposinf_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::isposinf_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_isreal(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::isreal(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_istft(gc_tensor self, int64_t n_fft, int64_t hop_length_v, int hop_length_null, int64_t win_length_v, int win_length_null, gc_tensor window, int center, int normalized, int onesided, int64_t length_v, int length_null, int return_complex) {
  PROTECT(
    torch::Tensor results__ = torch::istft(tensor_from_ocaml(self), n_fft, hop_length_null ? c10::nullopt : c10::optional<int64_t>(hop_length_v), win_length_null ? c10::nullopt : c10::optional<int64_t>(win_length_v), window ? std::make_optional(tensor_from_ocaml(window)) : std::nullopt, (bool)center, (bool)normalized, (bool)onesided, length_null ? c10::nullopt : c10::optional<int64_t>(length_v), (bool)return_complex);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_kaiser_window(int64_t window_length, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::kaiser_window(window_length, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_kaiser_window_beta(int64_t window_length, int periodic, double beta, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::kaiser_window(window_length, (bool)periodic, beta, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_kaiser_window_beta_out(gc_tensor out, int64_t window_length, int periodic, double beta) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::kaiser_window_out(out_local, window_length, (bool)periodic, beta);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_kaiser_window_out(gc_tensor out, int64_t window_length) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::kaiser_window_out(out_local, window_length);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_kaiser_window_periodic(int64_t window_length, int periodic, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::kaiser_window(window_length, (bool)periodic, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_kaiser_window_periodic_out(gc_tensor out, int64_t window_length, int periodic) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::kaiser_window_out(out_local, window_length, (bool)periodic);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_kl_div(gc_tensor self, gc_tensor target, int64_t reduction, int log_target) {
  PROTECT(
    torch::Tensor results__ = torch::kl_div(tensor_from_ocaml(self), tensor_from_ocaml(target), reduction, (bool)log_target);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_kron(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::kron(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_kron_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::kron_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

void atg_kthvalue(raw_tensor *out__, gc_tensor self, int64_t k, int64_t dim, int keepdim) {
  PROTECT(
    auto results__ = torch::kthvalue(tensor_from_ocaml(self), k, dim, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_kthvalue_values(raw_tensor *out__, gc_tensor values, gc_tensor indices, gc_tensor self, int64_t k, int64_t dim, int keepdim) {
  torch::Tensor values_local = tensor_from_ocaml(values);
  torch::Tensor indices_local = tensor_from_ocaml(indices);
  PROTECT(
    auto results__ = torch::kthvalue_out(values_local, indices_local, tensor_from_ocaml(self), k, dim, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_l1_loss(gc_tensor self, gc_tensor target, int64_t reduction) {
  PROTECT(
    torch::Tensor results__ = torch::l1_loss(tensor_from_ocaml(self), tensor_from_ocaml(target), reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_layer_norm(gc_tensor input, int64_t *normalized_shape_data, int normalized_shape_len, gc_tensor weight, gc_tensor bias, double eps, int cudnn_enable) {
  PROTECT(
    torch::Tensor results__ = torch::layer_norm(tensor_from_ocaml(input), torch::IntArrayRef(normalized_shape_data, normalized_shape_len), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, eps, (bool)cudnn_enable);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lcm(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::lcm(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lcm_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::lcm_(self_local, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lcm_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::lcm_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ldexp(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::ldexp(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ldexp_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::ldexp_(self_local, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ldexp_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::ldexp_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_le(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::le(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_le_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).le_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_le_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::le_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_le_tensor(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::le(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_le_tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).le_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_le_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::le_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_leaky_relu(gc_tensor self, scalar negative_slope) {
  PROTECT(
    torch::Tensor results__ = torch::leaky_relu(tensor_from_ocaml(self), negative_slope ? *negative_slope : c10::Scalar{0.01} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_leaky_relu_(gc_tensor self, scalar negative_slope) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::leaky_relu_(self_local, negative_slope ? *negative_slope : c10::Scalar{0.01} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_leaky_relu_backward(gc_tensor grad_output, gc_tensor self, scalar negative_slope, int self_is_result) {
  PROTECT(
    torch::Tensor results__ = torch::leaky_relu_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), *negative_slope, (bool)self_is_result);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_leaky_relu_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, scalar negative_slope, int self_is_result) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::leaky_relu_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), *negative_slope, (bool)self_is_result);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_leaky_relu_out(gc_tensor out, gc_tensor self, scalar negative_slope) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::leaky_relu_out(out_local, tensor_from_ocaml(self), negative_slope ? *negative_slope : c10::Scalar{0.01} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lerp(gc_tensor self, gc_tensor end, scalar weight) {
  PROTECT(
    torch::Tensor results__ = torch::lerp(tensor_from_ocaml(self), tensor_from_ocaml(end), *weight);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lerp_(gc_tensor self, gc_tensor end, scalar weight) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).lerp_(tensor_from_ocaml(end), *weight);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lerp_scalar_out(gc_tensor out, gc_tensor self, gc_tensor end, scalar weight) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::lerp_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(end), *weight);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lerp_tensor(gc_tensor self, gc_tensor end, gc_tensor weight) {
  PROTECT(
    torch::Tensor results__ = torch::lerp(tensor_from_ocaml(self), tensor_from_ocaml(end), tensor_from_ocaml(weight));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lerp_tensor_(gc_tensor self, gc_tensor end, gc_tensor weight) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).lerp_(tensor_from_ocaml(end), tensor_from_ocaml(weight));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lerp_tensor_out(gc_tensor out, gc_tensor self, gc_tensor end, gc_tensor weight) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::lerp_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(end), tensor_from_ocaml(weight));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_less(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::less(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_less_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).less_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_less_equal(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::less_equal(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_less_equal_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).less_equal_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_less_equal_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::less_equal_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_less_equal_tensor(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::less_equal(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_less_equal_tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).less_equal_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_less_equal_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::less_equal_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_less_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::less_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_less_tensor(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::less(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_less_tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).less_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_less_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::less_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lgamma(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::lgamma(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lgamma_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).lgamma_();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lgamma_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::lgamma_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lift(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::lift(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lift_fresh(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::lift_fresh(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lift_fresh_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::lift_fresh_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lift_fresh_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::lift_fresh_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lift_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::lift_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_cholesky(gc_tensor self, int upper) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_cholesky(tensor_from_ocaml(self), (bool)upper);
    return tensor_to_ocaml(results__);
  )
}

void atg_linalg_cholesky_ex(raw_tensor *out__, gc_tensor self, int upper, int check_errors) {
  PROTECT(
    auto results__ = torch::linalg_cholesky_ex(tensor_from_ocaml(self), (bool)upper, (bool)check_errors);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_linalg_cholesky_ex_l(raw_tensor *out__, gc_tensor L, gc_tensor info, gc_tensor self, int upper, int check_errors) {
  torch::Tensor L_local = tensor_from_ocaml(L);
  torch::Tensor info_local = tensor_from_ocaml(info);
  PROTECT(
    auto results__ = torch::linalg_cholesky_ex_out(L_local, info_local, tensor_from_ocaml(self), (bool)upper, (bool)check_errors);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_linalg_cholesky_out(gc_tensor out, gc_tensor self, int upper) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_cholesky_out(out_local, tensor_from_ocaml(self), (bool)upper);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_cond(gc_tensor self, scalar p) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_cond(tensor_from_ocaml(self), *p);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_cond_out(gc_tensor out, gc_tensor self, scalar p) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_cond_out(out_local, tensor_from_ocaml(self), *p);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_cond_p_str(gc_tensor self, char * p) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_cond(tensor_from_ocaml(self), std::string(p));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_cond_p_str_out(gc_tensor out, gc_tensor self, char * p) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_cond_out(out_local, tensor_from_ocaml(self), std::string(p));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_cross(gc_tensor self, gc_tensor other, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_cross(tensor_from_ocaml(self), tensor_from_ocaml(other), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_cross_out(gc_tensor out, gc_tensor self, gc_tensor other, int64_t dim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_cross_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_det(gc_tensor A) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_det(tensor_from_ocaml(A));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_det_out(gc_tensor out, gc_tensor A) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_det_out(out_local, tensor_from_ocaml(A));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_diagonal(gc_tensor A, int64_t offset, int64_t dim1, int64_t dim2) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_diagonal(tensor_from_ocaml(A), offset, dim1, dim2);
    return tensor_to_ocaml(results__);
  )
}

void atg_linalg_eig(raw_tensor *out__, gc_tensor self) {
  PROTECT(
    auto results__ = torch::linalg_eig(tensor_from_ocaml(self));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_linalg_eig_out(raw_tensor *out__, gc_tensor eigenvalues, gc_tensor eigenvectors, gc_tensor self) {
  torch::Tensor eigenvalues_local = tensor_from_ocaml(eigenvalues);
  torch::Tensor eigenvectors_local = tensor_from_ocaml(eigenvectors);
  PROTECT(
    auto results__ = torch::linalg_eig_out(eigenvalues_local, eigenvectors_local, tensor_from_ocaml(self));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_linalg_eigh(raw_tensor *out__, gc_tensor self, char * UPLO) {
  PROTECT(
    auto results__ = torch::linalg_eigh(tensor_from_ocaml(self), std::string(UPLO));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_linalg_eigh_eigvals(raw_tensor *out__, gc_tensor eigvals, gc_tensor eigvecs, gc_tensor self, char * UPLO) {
  torch::Tensor eigvals_local = tensor_from_ocaml(eigvals);
  torch::Tensor eigvecs_local = tensor_from_ocaml(eigvecs);
  PROTECT(
    auto results__ = torch::linalg_eigh_out(eigvals_local, eigvecs_local, tensor_from_ocaml(self), std::string(UPLO));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_linalg_eigvals(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_eigvals(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_eigvals_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_eigvals_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_eigvalsh(gc_tensor self, char * UPLO) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_eigvalsh(tensor_from_ocaml(self), std::string(UPLO));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_eigvalsh_out(gc_tensor out, gc_tensor self, char * UPLO) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_eigvalsh_out(out_local, tensor_from_ocaml(self), std::string(UPLO));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_householder_product(gc_tensor input, gc_tensor tau) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_householder_product(tensor_from_ocaml(input), tensor_from_ocaml(tau));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_householder_product_out(gc_tensor out, gc_tensor input, gc_tensor tau) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_householder_product_out(out_local, tensor_from_ocaml(input), tensor_from_ocaml(tau));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_inv(gc_tensor A) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_inv(tensor_from_ocaml(A));
    return tensor_to_ocaml(results__);
  )
}

void atg_linalg_inv_ex(raw_tensor *out__, gc_tensor A, int check_errors) {
  PROTECT(
    auto results__ = torch::linalg_inv_ex(tensor_from_ocaml(A), (bool)check_errors);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_linalg_inv_ex_inverse(raw_tensor *out__, gc_tensor inverse, gc_tensor info, gc_tensor A, int check_errors) {
  torch::Tensor inverse_local = tensor_from_ocaml(inverse);
  torch::Tensor info_local = tensor_from_ocaml(info);
  PROTECT(
    auto results__ = torch::linalg_inv_ex_out(inverse_local, info_local, tensor_from_ocaml(A), (bool)check_errors);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_linalg_inv_out(gc_tensor out, gc_tensor A) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_inv_out(out_local, tensor_from_ocaml(A));
    return tensor_to_ocaml(results__);
  )
}

void atg_linalg_ldl_factor(raw_tensor *out__, gc_tensor self, int hermitian) {
  PROTECT(
    auto results__ = torch::linalg_ldl_factor(tensor_from_ocaml(self), (bool)hermitian);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_linalg_ldl_factor_ex(raw_tensor *out__, gc_tensor self, int hermitian, int check_errors) {
  PROTECT(
    auto results__ = torch::linalg_ldl_factor_ex(tensor_from_ocaml(self), (bool)hermitian, (bool)check_errors);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_linalg_ldl_factor_ex_out(raw_tensor *out__, gc_tensor LD, gc_tensor pivots, gc_tensor info, gc_tensor self, int hermitian, int check_errors) {
  torch::Tensor LD_local = tensor_from_ocaml(LD);
  torch::Tensor pivots_local = tensor_from_ocaml(pivots);
  torch::Tensor info_local = tensor_from_ocaml(info);
  PROTECT(
    auto results__ = torch::linalg_ldl_factor_ex_out(LD_local, pivots_local, info_local, tensor_from_ocaml(self), (bool)hermitian, (bool)check_errors);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_linalg_ldl_factor_out(raw_tensor *out__, gc_tensor LD, gc_tensor pivots, gc_tensor self, int hermitian) {
  torch::Tensor LD_local = tensor_from_ocaml(LD);
  torch::Tensor pivots_local = tensor_from_ocaml(pivots);
  PROTECT(
    auto results__ = torch::linalg_ldl_factor_out(LD_local, pivots_local, tensor_from_ocaml(self), (bool)hermitian);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_linalg_ldl_solve(gc_tensor LD, gc_tensor pivots, gc_tensor B, int hermitian) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_ldl_solve(tensor_from_ocaml(LD), tensor_from_ocaml(pivots), tensor_from_ocaml(B), (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_ldl_solve_out(gc_tensor out, gc_tensor LD, gc_tensor pivots, gc_tensor B, int hermitian) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_ldl_solve_out(out_local, tensor_from_ocaml(LD), tensor_from_ocaml(pivots), tensor_from_ocaml(B), (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

void atg_linalg_lstsq(raw_tensor *out__, gc_tensor self, gc_tensor b, double rcond_v, int rcond_null, char * driver_v, int driver_null) {
  PROTECT(
    auto results__ = torch::linalg_lstsq(tensor_from_ocaml(self), tensor_from_ocaml(b), rcond_null ? c10::nullopt : c10::optional<double>(rcond_v), driver_null ? c10::nullopt : c10::optional<c10::string_view>(driver_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

void atg_linalg_lstsq_out(raw_tensor *out__, gc_tensor solution, gc_tensor residuals, gc_tensor rank, gc_tensor singular_values, gc_tensor self, gc_tensor b, double rcond_v, int rcond_null, char * driver_v, int driver_null) {
  torch::Tensor solution_local = tensor_from_ocaml(solution);
  torch::Tensor residuals_local = tensor_from_ocaml(residuals);
  torch::Tensor rank_local = tensor_from_ocaml(rank);
  torch::Tensor singular_values_local = tensor_from_ocaml(singular_values);
  PROTECT(
    auto results__ = torch::linalg_lstsq_out(solution_local, residuals_local, rank_local, singular_values_local, tensor_from_ocaml(self), tensor_from_ocaml(b), rcond_null ? c10::nullopt : c10::optional<double>(rcond_v), driver_null ? c10::nullopt : c10::optional<c10::string_view>(driver_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

void atg_linalg_lu(raw_tensor *out__, gc_tensor A, int pivot) {
  PROTECT(
    auto results__ = torch::linalg_lu(tensor_from_ocaml(A), (bool)pivot);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_linalg_lu_factor(raw_tensor *out__, gc_tensor A, int pivot) {
  PROTECT(
    auto results__ = torch::linalg_lu_factor(tensor_from_ocaml(A), (bool)pivot);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_linalg_lu_factor_ex(raw_tensor *out__, gc_tensor A, int pivot, int check_errors) {
  PROTECT(
    auto results__ = torch::linalg_lu_factor_ex(tensor_from_ocaml(A), (bool)pivot, (bool)check_errors);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_linalg_lu_factor_ex_out(raw_tensor *out__, gc_tensor LU, gc_tensor pivots, gc_tensor info, gc_tensor A, int pivot, int check_errors) {
  torch::Tensor LU_local = tensor_from_ocaml(LU);
  torch::Tensor pivots_local = tensor_from_ocaml(pivots);
  torch::Tensor info_local = tensor_from_ocaml(info);
  PROTECT(
    auto results__ = torch::linalg_lu_factor_ex_out(LU_local, pivots_local, info_local, tensor_from_ocaml(A), (bool)pivot, (bool)check_errors);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_linalg_lu_factor_out(raw_tensor *out__, gc_tensor LU, gc_tensor pivots, gc_tensor A, int pivot) {
  torch::Tensor LU_local = tensor_from_ocaml(LU);
  torch::Tensor pivots_local = tensor_from_ocaml(pivots);
  PROTECT(
    auto results__ = torch::linalg_lu_factor_out(LU_local, pivots_local, tensor_from_ocaml(A), (bool)pivot);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_linalg_lu_out(raw_tensor *out__, gc_tensor P, gc_tensor L, gc_tensor U, gc_tensor A, int pivot) {
  torch::Tensor P_local = tensor_from_ocaml(P);
  torch::Tensor L_local = tensor_from_ocaml(L);
  torch::Tensor U_local = tensor_from_ocaml(U);
  PROTECT(
    auto results__ = torch::linalg_lu_out(P_local, L_local, U_local, tensor_from_ocaml(A), (bool)pivot);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

raw_tensor atg_linalg_lu_solve(gc_tensor LU, gc_tensor pivots, gc_tensor B, int left, int adjoint) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_lu_solve(tensor_from_ocaml(LU), tensor_from_ocaml(pivots), tensor_from_ocaml(B), (bool)left, (bool)adjoint);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_lu_solve_out(gc_tensor out, gc_tensor LU, gc_tensor pivots, gc_tensor B, int left, int adjoint) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_lu_solve_out(out_local, tensor_from_ocaml(LU), tensor_from_ocaml(pivots), tensor_from_ocaml(B), (bool)left, (bool)adjoint);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_matmul(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_matmul(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_matmul_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_matmul_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_matrix_exp(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_matrix_exp(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_matrix_exp_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_matrix_exp_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_matrix_power(gc_tensor self, int64_t n) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_matrix_power(tensor_from_ocaml(self), n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_matrix_power_out(gc_tensor out, gc_tensor self, int64_t n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_matrix_power_out(out_local, tensor_from_ocaml(self), n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_matrix_rank(gc_tensor self, double tol, int hermitian) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_matrix_rank(tensor_from_ocaml(self), tol, (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_matrix_rank_atol_rtol_float(gc_tensor self, double atol_v, int atol_null, double rtol_v, int rtol_null, int hermitian) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_matrix_rank(tensor_from_ocaml(self), atol_null ? c10::nullopt : c10::optional<double>(atol_v), rtol_null ? c10::nullopt : c10::optional<double>(rtol_v), (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_matrix_rank_atol_rtol_float_out(gc_tensor out, gc_tensor self, double atol_v, int atol_null, double rtol_v, int rtol_null, int hermitian) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_matrix_rank_out(out_local, tensor_from_ocaml(self), atol_null ? c10::nullopt : c10::optional<double>(atol_v), rtol_null ? c10::nullopt : c10::optional<double>(rtol_v), (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_matrix_rank_atol_rtol_tensor(gc_tensor input, gc_tensor atol, gc_tensor rtol, int hermitian) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_matrix_rank(tensor_from_ocaml(input), atol ? std::make_optional(tensor_from_ocaml(atol)) : std::nullopt, rtol ? std::make_optional(tensor_from_ocaml(rtol)) : std::nullopt, (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_matrix_rank_atol_rtol_tensor_out(gc_tensor out, gc_tensor input, gc_tensor atol, gc_tensor rtol, int hermitian) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_matrix_rank_out(out_local, tensor_from_ocaml(input), atol ? std::make_optional(tensor_from_ocaml(atol)) : std::nullopt, rtol ? std::make_optional(tensor_from_ocaml(rtol)) : std::nullopt, (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_matrix_rank_out(gc_tensor out, gc_tensor self, double tol, int hermitian) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_matrix_rank_out(out_local, tensor_from_ocaml(self), tol, (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_matrix_rank_out_tol_tensor(gc_tensor out, gc_tensor input, gc_tensor tol, int hermitian) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_matrix_rank_out(out_local, tensor_from_ocaml(input), tensor_from_ocaml(tol), (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_matrix_rank_tol_tensor(gc_tensor input, gc_tensor tol, int hermitian) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_matrix_rank(tensor_from_ocaml(input), tensor_from_ocaml(tol), (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_multi_dot(gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_multi_dot(of_carray_tensor(tensors_data, tensors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_multi_dot_out(gc_tensor out, gc_tensor *tensors_data, int tensors_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_multi_dot_out(out_local, of_carray_tensor(tensors_data, tensors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_pinv(gc_tensor self, double rcond, int hermitian) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_pinv(tensor_from_ocaml(self), rcond, (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_pinv_atol_rtol_float(gc_tensor self, double atol_v, int atol_null, double rtol_v, int rtol_null, int hermitian) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_pinv(tensor_from_ocaml(self), atol_null ? c10::nullopt : c10::optional<double>(atol_v), rtol_null ? c10::nullopt : c10::optional<double>(rtol_v), (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_pinv_atol_rtol_float_out(gc_tensor out, gc_tensor self, double atol_v, int atol_null, double rtol_v, int rtol_null, int hermitian) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_pinv_out(out_local, tensor_from_ocaml(self), atol_null ? c10::nullopt : c10::optional<double>(atol_v), rtol_null ? c10::nullopt : c10::optional<double>(rtol_v), (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_pinv_atol_rtol_tensor(gc_tensor self, gc_tensor atol, gc_tensor rtol, int hermitian) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_pinv(tensor_from_ocaml(self), atol ? std::make_optional(tensor_from_ocaml(atol)) : std::nullopt, rtol ? std::make_optional(tensor_from_ocaml(rtol)) : std::nullopt, (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_pinv_atol_rtol_tensor_out(gc_tensor out, gc_tensor self, gc_tensor atol, gc_tensor rtol, int hermitian) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_pinv_out(out_local, tensor_from_ocaml(self), atol ? std::make_optional(tensor_from_ocaml(atol)) : std::nullopt, rtol ? std::make_optional(tensor_from_ocaml(rtol)) : std::nullopt, (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_pinv_out(gc_tensor out, gc_tensor self, double rcond, int hermitian) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_pinv_out(out_local, tensor_from_ocaml(self), rcond, (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_pinv_out_rcond_tensor(gc_tensor out, gc_tensor self, gc_tensor rcond, int hermitian) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_pinv_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(rcond), (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_pinv_rcond_tensor(gc_tensor self, gc_tensor rcond, int hermitian) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_pinv(tensor_from_ocaml(self), tensor_from_ocaml(rcond), (bool)hermitian);
    return tensor_to_ocaml(results__);
  )
}

void atg_linalg_qr(raw_tensor *out__, gc_tensor A, char * mode) {
  PROTECT(
    auto results__ = torch::linalg_qr(tensor_from_ocaml(A), std::string(mode));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_linalg_qr_out(raw_tensor *out__, gc_tensor Q, gc_tensor R, gc_tensor A, char * mode) {
  torch::Tensor Q_local = tensor_from_ocaml(Q);
  torch::Tensor R_local = tensor_from_ocaml(R);
  PROTECT(
    auto results__ = torch::linalg_qr_out(Q_local, R_local, tensor_from_ocaml(A), std::string(mode));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_linalg_slogdet(raw_tensor *out__, gc_tensor A) {
  PROTECT(
    auto results__ = torch::linalg_slogdet(tensor_from_ocaml(A));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_linalg_slogdet_out(raw_tensor *out__, gc_tensor sign, gc_tensor logabsdet, gc_tensor A) {
  torch::Tensor sign_local = tensor_from_ocaml(sign);
  torch::Tensor logabsdet_local = tensor_from_ocaml(logabsdet);
  PROTECT(
    auto results__ = torch::linalg_slogdet_out(sign_local, logabsdet_local, tensor_from_ocaml(A));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_linalg_solve(gc_tensor A, gc_tensor B, int left) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_solve(tensor_from_ocaml(A), tensor_from_ocaml(B), (bool)left);
    return tensor_to_ocaml(results__);
  )
}

void atg_linalg_solve_ex(raw_tensor *out__, gc_tensor A, gc_tensor B, int left, int check_errors) {
  PROTECT(
    auto results__ = torch::linalg_solve_ex(tensor_from_ocaml(A), tensor_from_ocaml(B), (bool)left, (bool)check_errors);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_linalg_solve_ex_out(raw_tensor *out__, gc_tensor result, gc_tensor info, gc_tensor A, gc_tensor B, int left, int check_errors) {
  torch::Tensor result_local = tensor_from_ocaml(result);
  torch::Tensor info_local = tensor_from_ocaml(info);
  PROTECT(
    auto results__ = torch::linalg_solve_ex_out(result_local, info_local, tensor_from_ocaml(A), tensor_from_ocaml(B), (bool)left, (bool)check_errors);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_linalg_solve_out(gc_tensor out, gc_tensor A, gc_tensor B, int left) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_solve_out(out_local, tensor_from_ocaml(A), tensor_from_ocaml(B), (bool)left);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_solve_triangular(gc_tensor self, gc_tensor B, int upper, int left, int unitriangular) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_solve_triangular(tensor_from_ocaml(self), tensor_from_ocaml(B), (bool)upper, (bool)left, (bool)unitriangular);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_solve_triangular_out(gc_tensor out, gc_tensor self, gc_tensor B, int upper, int left, int unitriangular) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_solve_triangular_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(B), (bool)upper, (bool)left, (bool)unitriangular);
    return tensor_to_ocaml(results__);
  )
}

void atg_linalg_svd(raw_tensor *out__, gc_tensor A, int full_matrices, char * driver_v, int driver_null) {
  PROTECT(
    auto results__ = torch::linalg_svd(tensor_from_ocaml(A), (bool)full_matrices, driver_null ? c10::nullopt : c10::optional<c10::string_view>(driver_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_linalg_svd_u(raw_tensor *out__, gc_tensor U, gc_tensor S, gc_tensor Vh, gc_tensor A, int full_matrices, char * driver_v, int driver_null) {
  torch::Tensor U_local = tensor_from_ocaml(U);
  torch::Tensor S_local = tensor_from_ocaml(S);
  torch::Tensor Vh_local = tensor_from_ocaml(Vh);
  PROTECT(
    auto results__ = torch::linalg_svd_out(U_local, S_local, Vh_local, tensor_from_ocaml(A), (bool)full_matrices, driver_null ? c10::nullopt : c10::optional<c10::string_view>(driver_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

raw_tensor atg_linalg_svdvals(gc_tensor A, char * driver_v, int driver_null) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_svdvals(tensor_from_ocaml(A), driver_null ? c10::nullopt : c10::optional<c10::string_view>(driver_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_svdvals_out(gc_tensor out, gc_tensor A, char * driver_v, int driver_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_svdvals_out(out_local, tensor_from_ocaml(A), driver_null ? c10::nullopt : c10::optional<c10::string_view>(driver_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_tensorinv(gc_tensor self, int64_t ind) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_tensorinv(tensor_from_ocaml(self), ind);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_tensorinv_out(gc_tensor out, gc_tensor self, int64_t ind) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_tensorinv_out(out_local, tensor_from_ocaml(self), ind);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_tensorsolve(gc_tensor self, gc_tensor other, int64_t *dims_data, int dims_len) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_tensorsolve(tensor_from_ocaml(self), tensor_from_ocaml(other), dims_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dims_data, dims_len)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_tensorsolve_out(gc_tensor out, gc_tensor self, gc_tensor other, int64_t *dims_data, int dims_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_tensorsolve_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other), dims_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dims_data, dims_len)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_vander(gc_tensor x, int64_t n_v, int n_null) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_vander(tensor_from_ocaml(x), n_null ? c10::nullopt : c10::optional<int64_t>(n_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_vecdot(gc_tensor x, gc_tensor y, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::linalg_vecdot(tensor_from_ocaml(x), tensor_from_ocaml(y), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linalg_vecdot_out(gc_tensor out, gc_tensor x, gc_tensor y, int64_t dim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linalg_vecdot_out(out_local, tensor_from_ocaml(x), tensor_from_ocaml(y), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linear(gc_tensor input, gc_tensor weight, gc_tensor bias) {
  PROTECT(
    torch::Tensor results__ = torch::linear(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linear_out(gc_tensor out, gc_tensor input, gc_tensor weight, gc_tensor bias) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linear_out(out_local, tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linspace(scalar start, scalar end, int64_t steps, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::linspace(*start, *end, steps, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linspace_out(gc_tensor out, scalar start, scalar end, int64_t steps) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linspace_out(out_local, *start, *end, steps);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linspace_scalar_tensor(scalar start, gc_tensor end, int64_t steps, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::linspace(*start, tensor_from_ocaml(end), steps, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linspace_scalar_tensor_out(gc_tensor out, scalar start, gc_tensor end, int64_t steps) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linspace_out(out_local, *start, tensor_from_ocaml(end), steps);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linspace_tensor_scalar(gc_tensor start, scalar end, int64_t steps, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::linspace(tensor_from_ocaml(start), *end, steps, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linspace_tensor_scalar_out(gc_tensor out, gc_tensor start, scalar end, int64_t steps) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linspace_out(out_local, tensor_from_ocaml(start), *end, steps);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linspace_tensor_tensor(gc_tensor start, gc_tensor end, int64_t steps, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::linspace(tensor_from_ocaml(start), tensor_from_ocaml(end), steps, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_linspace_tensor_tensor_out(gc_tensor out, gc_tensor start, gc_tensor end, int64_t steps) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::linspace_out(out_local, tensor_from_ocaml(start), tensor_from_ocaml(end), steps);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::log(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log10(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::log10(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log10_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::log10_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log10_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::log10_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log1p(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::log1p(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log1p_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::log1p_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log1p_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::log1p_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log2(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::log2(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log2_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::log2_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log2_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::log2_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::log_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log_normal(gc_tensor self, double mean, double std) {
  PROTECT(
    torch::Tensor results__ = torch::log_normal(tensor_from_ocaml(self), mean, std);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log_normal_(gc_tensor self, double mean, double std) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).log_normal_(mean, std);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log_normal_out(gc_tensor out, gc_tensor self, double mean, double std) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::log_normal_out(out_local, tensor_from_ocaml(self), mean, std);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::log_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log_sigmoid(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::log_sigmoid(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log_sigmoid_backward(gc_tensor grad_output, gc_tensor self, gc_tensor buffer) {
  PROTECT(
    torch::Tensor results__ = torch::log_sigmoid_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(buffer));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log_sigmoid_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, gc_tensor buffer) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::log_sigmoid_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(buffer));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log_sigmoid_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::log_sigmoid_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log_softmax(gc_tensor self, int64_t dim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::log_softmax(tensor_from_ocaml(self), dim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_log_softmax_int_out(gc_tensor out, gc_tensor self, int64_t dim, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::log_softmax_out(out_local, tensor_from_ocaml(self), dim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logaddexp(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::logaddexp(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logaddexp2(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::logaddexp2(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logaddexp2_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::logaddexp2_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logaddexp_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::logaddexp_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logcumsumexp(gc_tensor self, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::logcumsumexp(tensor_from_ocaml(self), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logcumsumexp_out(gc_tensor out, gc_tensor self, int64_t dim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::logcumsumexp_out(out_local, tensor_from_ocaml(self), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logdet(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::logdet(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logical_and(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::logical_and(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logical_and_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).logical_and_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logical_and_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::logical_and_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logical_not(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::logical_not(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logical_not_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).logical_not_();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logical_not_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::logical_not_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logical_or(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::logical_or(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logical_or_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).logical_or_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logical_or_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::logical_or_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logical_xor(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::logical_xor(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logical_xor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).logical_xor_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logical_xor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::logical_xor_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logit(gc_tensor self, double eps_v, int eps_null) {
  PROTECT(
    torch::Tensor results__ = torch::logit(tensor_from_ocaml(self), eps_null ? c10::nullopt : c10::optional<double>(eps_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logit_(gc_tensor self, double eps_v, int eps_null) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::logit_(self_local, eps_null ? c10::nullopt : c10::optional<double>(eps_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logit_backward(gc_tensor grad_output, gc_tensor self, double eps_v, int eps_null) {
  PROTECT(
    torch::Tensor results__ = torch::logit_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), eps_null ? c10::nullopt : c10::optional<double>(eps_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logit_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, double eps_v, int eps_null) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::logit_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), eps_null ? c10::nullopt : c10::optional<double>(eps_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logit_out(gc_tensor out, gc_tensor self, double eps_v, int eps_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::logit_out(out_local, tensor_from_ocaml(self), eps_null ? c10::nullopt : c10::optional<double>(eps_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logspace(scalar start, scalar end, int64_t steps, double base, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::logspace(*start, *end, steps, base, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logspace_out(gc_tensor out, scalar start, scalar end, int64_t steps, double base) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::logspace_out(out_local, *start, *end, steps, base);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logspace_scalar_tensor(scalar start, gc_tensor end, int64_t steps, double base, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::logspace(*start, tensor_from_ocaml(end), steps, base, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logspace_scalar_tensor_out(gc_tensor out, scalar start, gc_tensor end, int64_t steps, double base) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::logspace_out(out_local, *start, tensor_from_ocaml(end), steps, base);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logspace_tensor_scalar(gc_tensor start, scalar end, int64_t steps, double base, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::logspace(tensor_from_ocaml(start), *end, steps, base, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logspace_tensor_scalar_out(gc_tensor out, gc_tensor start, scalar end, int64_t steps, double base) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::logspace_out(out_local, tensor_from_ocaml(start), *end, steps, base);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logspace_tensor_tensor(gc_tensor start, gc_tensor end, int64_t steps, double base, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::logspace(tensor_from_ocaml(start), tensor_from_ocaml(end), steps, base, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logspace_tensor_tensor_out(gc_tensor out, gc_tensor start, gc_tensor end, int64_t steps, double base) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::logspace_out(out_local, tensor_from_ocaml(start), tensor_from_ocaml(end), steps, base);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logsumexp(gc_tensor self, int64_t *dim_data, int dim_len, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::logsumexp(tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_logsumexp_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::logsumexp_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

void atg_lstm(raw_tensor *out__, gc_tensor input, gc_tensor *hx_data, int hx_len, gc_tensor *params_data, int params_len, int has_biases, int64_t num_layers, double dropout, int train, int bidirectional, int batch_first) {
  PROTECT(
    auto results__ = torch::lstm(tensor_from_ocaml(input), of_carray_tensor(hx_data, hx_len), of_carray_tensor(params_data, params_len), (bool)has_biases, num_layers, dropout, (bool)train, (bool)bidirectional, (bool)batch_first);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_lstm_cell(raw_tensor *out__, gc_tensor input, gc_tensor *hx_data, int hx_len, gc_tensor w_ih, gc_tensor w_hh, gc_tensor b_ih, gc_tensor b_hh) {
  PROTECT(
    auto results__ = torch::lstm_cell(tensor_from_ocaml(input), of_carray_tensor(hx_data, hx_len), tensor_from_ocaml(w_ih), tensor_from_ocaml(w_hh), b_ih ? std::make_optional(tensor_from_ocaml(b_ih)) : std::nullopt, b_hh ? std::make_optional(tensor_from_ocaml(b_hh)) : std::nullopt);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_lstm_data(raw_tensor *out__, gc_tensor data, gc_tensor batch_sizes, gc_tensor *hx_data, int hx_len, gc_tensor *params_data, int params_len, int has_biases, int64_t num_layers, double dropout, int train, int bidirectional) {
  PROTECT(
    auto results__ = torch::lstm(tensor_from_ocaml(data), tensor_from_ocaml(batch_sizes), of_carray_tensor(hx_data, hx_len), of_carray_tensor(params_data, params_len), (bool)has_biases, num_layers, dropout, (bool)train, (bool)bidirectional);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_lstm_mps_backward(gc_tensor out0, gc_tensor *out1_data, int out1_len, gc_tensor *out2_data, int out2_len, gc_tensor grad_y, gc_tensor grad_hy, gc_tensor grad_cy, gc_tensor z_state, gc_tensor cell_state_fwd, gc_tensor input, gc_tensor layersOutputs, gc_tensor *hx_data, int hx_len, gc_tensor *params_data, int params_len, int has_biases, int64_t num_layers, double dropout, int train, int bidirectional, int batch_first) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  PROTECT(
    torch::lstm_mps_backward_out(out0_local, of_carray_tensor(out1_data, out1_len), of_carray_tensor(out2_data, out2_len), grad_y ? std::make_optional(tensor_from_ocaml(grad_y)) : std::nullopt, grad_hy ? std::make_optional(tensor_from_ocaml(grad_hy)) : std::nullopt, grad_cy ? std::make_optional(tensor_from_ocaml(grad_cy)) : std::nullopt, tensor_from_ocaml(z_state), tensor_from_ocaml(cell_state_fwd), tensor_from_ocaml(input), tensor_from_ocaml(layersOutputs), of_carray_tensor(hx_data, hx_len), of_carray_tensor(params_data, params_len), (bool)has_biases, num_layers, dropout, (bool)train, (bool)bidirectional, (bool)batch_first);
  )
}

raw_tensor atg_lt(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::lt(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lt_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).lt_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lt_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::lt_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lt_tensor(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::lt(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lt_tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).lt_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lt_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::lt_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lu_solve(gc_tensor self, gc_tensor LU_data, gc_tensor LU_pivots) {
  PROTECT(
    torch::Tensor results__ = torch::lu_solve(tensor_from_ocaml(self), tensor_from_ocaml(LU_data), tensor_from_ocaml(LU_pivots));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_lu_solve_out(gc_tensor out, gc_tensor self, gc_tensor LU_data, gc_tensor LU_pivots) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::lu_solve_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(LU_data), tensor_from_ocaml(LU_pivots));
    return tensor_to_ocaml(results__);
  )
}

void atg_lu_unpack(raw_tensor *out__, gc_tensor LU_data, gc_tensor LU_pivots, int unpack_data, int unpack_pivots) {
  PROTECT(
    auto results__ = torch::lu_unpack(tensor_from_ocaml(LU_data), tensor_from_ocaml(LU_pivots), (bool)unpack_data, (bool)unpack_pivots);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_lu_unpack_out(raw_tensor *out__, gc_tensor P, gc_tensor L, gc_tensor U, gc_tensor LU_data, gc_tensor LU_pivots, int unpack_data, int unpack_pivots) {
  torch::Tensor P_local = tensor_from_ocaml(P);
  torch::Tensor L_local = tensor_from_ocaml(L);
  torch::Tensor U_local = tensor_from_ocaml(U);
  PROTECT(
    auto results__ = torch::lu_unpack_out(P_local, L_local, U_local, tensor_from_ocaml(LU_data), tensor_from_ocaml(LU_pivots), (bool)unpack_data, (bool)unpack_pivots);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

raw_tensor atg_margin_ranking_loss(gc_tensor input1, gc_tensor input2, gc_tensor target, double margin, int64_t reduction) {
  PROTECT(
    torch::Tensor results__ = torch::margin_ranking_loss(tensor_from_ocaml(input1), tensor_from_ocaml(input2), tensor_from_ocaml(target), margin, reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_masked_fill(gc_tensor self, gc_tensor mask, scalar value) {
  PROTECT(
    torch::Tensor results__ = torch::masked_fill(tensor_from_ocaml(self), tensor_from_ocaml(mask), *value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_masked_fill_(gc_tensor self, gc_tensor mask, scalar value) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).masked_fill_(tensor_from_ocaml(mask), *value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_masked_fill_scalar_out(gc_tensor out, gc_tensor self, gc_tensor mask, scalar value) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::masked_fill_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mask), *value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_masked_fill_tensor(gc_tensor self, gc_tensor mask, gc_tensor value) {
  PROTECT(
    torch::Tensor results__ = torch::masked_fill(tensor_from_ocaml(self), tensor_from_ocaml(mask), tensor_from_ocaml(value));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_masked_fill_tensor_(gc_tensor self, gc_tensor mask, gc_tensor value) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).masked_fill_(tensor_from_ocaml(mask), tensor_from_ocaml(value));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_masked_fill_tensor_out(gc_tensor out, gc_tensor self, gc_tensor mask, gc_tensor value) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::masked_fill_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mask), tensor_from_ocaml(value));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_masked_scatter(gc_tensor self, gc_tensor mask, gc_tensor source) {
  PROTECT(
    torch::Tensor results__ = torch::masked_scatter(tensor_from_ocaml(self), tensor_from_ocaml(mask), tensor_from_ocaml(source));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_masked_scatter_(gc_tensor self, gc_tensor mask, gc_tensor source) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).masked_scatter_(tensor_from_ocaml(mask), tensor_from_ocaml(source));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_masked_scatter_backward(gc_tensor grad_output, gc_tensor mask, int64_t *sizes_data, int sizes_len) {
  PROTECT(
    torch::Tensor results__ = torch::masked_scatter_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(mask), torch::IntArrayRef(sizes_data, sizes_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_masked_scatter_out(gc_tensor out, gc_tensor self, gc_tensor mask, gc_tensor source) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::masked_scatter_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mask), tensor_from_ocaml(source));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_masked_select(gc_tensor self, gc_tensor mask) {
  PROTECT(
    torch::Tensor results__ = torch::masked_select(tensor_from_ocaml(self), tensor_from_ocaml(mask));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_masked_select_backward(gc_tensor grad, gc_tensor input, gc_tensor mask) {
  PROTECT(
    torch::Tensor results__ = torch::masked_select_backward(tensor_from_ocaml(grad), tensor_from_ocaml(input), tensor_from_ocaml(mask));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_masked_select_out(gc_tensor out, gc_tensor self, gc_tensor mask) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::masked_select_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mask));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_matmul(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::matmul(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_matmul_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::matmul_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_matrix_exp(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::matrix_exp(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_matrix_exp_backward(gc_tensor self, gc_tensor grad) {
  PROTECT(
    torch::Tensor results__ = torch::matrix_exp_backward(tensor_from_ocaml(self), tensor_from_ocaml(grad));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_matrix_h(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).matrix_H();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_matrix_power(gc_tensor self, int64_t n) {
  PROTECT(
    torch::Tensor results__ = torch::matrix_power(tensor_from_ocaml(self), n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_matrix_power_out(gc_tensor out, gc_tensor self, int64_t n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::matrix_power_out(out_local, tensor_from_ocaml(self), n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_max(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::max(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

void atg_max_dim(raw_tensor *out__, gc_tensor self, int64_t dim, int keepdim) {
  PROTECT(
    auto results__ = torch::max(tensor_from_ocaml(self), dim, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_max_dim_max(raw_tensor *out__, gc_tensor max, gc_tensor max_values, gc_tensor self, int64_t dim, int keepdim) {
  torch::Tensor max_local = tensor_from_ocaml(max);
  torch::Tensor max_values_local = tensor_from_ocaml(max_values);
  PROTECT(
    auto results__ = torch::max_out(max_local, max_values_local, tensor_from_ocaml(self), dim, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_max_other(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::max(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_max_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::max_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_max_pool1d(gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  PROTECT(
    torch::Tensor results__ = torch::max_pool1d(tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

void atg_max_pool1d_with_indices(raw_tensor *out__, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  PROTECT(
    auto results__ = torch::max_pool1d_with_indices(tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_max_pool2d(gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  PROTECT(
    torch::Tensor results__ = torch::max_pool2d(tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_max_pool2d_backward(gc_tensor grad_output, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  PROTECT(
    torch::Tensor results__ = torch::max_pool2d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_max_pool2d_backward_out(gc_tensor out, gc_tensor grad_output, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::max_pool2d_backward_out(out_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

void atg_max_pool2d_with_indices(raw_tensor *out__, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  PROTECT(
    auto results__ = torch::max_pool2d_with_indices(tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_max_pool2d_with_indices_backward(gc_tensor grad_output, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode, gc_tensor indices) {
  PROTECT(
    torch::Tensor results__ = torch::max_pool2d_with_indices_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode, tensor_from_ocaml(indices));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_max_pool2d_with_indices_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode, gc_tensor indices) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::max_pool2d_with_indices_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode, tensor_from_ocaml(indices));
    return tensor_to_ocaml(results__);
  )
}

void atg_max_pool2d_with_indices_out(raw_tensor *out__, gc_tensor out, gc_tensor indices, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  torch::Tensor indices_local = tensor_from_ocaml(indices);
  PROTECT(
    auto results__ = torch::max_pool2d_with_indices_out(out_local, indices_local, tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_max_pool3d(gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  PROTECT(
    torch::Tensor results__ = torch::max_pool3d(tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

void atg_max_pool3d_with_indices(raw_tensor *out__, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  PROTECT(
    auto results__ = torch::max_pool3d_with_indices(tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_max_pool3d_with_indices_backward(gc_tensor grad_output, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode, gc_tensor indices) {
  PROTECT(
    torch::Tensor results__ = torch::max_pool3d_with_indices_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode, tensor_from_ocaml(indices));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_max_pool3d_with_indices_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode, gc_tensor indices) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::max_pool3d_with_indices_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode, tensor_from_ocaml(indices));
    return tensor_to_ocaml(results__);
  )
}

void atg_max_pool3d_with_indices_out(raw_tensor *out__, gc_tensor out, gc_tensor indices, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  torch::Tensor indices_local = tensor_from_ocaml(indices);
  PROTECT(
    auto results__ = torch::max_pool3d_with_indices_out(out_local, indices_local, tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_max_unary_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::max_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_max_unpool2d(gc_tensor self, gc_tensor indices, int64_t *output_size_data, int output_size_len) {
  PROTECT(
    torch::Tensor results__ = torch::max_unpool2d(tensor_from_ocaml(self), tensor_from_ocaml(indices), torch::IntArrayRef(output_size_data, output_size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_max_unpool2d_out(gc_tensor out, gc_tensor self, gc_tensor indices, int64_t *output_size_data, int output_size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::max_unpool2d_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(indices), torch::IntArrayRef(output_size_data, output_size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_max_unpool3d(gc_tensor self, gc_tensor indices, int64_t *output_size_data, int output_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len) {
  PROTECT(
    torch::Tensor results__ = torch::max_unpool3d(tensor_from_ocaml(self), tensor_from_ocaml(indices), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_max_unpool3d_out(gc_tensor out, gc_tensor self, gc_tensor indices, int64_t *output_size_data, int output_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::max_unpool3d_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(indices), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_maximum(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::maximum(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_maximum_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::maximum_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mean(gc_tensor self, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::mean(tensor_from_ocaml(self), torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mean_dim(gc_tensor self, int64_t *dim_data, int dim_len, int keepdim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::mean(tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mean_dtype_out(gc_tensor out, gc_tensor self, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mean_out(out_local, tensor_from_ocaml(self), torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mean_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int keepdim, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mean_out(out_local, tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_median(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::median(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

void atg_median_dim(raw_tensor *out__, gc_tensor self, int64_t dim, int keepdim) {
  PROTECT(
    auto results__ = torch::median(tensor_from_ocaml(self), dim, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_median_dim_values(raw_tensor *out__, gc_tensor values, gc_tensor indices, gc_tensor self, int64_t dim, int keepdim) {
  torch::Tensor values_local = tensor_from_ocaml(values);
  torch::Tensor indices_local = tensor_from_ocaml(indices);
  PROTECT(
    auto results__ = torch::median_out(values_local, indices_local, tensor_from_ocaml(self), dim, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_median_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::median_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg_meshgrid(gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    auto results__ = torch::meshgrid(of_carray_tensor(tensors_data, tensors_len));
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor *atg_meshgrid_indexing(gc_tensor *tensors_data, int tensors_len, char * indexing) {
  PROTECT(
    auto results__ = torch::meshgrid(of_carray_tensor(tensors_data, tensors_len), std::string(indexing));
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor atg_mh(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).mH();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_min(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::min(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

void atg_min_dim(raw_tensor *out__, gc_tensor self, int64_t dim, int keepdim) {
  PROTECT(
    auto results__ = torch::min(tensor_from_ocaml(self), dim, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_min_dim_min(raw_tensor *out__, gc_tensor min, gc_tensor min_indices, gc_tensor self, int64_t dim, int keepdim) {
  torch::Tensor min_local = tensor_from_ocaml(min);
  torch::Tensor min_indices_local = tensor_from_ocaml(min_indices);
  PROTECT(
    auto results__ = torch::min_out(min_local, min_indices_local, tensor_from_ocaml(self), dim, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_min_other(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::min(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_min_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::min_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_min_unary_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::min_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_minimum(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::minimum(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_minimum_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::minimum_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

void atg_miopen_batch_norm(raw_tensor *out__, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, int training, double exponential_average_factor, double epsilon) {
  PROTECT(
    auto results__ = torch::miopen_batch_norm(tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, (bool)training, exponential_average_factor, epsilon);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_miopen_batch_norm_backward(raw_tensor *out__, gc_tensor input, gc_tensor grad_output, gc_tensor weight, gc_tensor running_mean, gc_tensor running_var, gc_tensor save_mean, gc_tensor save_var, double epsilon) {
  PROTECT(
    auto results__ = torch::miopen_batch_norm_backward(tensor_from_ocaml(input), tensor_from_ocaml(grad_output), tensor_from_ocaml(weight), running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, save_mean ? std::make_optional(tensor_from_ocaml(save_mean)) : std::nullopt, save_var ? std::make_optional(tensor_from_ocaml(save_var)) : std::nullopt, epsilon);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_miopen_batch_norm_backward_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor input, gc_tensor grad_output, gc_tensor weight, gc_tensor running_mean, gc_tensor running_var, gc_tensor save_mean, gc_tensor save_var, double epsilon) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  PROTECT(
    auto results__ = torch::miopen_batch_norm_backward_out(out0_local, out1_local, out2_local, tensor_from_ocaml(input), tensor_from_ocaml(grad_output), tensor_from_ocaml(weight), running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, save_mean ? std::make_optional(tensor_from_ocaml(save_mean)) : std::nullopt, save_var ? std::make_optional(tensor_from_ocaml(save_var)) : std::nullopt, epsilon);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_miopen_batch_norm_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, int training, double exponential_average_factor, double epsilon) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  PROTECT(
    auto results__ = torch::miopen_batch_norm_out(out0_local, out1_local, out2_local, tensor_from_ocaml(input), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, (bool)training, exponential_average_factor, epsilon);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

raw_tensor atg_miopen_convolution(gc_tensor self, gc_tensor weight, gc_tensor bias, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups, int benchmark, int deterministic) {
  PROTECT(
    torch::Tensor results__ = torch::miopen_convolution(tensor_from_ocaml(self), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups, (bool)benchmark, (bool)deterministic);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_miopen_convolution_add_relu(gc_tensor self, gc_tensor weight, gc_tensor z, scalar alpha, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::miopen_convolution_add_relu(tensor_from_ocaml(self), tensor_from_ocaml(weight), tensor_from_ocaml(z), *alpha, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_miopen_convolution_out(gc_tensor out, gc_tensor self, gc_tensor weight, gc_tensor bias, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups, int benchmark, int deterministic) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::miopen_convolution_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups, (bool)benchmark, (bool)deterministic);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_miopen_convolution_relu(gc_tensor self, gc_tensor weight, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::miopen_convolution_relu(tensor_from_ocaml(self), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_miopen_convolution_transpose(gc_tensor self, gc_tensor weight, gc_tensor bias, int64_t *padding_data, int padding_len, int64_t *output_padding_data, int output_padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups, int benchmark, int deterministic) {
  PROTECT(
    torch::Tensor results__ = torch::miopen_convolution_transpose(tensor_from_ocaml(self), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(output_padding_data, output_padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups, (bool)benchmark, (bool)deterministic);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_miopen_convolution_transpose_out(gc_tensor out, gc_tensor self, gc_tensor weight, gc_tensor bias, int64_t *padding_data, int padding_len, int64_t *output_padding_data, int output_padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups, int benchmark, int deterministic) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::miopen_convolution_transpose_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(output_padding_data, output_padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups, (bool)benchmark, (bool)deterministic);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_miopen_depthwise_convolution(gc_tensor self, gc_tensor weight, gc_tensor bias, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups, int benchmark, int deterministic) {
  PROTECT(
    torch::Tensor results__ = torch::miopen_depthwise_convolution(tensor_from_ocaml(self), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups, (bool)benchmark, (bool)deterministic);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_miopen_depthwise_convolution_out(gc_tensor out, gc_tensor self, gc_tensor weight, gc_tensor bias, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups, int benchmark, int deterministic) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::miopen_depthwise_convolution_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups, (bool)benchmark, (bool)deterministic);
    return tensor_to_ocaml(results__);
  )
}

void atg_miopen_rnn(raw_tensor *out__, gc_tensor input, gc_tensor *weight_data, int weight_len, int64_t weight_stride0, gc_tensor hx, gc_tensor cx, int64_t mode, int64_t hidden_size, int64_t num_layers, int batch_first, double dropout, int train, int bidirectional, int64_t *batch_sizes_data, int batch_sizes_len, gc_tensor dropout_state) {
  PROTECT(
    auto results__ = torch::miopen_rnn(tensor_from_ocaml(input), of_carray_tensor(weight_data, weight_len), weight_stride0, tensor_from_ocaml(hx), cx ? std::make_optional(tensor_from_ocaml(cx)) : std::nullopt, mode, hidden_size, num_layers, (bool)batch_first, dropout, (bool)train, (bool)bidirectional, torch::IntArrayRef(batch_sizes_data, batch_sizes_len), dropout_state ? std::make_optional(tensor_from_ocaml(dropout_state)) : std::nullopt);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
    out__[4] = tensor_to_ocaml(std::get<4>(results__));
  )
}

void atg_miopen_rnn_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor out3, gc_tensor out4, gc_tensor input, gc_tensor *weight_data, int weight_len, int64_t weight_stride0, gc_tensor hx, gc_tensor cx, int64_t mode, int64_t hidden_size, int64_t num_layers, int batch_first, double dropout, int train, int bidirectional, int64_t *batch_sizes_data, int batch_sizes_len, gc_tensor dropout_state) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  torch::Tensor out3_local = tensor_from_ocaml(out3);
  torch::Tensor out4_local = tensor_from_ocaml(out4);
  PROTECT(
    auto results__ = torch::miopen_rnn_out(out0_local, out1_local, out2_local, out3_local, out4_local, tensor_from_ocaml(input), of_carray_tensor(weight_data, weight_len), weight_stride0, tensor_from_ocaml(hx), cx ? std::make_optional(tensor_from_ocaml(cx)) : std::nullopt, mode, hidden_size, num_layers, (bool)batch_first, dropout, (bool)train, (bool)bidirectional, torch::IntArrayRef(batch_sizes_data, batch_sizes_len), dropout_state ? std::make_optional(tensor_from_ocaml(dropout_state)) : std::nullopt);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
    out__[4] = tensor_to_ocaml(std::get<4>(results__));
  )
}

raw_tensor atg_mish(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::mish(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mish_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::mish_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mish_backward(gc_tensor grad_output, gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::mish_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mish_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mish_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_adaptive_avg_pool2d(gc_tensor self, int64_t *output_size_data, int output_size_len) {
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_adaptive_avg_pool2d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_adaptive_avg_pool2d_backward(gc_tensor grad_output, gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_adaptive_avg_pool2d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_adaptive_avg_pool2d_backward_out(gc_tensor out, gc_tensor grad_output, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_adaptive_avg_pool2d_backward_out(out_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_adaptive_avg_pool2d_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_adaptive_avg_pool2d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_convolution(gc_tensor self, gc_tensor weight, gc_tensor bias, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_convolution(tensor_from_ocaml(self), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_convolution_out(gc_tensor out, gc_tensor self, gc_tensor weight, gc_tensor bias, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_convolution_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_linear(gc_tensor self, gc_tensor weight, gc_tensor bias) {
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_linear(tensor_from_ocaml(self), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_linear_backward_input(int64_t *input_size_data, int input_size_len, gc_tensor grad_output, gc_tensor weight) {
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_linear_backward_input(torch::IntArrayRef(input_size_data, input_size_len), tensor_from_ocaml(grad_output), tensor_from_ocaml(weight));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_linear_backward_input_out(gc_tensor out, int64_t *input_size_data, int input_size_len, gc_tensor grad_output, gc_tensor weight) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_linear_backward_input_out(out_local, torch::IntArrayRef(input_size_data, input_size_len), tensor_from_ocaml(grad_output), tensor_from_ocaml(weight));
    return tensor_to_ocaml(results__);
  )
}

void atg_mkldnn_linear_backward_weights(raw_tensor *out__, gc_tensor grad_output, gc_tensor input, gc_tensor weight, int bias_defined) {
  PROTECT(
    auto results__ = torch::mkldnn_linear_backward_weights(tensor_from_ocaml(grad_output), tensor_from_ocaml(input), tensor_from_ocaml(weight), (bool)bias_defined);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_mkldnn_linear_backward_weights_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor grad_output, gc_tensor input, gc_tensor weight, int bias_defined) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::mkldnn_linear_backward_weights_out(out0_local, out1_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(input), tensor_from_ocaml(weight), (bool)bias_defined);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_mkldnn_linear_out(gc_tensor out, gc_tensor self, gc_tensor weight, gc_tensor bias) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_linear_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_max_pool2d(gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_max_pool2d(tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_max_pool2d_backward(gc_tensor grad_output, gc_tensor output, gc_tensor input, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_max_pool2d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(output), tensor_from_ocaml(input), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_max_pool2d_backward_out(gc_tensor out, gc_tensor grad_output, gc_tensor output, gc_tensor input, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_max_pool2d_backward_out(out_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(output), tensor_from_ocaml(input), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_max_pool2d_out(gc_tensor out, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_max_pool2d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_max_pool3d(gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_max_pool3d(tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_max_pool3d_backward(gc_tensor grad_output, gc_tensor output, gc_tensor input, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_max_pool3d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(output), tensor_from_ocaml(input), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_max_pool3d_backward_out(gc_tensor out, gc_tensor grad_output, gc_tensor output, gc_tensor input, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_max_pool3d_backward_out(out_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(output), tensor_from_ocaml(input), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_max_pool3d_out(gc_tensor out, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_max_pool3d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_reorder_conv2d_weight(gc_tensor self, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups, int64_t *input_size_data, int input_size_len) {
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_reorder_conv2d_weight(tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups, input_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(input_size_data, input_size_len)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_reorder_conv2d_weight_out(gc_tensor out, gc_tensor self, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups, int64_t *input_size_data, int input_size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_reorder_conv2d_weight_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups, input_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(input_size_data, input_size_len)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_reorder_conv3d_weight(gc_tensor self, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups, int64_t *input_size_data, int input_size_len) {
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_reorder_conv3d_weight(tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups, input_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(input_size_data, input_size_len)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mkldnn_reorder_conv3d_weight_out(gc_tensor out, gc_tensor self, int64_t *padding_data, int padding_len, int64_t *stride_data, int stride_len, int64_t *dilation_data, int dilation_len, int64_t groups, int64_t *input_size_data, int input_size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mkldnn_reorder_conv3d_weight_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(dilation_data, dilation_len), groups, input_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(input_size_data, input_size_len)));
    return tensor_to_ocaml(results__);
  )
}

void atg_mkldnn_rnn_layer(raw_tensor *out__, gc_tensor input, gc_tensor weight0, gc_tensor weight1, gc_tensor weight2, gc_tensor weight3, gc_tensor hx_, gc_tensor cx_, int reverse, int64_t *batch_sizes_data, int batch_sizes_len, int64_t mode, int64_t hidden_size, int64_t num_layers, int has_biases, int bidirectional, int batch_first, int train) {
  PROTECT(
    auto results__ = torch::mkldnn_rnn_layer(tensor_from_ocaml(input), tensor_from_ocaml(weight0), tensor_from_ocaml(weight1), tensor_from_ocaml(weight2), tensor_from_ocaml(weight3), tensor_from_ocaml(hx_), tensor_from_ocaml(cx_), (bool)reverse, torch::IntArrayRef(batch_sizes_data, batch_sizes_len), mode, hidden_size, num_layers, (bool)has_biases, (bool)bidirectional, (bool)batch_first, (bool)train);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

void atg_mkldnn_rnn_layer_backward(raw_tensor *out__, gc_tensor input, gc_tensor weight1, gc_tensor weight2, gc_tensor weight3, gc_tensor weight4, gc_tensor hx_, gc_tensor cx_tmp, gc_tensor output, gc_tensor hy_, gc_tensor cy_, gc_tensor grad_output, gc_tensor grad_hy, gc_tensor grad_cy, int reverse, int64_t mode, int64_t hidden_size, int64_t num_layers, int has_biases, int train, int bidirectional, int64_t *batch_sizes_data, int batch_sizes_len, int batch_first, gc_tensor workspace) {
  PROTECT(
    auto results__ = torch::mkldnn_rnn_layer_backward(tensor_from_ocaml(input), tensor_from_ocaml(weight1), tensor_from_ocaml(weight2), tensor_from_ocaml(weight3), tensor_from_ocaml(weight4), tensor_from_ocaml(hx_), tensor_from_ocaml(cx_tmp), tensor_from_ocaml(output), tensor_from_ocaml(hy_), tensor_from_ocaml(cy_), grad_output ? std::make_optional(tensor_from_ocaml(grad_output)) : std::nullopt, grad_hy ? std::make_optional(tensor_from_ocaml(grad_hy)) : std::nullopt, grad_cy ? std::make_optional(tensor_from_ocaml(grad_cy)) : std::nullopt, (bool)reverse, mode, hidden_size, num_layers, (bool)has_biases, (bool)train, (bool)bidirectional, torch::IntArrayRef(batch_sizes_data, batch_sizes_len), (bool)batch_first, tensor_from_ocaml(workspace));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
    out__[4] = tensor_to_ocaml(std::get<4>(results__));
    out__[5] = tensor_to_ocaml(std::get<5>(results__));
    out__[6] = tensor_to_ocaml(std::get<6>(results__));
  )
}

void atg_mkldnn_rnn_layer_backward_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor out3, gc_tensor out4, gc_tensor out5, gc_tensor out6, gc_tensor input, gc_tensor weight1, gc_tensor weight2, gc_tensor weight3, gc_tensor weight4, gc_tensor hx_, gc_tensor cx_tmp, gc_tensor output, gc_tensor hy_, gc_tensor cy_, gc_tensor grad_output, gc_tensor grad_hy, gc_tensor grad_cy, int reverse, int64_t mode, int64_t hidden_size, int64_t num_layers, int has_biases, int train, int bidirectional, int64_t *batch_sizes_data, int batch_sizes_len, int batch_first, gc_tensor workspace) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  torch::Tensor out3_local = tensor_from_ocaml(out3);
  torch::Tensor out4_local = tensor_from_ocaml(out4);
  torch::Tensor out5_local = tensor_from_ocaml(out5);
  torch::Tensor out6_local = tensor_from_ocaml(out6);
  PROTECT(
    auto results__ = torch::mkldnn_rnn_layer_backward_out(out0_local, out1_local, out2_local, out3_local, out4_local, out5_local, out6_local, tensor_from_ocaml(input), tensor_from_ocaml(weight1), tensor_from_ocaml(weight2), tensor_from_ocaml(weight3), tensor_from_ocaml(weight4), tensor_from_ocaml(hx_), tensor_from_ocaml(cx_tmp), tensor_from_ocaml(output), tensor_from_ocaml(hy_), tensor_from_ocaml(cy_), grad_output ? std::make_optional(tensor_from_ocaml(grad_output)) : std::nullopt, grad_hy ? std::make_optional(tensor_from_ocaml(grad_hy)) : std::nullopt, grad_cy ? std::make_optional(tensor_from_ocaml(grad_cy)) : std::nullopt, (bool)reverse, mode, hidden_size, num_layers, (bool)has_biases, (bool)train, (bool)bidirectional, torch::IntArrayRef(batch_sizes_data, batch_sizes_len), (bool)batch_first, tensor_from_ocaml(workspace));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
    out__[4] = tensor_to_ocaml(std::get<4>(results__));
    out__[5] = tensor_to_ocaml(std::get<5>(results__));
    out__[6] = tensor_to_ocaml(std::get<6>(results__));
  )
}

void atg_mkldnn_rnn_layer_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor out3, gc_tensor input, gc_tensor weight0, gc_tensor weight1, gc_tensor weight2, gc_tensor weight3, gc_tensor hx_, gc_tensor cx_, int reverse, int64_t *batch_sizes_data, int batch_sizes_len, int64_t mode, int64_t hidden_size, int64_t num_layers, int has_biases, int bidirectional, int batch_first, int train) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  torch::Tensor out3_local = tensor_from_ocaml(out3);
  PROTECT(
    auto results__ = torch::mkldnn_rnn_layer_out(out0_local, out1_local, out2_local, out3_local, tensor_from_ocaml(input), tensor_from_ocaml(weight0), tensor_from_ocaml(weight1), tensor_from_ocaml(weight2), tensor_from_ocaml(weight3), tensor_from_ocaml(hx_), tensor_from_ocaml(cx_), (bool)reverse, torch::IntArrayRef(batch_sizes_data, batch_sizes_len), mode, hidden_size, num_layers, (bool)has_biases, (bool)bidirectional, (bool)batch_first, (bool)train);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
    out__[3] = tensor_to_ocaml(std::get<3>(results__));
  )
}

raw_tensor atg_mm(gc_tensor self, gc_tensor mat2) {
  PROTECT(
    torch::Tensor results__ = torch::mm(tensor_from_ocaml(self), tensor_from_ocaml(mat2));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mm_out(gc_tensor out, gc_tensor self, gc_tensor mat2) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mm_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mat2));
    return tensor_to_ocaml(results__);
  )
}

void atg_mode(raw_tensor *out__, gc_tensor self, int64_t dim, int keepdim) {
  PROTECT(
    auto results__ = torch::mode(tensor_from_ocaml(self), dim, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_mode_values(raw_tensor *out__, gc_tensor values, gc_tensor indices, gc_tensor self, int64_t dim, int keepdim) {
  torch::Tensor values_local = tensor_from_ocaml(values);
  torch::Tensor indices_local = tensor_from_ocaml(indices);
  PROTECT(
    auto results__ = torch::mode_out(values_local, indices_local, tensor_from_ocaml(self), dim, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_moveaxis(gc_tensor self, int64_t *source_data, int source_len, int64_t *destination_data, int destination_len) {
  PROTECT(
    torch::Tensor results__ = torch::moveaxis(tensor_from_ocaml(self), torch::IntArrayRef(source_data, source_len), torch::IntArrayRef(destination_data, destination_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_moveaxis_int(gc_tensor self, int64_t source, int64_t destination) {
  PROTECT(
    torch::Tensor results__ = torch::moveaxis(tensor_from_ocaml(self), source, destination);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_movedim(gc_tensor self, int64_t *source_data, int source_len, int64_t *destination_data, int destination_len) {
  PROTECT(
    torch::Tensor results__ = torch::movedim(tensor_from_ocaml(self), torch::IntArrayRef(source_data, source_len), torch::IntArrayRef(destination_data, destination_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_movedim_int(gc_tensor self, int64_t source, int64_t destination) {
  PROTECT(
    torch::Tensor results__ = torch::movedim(tensor_from_ocaml(self), source, destination);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mse_loss(gc_tensor self, gc_tensor target, int64_t reduction) {
  PROTECT(
    torch::Tensor results__ = torch::mse_loss(tensor_from_ocaml(self), tensor_from_ocaml(target), reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mse_loss_backward(gc_tensor grad_output, gc_tensor self, gc_tensor target, int64_t reduction) {
  PROTECT(
    torch::Tensor results__ = torch::mse_loss_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mse_loss_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, gc_tensor target, int64_t reduction) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::mse_loss_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mse_loss_out(gc_tensor out, gc_tensor self, gc_tensor target, int64_t reduction) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mse_loss_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(target), reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_msort(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::msort(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_msort_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::msort_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mt(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).mT();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mul(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::mul(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mul_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).mul_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mul_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mul_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mul_scalar(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::mul(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mul_scalar_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).mul_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mul_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mul_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_multi_margin_loss_backward(gc_tensor grad_output, gc_tensor self, gc_tensor target, scalar p, scalar margin, gc_tensor weight, int64_t reduction) {
  PROTECT(
    torch::Tensor results__ = torch::multi_margin_loss_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), *p, *margin, weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_multi_margin_loss_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, gc_tensor target, scalar p, scalar margin, gc_tensor weight, int64_t reduction) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::multi_margin_loss_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), *p, *margin, weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_multilabel_margin_loss(gc_tensor self, gc_tensor target, int64_t reduction) {
  PROTECT(
    torch::Tensor results__ = torch::multilabel_margin_loss(tensor_from_ocaml(self), tensor_from_ocaml(target), reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_multilabel_margin_loss_backward(gc_tensor grad_output, gc_tensor self, gc_tensor target, int64_t reduction, gc_tensor is_target) {
  PROTECT(
    torch::Tensor results__ = torch::multilabel_margin_loss_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), reduction, tensor_from_ocaml(is_target));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_multilabel_margin_loss_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, gc_tensor target, int64_t reduction, gc_tensor is_target) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::multilabel_margin_loss_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), reduction, tensor_from_ocaml(is_target));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_multilabel_margin_loss_out(gc_tensor out, gc_tensor self, gc_tensor target, int64_t reduction) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::multilabel_margin_loss_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(target), reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_multinomial(gc_tensor self, int64_t num_samples, int replacement) {
  PROTECT(
    torch::Tensor results__ = torch::multinomial(tensor_from_ocaml(self), num_samples, (bool)replacement);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_multinomial_out(gc_tensor out, gc_tensor self, int64_t num_samples, int replacement) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::multinomial_out(out_local, tensor_from_ocaml(self), num_samples, (bool)replacement);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_multiply(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::multiply(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_multiply_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).multiply_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_multiply_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::multiply_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_multiply_scalar(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::multiply(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_multiply_scalar_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).multiply_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mv(gc_tensor self, gc_tensor vec) {
  PROTECT(
    torch::Tensor results__ = torch::mv(tensor_from_ocaml(self), tensor_from_ocaml(vec));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mv_out(gc_tensor out, gc_tensor self, gc_tensor vec) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mv_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(vec));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mvlgamma(gc_tensor self, int64_t p) {
  PROTECT(
    torch::Tensor results__ = torch::mvlgamma(tensor_from_ocaml(self), p);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mvlgamma_(gc_tensor self, int64_t p) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).mvlgamma_(p);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_mvlgamma_out(gc_tensor out, gc_tensor self, int64_t p) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::mvlgamma_out(out_local, tensor_from_ocaml(self), p);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nan_to_num(gc_tensor self, double nan_v, int nan_null, double posinf_v, int posinf_null, double neginf_v, int neginf_null) {
  PROTECT(
    torch::Tensor results__ = torch::nan_to_num(tensor_from_ocaml(self), nan_null ? c10::nullopt : c10::optional<double>(nan_v), posinf_null ? c10::nullopt : c10::optional<double>(posinf_v), neginf_null ? c10::nullopt : c10::optional<double>(neginf_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nan_to_num_(gc_tensor self, double nan_v, int nan_null, double posinf_v, int posinf_null, double neginf_v, int neginf_null) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::nan_to_num_(self_local, nan_null ? c10::nullopt : c10::optional<double>(nan_v), posinf_null ? c10::nullopt : c10::optional<double>(posinf_v), neginf_null ? c10::nullopt : c10::optional<double>(neginf_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nan_to_num_out(gc_tensor out, gc_tensor self, double nan_v, int nan_null, double posinf_v, int posinf_null, double neginf_v, int neginf_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::nan_to_num_out(out_local, tensor_from_ocaml(self), nan_null ? c10::nullopt : c10::optional<double>(nan_v), posinf_null ? c10::nullopt : c10::optional<double>(posinf_v), neginf_null ? c10::nullopt : c10::optional<double>(neginf_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nanmean(gc_tensor self, int64_t *dim_data, int dim_len, int keepdim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::nanmean(tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nanmean_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int keepdim, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::nanmean_out(out_local, tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nanmedian(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::nanmedian(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

void atg_nanmedian_dim(raw_tensor *out__, gc_tensor self, int64_t dim, int keepdim) {
  PROTECT(
    auto results__ = torch::nanmedian(tensor_from_ocaml(self), dim, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_nanmedian_dim_values(raw_tensor *out__, gc_tensor values, gc_tensor indices, gc_tensor self, int64_t dim, int keepdim) {
  torch::Tensor values_local = tensor_from_ocaml(values);
  torch::Tensor indices_local = tensor_from_ocaml(indices);
  PROTECT(
    auto results__ = torch::nanmedian_out(values_local, indices_local, tensor_from_ocaml(self), dim, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_nanmedian_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::nanmedian_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nanquantile(gc_tensor self, gc_tensor q, int64_t dim_v, int dim_null, int keepdim, char * interpolation) {
  PROTECT(
    torch::Tensor results__ = torch::nanquantile(tensor_from_ocaml(self), tensor_from_ocaml(q), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), (bool)keepdim, std::string(interpolation));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nanquantile_out(gc_tensor out, gc_tensor self, gc_tensor q, int64_t dim_v, int dim_null, int keepdim, char * interpolation) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::nanquantile_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(q), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), (bool)keepdim, std::string(interpolation));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nanquantile_scalar(gc_tensor self, double q, int64_t dim_v, int dim_null, int keepdim, char * interpolation) {
  PROTECT(
    torch::Tensor results__ = torch::nanquantile(tensor_from_ocaml(self), q, dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), (bool)keepdim, std::string(interpolation));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nanquantile_scalar_out(gc_tensor out, gc_tensor self, double q, int64_t dim_v, int dim_null, int keepdim, char * interpolation) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::nanquantile_out(out_local, tensor_from_ocaml(self), q, dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), (bool)keepdim, std::string(interpolation));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nansum(gc_tensor self, int64_t *dim_data, int dim_len, int keepdim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::nansum(tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nansum_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int keepdim, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::nansum_out(out_local, tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_narrow(gc_tensor self, int64_t dim, int64_t start, int64_t length) {
  PROTECT(
    torch::Tensor results__ = torch::narrow(tensor_from_ocaml(self), dim, start, length);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_narrow_copy(gc_tensor self, int64_t dim, int64_t start, int64_t length) {
  PROTECT(
    torch::Tensor results__ = torch::narrow_copy(tensor_from_ocaml(self), dim, start, length);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_narrow_copy_out(gc_tensor out, gc_tensor self, int64_t dim, int64_t start, int64_t length) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::narrow_copy_out(out_local, tensor_from_ocaml(self), dim, start, length);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_narrow_tensor(gc_tensor self, int64_t dim, gc_tensor start, int64_t length) {
  PROTECT(
    torch::Tensor results__ = torch::narrow(tensor_from_ocaml(self), dim, tensor_from_ocaml(start), length);
    return tensor_to_ocaml(results__);
  )
}

void atg_native_batch_norm(raw_tensor *out__, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, int training, double momentum, double eps) {
  PROTECT(
    auto results__ = torch::native_batch_norm(tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, (bool)training, momentum, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_native_batch_norm_out(raw_tensor *out__, gc_tensor out, gc_tensor save_mean, gc_tensor save_invstd, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor running_mean, gc_tensor running_var, int training, double momentum, double eps) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  torch::Tensor save_mean_local = tensor_from_ocaml(save_mean);
  torch::Tensor save_invstd_local = tensor_from_ocaml(save_invstd);
  PROTECT(
    auto results__ = torch::native_batch_norm_out(out_local, save_mean_local, save_invstd_local, tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, running_mean ? std::make_optional(tensor_from_ocaml(running_mean)) : std::nullopt, running_var ? std::make_optional(tensor_from_ocaml(running_var)) : std::nullopt, (bool)training, momentum, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

raw_tensor atg_native_channel_shuffle(gc_tensor self, int64_t groups) {
  PROTECT(
    torch::Tensor results__ = torch::native_channel_shuffle(tensor_from_ocaml(self), groups);
    return tensor_to_ocaml(results__);
  )
}

void atg_native_dropout(raw_tensor *out__, gc_tensor input, double p, int train) {
  PROTECT(
    auto results__ = torch::native_dropout(tensor_from_ocaml(input), p, (bool)train);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_native_dropout_backward(gc_tensor grad_output, gc_tensor mask, double scale) {
  PROTECT(
    torch::Tensor results__ = torch::native_dropout_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(mask), scale);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_native_dropout_backward_out(gc_tensor out, gc_tensor grad_output, gc_tensor mask, double scale) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::native_dropout_backward_out(out_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(mask), scale);
    return tensor_to_ocaml(results__);
  )
}

void atg_native_dropout_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor input, double p, int train) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::native_dropout_out(out0_local, out1_local, tensor_from_ocaml(input), p, (bool)train);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_native_group_norm(raw_tensor *out__, gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t n, int64_t C, int64_t HxW, int64_t group, double eps) {
  PROTECT(
    auto results__ = torch::native_group_norm(tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, n, C, HxW, group, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_native_group_norm_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor input, gc_tensor weight, gc_tensor bias, int64_t n, int64_t C, int64_t HxW, int64_t group, double eps) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  PROTECT(
    auto results__ = torch::native_group_norm_out(out0_local, out1_local, out2_local, tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, n, C, HxW, group, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_native_layer_norm(raw_tensor *out__, gc_tensor input, int64_t *normalized_shape_data, int normalized_shape_len, gc_tensor weight, gc_tensor bias, double eps) {
  PROTECT(
    auto results__ = torch::native_layer_norm(tensor_from_ocaml(input), torch::IntArrayRef(normalized_shape_data, normalized_shape_len), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_native_layer_norm_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor input, int64_t *normalized_shape_data, int normalized_shape_len, gc_tensor weight, gc_tensor bias, double eps) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  PROTECT(
    auto results__ = torch::native_layer_norm_out(out0_local, out1_local, out2_local, tensor_from_ocaml(input), torch::IntArrayRef(normalized_shape_data, normalized_shape_len), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, eps);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

raw_tensor atg_native_norm(gc_tensor self, scalar p) {
  PROTECT(
    torch::Tensor results__ = torch::native_norm(tensor_from_ocaml(self), p ? *p : c10::Scalar{2} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_native_norm_out(gc_tensor out, gc_tensor self, scalar p) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::native_norm_out(out_local, tensor_from_ocaml(self), p ? *p : c10::Scalar{2} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_native_norm_scalaropt_dim_dtype(gc_tensor self, scalar p, int64_t *dim_data, int dim_len, int keepdim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::native_norm(tensor_from_ocaml(self), *p, torch::IntArrayRef(dim_data, dim_len), (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_native_norm_scalaropt_dim_dtype_out(gc_tensor out, gc_tensor self, scalar p, int64_t *dim_data, int dim_len, int keepdim, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::native_norm_out(out_local, tensor_from_ocaml(self), *p, torch::IntArrayRef(dim_data, dim_len), (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ne(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::ne(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ne_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).ne_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ne_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::ne_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ne_tensor(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::ne(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ne_tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).ne_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ne_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::ne_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_neg(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::neg(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_neg_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::neg_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_neg_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::neg_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_negative(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::negative(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_negative_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::negative_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_negative_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::negative_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nested_to_padded_tensor(gc_tensor self, double padding, int64_t *output_size_data, int output_size_len) {
  PROTECT(
    torch::Tensor results__ = torch::nested_to_padded_tensor(tensor_from_ocaml(self), padding, output_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(output_size_data, output_size_len)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_new_empty(gc_tensor self, int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).new_empty(torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_new_empty_out(gc_tensor out, gc_tensor self, int64_t *size_data, int size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::new_empty_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_new_empty_strided(gc_tensor self, int64_t *size_data, int size_len, int64_t *stride_data, int stride_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).new_empty_strided(torch::IntArrayRef(size_data, size_len), torch::IntArrayRef(stride_data, stride_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_new_empty_strided_out(gc_tensor out, gc_tensor self, int64_t *size_data, int size_len, int64_t *stride_data, int stride_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::new_empty_strided_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), torch::IntArrayRef(stride_data, stride_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_new_full(gc_tensor self, int64_t *size_data, int size_len, scalar fill_value, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).new_full(torch::IntArrayRef(size_data, size_len), *fill_value, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_new_full_out(gc_tensor out, gc_tensor self, int64_t *size_data, int size_len, scalar fill_value) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::new_full_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), *fill_value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_new_ones(gc_tensor self, int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).new_ones(torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_new_ones_out(gc_tensor out, gc_tensor self, int64_t *size_data, int size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::new_ones_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_new_zeros(gc_tensor self, int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).new_zeros(torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_new_zeros_out(gc_tensor out, gc_tensor self, int64_t *size_data, int size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::new_zeros_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nextafter(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::nextafter(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nextafter_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).nextafter_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nextafter_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::nextafter_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nll_loss(gc_tensor self, gc_tensor target, gc_tensor weight, int64_t reduction, int64_t ignore_index) {
  PROTECT(
    torch::Tensor results__ = torch::nll_loss(tensor_from_ocaml(self), tensor_from_ocaml(target), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, reduction, ignore_index);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nll_loss2d(gc_tensor self, gc_tensor target, gc_tensor weight, int64_t reduction, int64_t ignore_index) {
  PROTECT(
    torch::Tensor results__ = torch::nll_loss2d(tensor_from_ocaml(self), tensor_from_ocaml(target), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, reduction, ignore_index);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nll_loss2d_backward(gc_tensor grad_output, gc_tensor self, gc_tensor target, gc_tensor weight, int64_t reduction, int64_t ignore_index, gc_tensor total_weight) {
  PROTECT(
    torch::Tensor results__ = torch::nll_loss2d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, reduction, ignore_index, tensor_from_ocaml(total_weight));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nll_loss2d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, gc_tensor target, gc_tensor weight, int64_t reduction, int64_t ignore_index, gc_tensor total_weight) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::nll_loss2d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, reduction, ignore_index, tensor_from_ocaml(total_weight));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nll_loss2d_out(gc_tensor out, gc_tensor self, gc_tensor target, gc_tensor weight, int64_t reduction, int64_t ignore_index) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::nll_loss2d_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(target), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, reduction, ignore_index);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nll_loss_backward(gc_tensor grad_output, gc_tensor self, gc_tensor target, gc_tensor weight, int64_t reduction, int64_t ignore_index, gc_tensor total_weight) {
  PROTECT(
    torch::Tensor results__ = torch::nll_loss_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, reduction, ignore_index, tensor_from_ocaml(total_weight));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nll_loss_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, gc_tensor target, gc_tensor weight, int64_t reduction, int64_t ignore_index, gc_tensor total_weight) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::nll_loss_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, reduction, ignore_index, tensor_from_ocaml(total_weight));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nll_loss_nd(gc_tensor self, gc_tensor target, gc_tensor weight, int64_t reduction, int64_t ignore_index) {
  PROTECT(
    torch::Tensor results__ = torch::nll_loss_nd(tensor_from_ocaml(self), tensor_from_ocaml(target), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, reduction, ignore_index);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nll_loss_out(gc_tensor out, gc_tensor self, gc_tensor target, gc_tensor weight, int64_t reduction, int64_t ignore_index) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::nll_loss_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(target), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, reduction, ignore_index);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nonzero(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::nonzero(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg_nonzero_numpy(gc_tensor self) {
  PROTECT(
    auto results__ = torch::nonzero_numpy(tensor_from_ocaml(self));
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor atg_nonzero_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::nonzero_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nonzero_static(gc_tensor self, int64_t size, int64_t fill_value) {
  PROTECT(
    torch::Tensor results__ = torch::nonzero_static(tensor_from_ocaml(self), size, fill_value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nonzero_static_out(gc_tensor out, gc_tensor self, int64_t size, int64_t fill_value) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::nonzero_static_out(out_local, tensor_from_ocaml(self), size, fill_value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_norm(gc_tensor self, scalar p) {
  PROTECT(
    torch::Tensor results__ = torch::norm(tensor_from_ocaml(self), p ? *p : c10::Scalar{2} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_norm_dtype_out(gc_tensor out, gc_tensor self, scalar p, int64_t *dim_data, int dim_len, int keepdim, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::norm_out(out_local, tensor_from_ocaml(self), *p, torch::IntArrayRef(dim_data, dim_len), (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_norm_except_dim(gc_tensor v, int64_t pow, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::norm_except_dim(tensor_from_ocaml(v), pow, dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_norm_out(gc_tensor out, gc_tensor self, scalar p, int64_t *dim_data, int dim_len, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::norm_out(out_local, tensor_from_ocaml(self), *p, torch::IntArrayRef(dim_data, dim_len), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_norm_scalar_out(gc_tensor out, gc_tensor self, scalar p) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::norm_out(out_local, tensor_from_ocaml(self), p ? *p : c10::Scalar{2} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_norm_scalaropt_dim(gc_tensor self, scalar p, int64_t *dim_data, int dim_len, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::norm(tensor_from_ocaml(self), *p, torch::IntArrayRef(dim_data, dim_len), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_norm_scalaropt_dim_dtype(gc_tensor self, scalar p, int64_t *dim_data, int dim_len, int keepdim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::norm(tensor_from_ocaml(self), *p, torch::IntArrayRef(dim_data, dim_len), (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_norm_scalaropt_dtype(gc_tensor self, scalar p, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::norm(tensor_from_ocaml(self), *p, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_norm_scalaropt_dtype_out(gc_tensor out, gc_tensor self, scalar p, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::norm_out(out_local, tensor_from_ocaml(self), *p, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_normal_(gc_tensor self, double mean, double std) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).normal_(mean, std);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_normal_functional(gc_tensor self, double mean, double std) {
  PROTECT(
    torch::Tensor results__ = torch::normal_functional(tensor_from_ocaml(self), mean, std);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_not_equal(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::not_equal(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_not_equal_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).not_equal_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_not_equal_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::not_equal_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_not_equal_tensor(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::not_equal(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_not_equal_tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).not_equal_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_not_equal_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::not_equal_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nuclear_norm(gc_tensor self, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::nuclear_norm(tensor_from_ocaml(self), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nuclear_norm_dim(gc_tensor self, int64_t *dim_data, int dim_len, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::nuclear_norm(tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nuclear_norm_dim_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::nuclear_norm_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_nuclear_norm_out(gc_tensor out, gc_tensor self, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::nuclear_norm_out(out_local, tensor_from_ocaml(self), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_numpy_t(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).numpy_T();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_one_hot(gc_tensor self, int64_t num_classes) {
  PROTECT(
    torch::Tensor results__ = torch::one_hot(tensor_from_ocaml(self), num_classes);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ones(int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::ones(torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ones_like(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::ones_like(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ones_like_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::ones_like_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ones_out(gc_tensor out, int64_t *size_data, int size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::ones_out(out_local, torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_orgqr(gc_tensor self, gc_tensor input2) {
  PROTECT(
    torch::Tensor results__ = torch::orgqr(tensor_from_ocaml(self), tensor_from_ocaml(input2));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_orgqr_out(gc_tensor out, gc_tensor self, gc_tensor input2) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::orgqr_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(input2));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ormqr(gc_tensor self, gc_tensor input2, gc_tensor input3, int left, int transpose) {
  PROTECT(
    torch::Tensor results__ = torch::ormqr(tensor_from_ocaml(self), tensor_from_ocaml(input2), tensor_from_ocaml(input3), (bool)left, (bool)transpose);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ormqr_out(gc_tensor out, gc_tensor self, gc_tensor input2, gc_tensor input3, int left, int transpose) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::ormqr_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(input2), tensor_from_ocaml(input3), (bool)left, (bool)transpose);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_outer(gc_tensor self, gc_tensor vec2) {
  PROTECT(
    torch::Tensor results__ = torch::outer(tensor_from_ocaml(self), tensor_from_ocaml(vec2));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_outer_out(gc_tensor out, gc_tensor self, gc_tensor vec2) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::outer_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(vec2));
    return tensor_to_ocaml(results__);
  )
}

int64_t atg_output_nr(gc_tensor self) {
  PROTECT(
    return tensor_from_ocaml(self).output_nr();
  )
  return 0;
}

raw_tensor atg_pad(gc_tensor self, int64_t *pad_data, int pad_len, char * mode, double value_v, int value_null) {
  PROTECT(
    torch::Tensor results__ = torch::pad(tensor_from_ocaml(self), torch::IntArrayRef(pad_data, pad_len), std::string(mode), value_null ? c10::nullopt : c10::optional<double>(value_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_pad_sequence(gc_tensor *sequences_data, int sequences_len, int batch_first, double padding_value, char * padding_side) {
  PROTECT(
    torch::Tensor results__ = torch::pad_sequence(of_carray_tensor(sequences_data, sequences_len), (bool)batch_first, padding_value, std::string(padding_side));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_pairwise_distance(gc_tensor x1, gc_tensor x2, double p, double eps, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::pairwise_distance(tensor_from_ocaml(x1), tensor_from_ocaml(x2), p, eps, (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_pdist(gc_tensor self, double p) {
  PROTECT(
    torch::Tensor results__ = torch::pdist(tensor_from_ocaml(self), p);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_permute(gc_tensor self, int64_t *dims_data, int dims_len) {
  PROTECT(
    torch::Tensor results__ = torch::permute(tensor_from_ocaml(self), torch::IntArrayRef(dims_data, dims_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_permute_copy(gc_tensor self, int64_t *dims_data, int dims_len) {
  PROTECT(
    torch::Tensor results__ = torch::permute_copy(tensor_from_ocaml(self), torch::IntArrayRef(dims_data, dims_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_permute_copy_out(gc_tensor out, gc_tensor self, int64_t *dims_data, int dims_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::permute_copy_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(dims_data, dims_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_pin_memory(gc_tensor self, int device) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).pin_memory(optional_device_of_int(device));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_pinverse(gc_tensor self, double rcond) {
  PROTECT(
    torch::Tensor results__ = torch::pinverse(tensor_from_ocaml(self), rcond);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_pixel_shuffle(gc_tensor self, int64_t upscale_factor) {
  PROTECT(
    torch::Tensor results__ = torch::pixel_shuffle(tensor_from_ocaml(self), upscale_factor);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_pixel_shuffle_out(gc_tensor out, gc_tensor self, int64_t upscale_factor) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::pixel_shuffle_out(out_local, tensor_from_ocaml(self), upscale_factor);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_pixel_unshuffle(gc_tensor self, int64_t downscale_factor) {
  PROTECT(
    torch::Tensor results__ = torch::pixel_unshuffle(tensor_from_ocaml(self), downscale_factor);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_pixel_unshuffle_out(gc_tensor out, gc_tensor self, int64_t downscale_factor) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::pixel_unshuffle_out(out_local, tensor_from_ocaml(self), downscale_factor);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_poisson(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::poisson(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_poisson_nll_loss(gc_tensor input, gc_tensor target, int log_input, int full, double eps, int64_t reduction) {
  PROTECT(
    torch::Tensor results__ = torch::poisson_nll_loss(tensor_from_ocaml(input), tensor_from_ocaml(target), (bool)log_input, (bool)full, eps, reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_poisson_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::poisson_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_polar(gc_tensor abs, gc_tensor angle) {
  PROTECT(
    torch::Tensor results__ = torch::polar(tensor_from_ocaml(abs), tensor_from_ocaml(angle));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_polar_out(gc_tensor out, gc_tensor abs, gc_tensor angle) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::polar_out(out_local, tensor_from_ocaml(abs), tensor_from_ocaml(angle));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_polygamma(int64_t n, gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::polygamma(n, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_polygamma_(gc_tensor self, int64_t n) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).polygamma_(n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_polygamma_out(gc_tensor out, int64_t n, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::polygamma_out(out_local, n, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_positive(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::positive(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_pow(gc_tensor self, gc_tensor exponent) {
  PROTECT(
    torch::Tensor results__ = torch::pow(tensor_from_ocaml(self), tensor_from_ocaml(exponent));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_pow_(gc_tensor self, scalar exponent) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).pow_(*exponent);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_pow_scalar(scalar self, gc_tensor exponent) {
  PROTECT(
    torch::Tensor results__ = torch::pow(*self, tensor_from_ocaml(exponent));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_pow_scalar_out(gc_tensor out, scalar self, gc_tensor exponent) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::pow_out(out_local, *self, tensor_from_ocaml(exponent));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_pow_tensor_(gc_tensor self, gc_tensor exponent) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).pow_(tensor_from_ocaml(exponent));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_pow_tensor_scalar(gc_tensor self, scalar exponent) {
  PROTECT(
    torch::Tensor results__ = torch::pow(tensor_from_ocaml(self), *exponent);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_pow_tensor_scalar_out(gc_tensor out, gc_tensor self, scalar exponent) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::pow_out(out_local, tensor_from_ocaml(self), *exponent);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_pow_tensor_tensor_out(gc_tensor out, gc_tensor self, gc_tensor exponent) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::pow_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(exponent));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_prelu(gc_tensor self, gc_tensor weight) {
  PROTECT(
    torch::Tensor results__ = torch::prelu(tensor_from_ocaml(self), tensor_from_ocaml(weight));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_prod(gc_tensor self, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::prod(tensor_from_ocaml(self), torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_prod_dim_int(gc_tensor self, int64_t dim, int keepdim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::prod(tensor_from_ocaml(self), dim, (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_prod_int_out(gc_tensor out, gc_tensor self, int64_t dim, int keepdim, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::prod_out(out_local, tensor_from_ocaml(self), dim, (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_prod_out(gc_tensor out, gc_tensor self, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::prod_out(out_local, tensor_from_ocaml(self), torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_put(gc_tensor self, gc_tensor index, gc_tensor source, int accumulate) {
  PROTECT(
    torch::Tensor results__ = torch::put(tensor_from_ocaml(self), tensor_from_ocaml(index), tensor_from_ocaml(source), (bool)accumulate);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_put_(gc_tensor self, gc_tensor index, gc_tensor source, int accumulate) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).put_(tensor_from_ocaml(index), tensor_from_ocaml(source), (bool)accumulate);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_put_out(gc_tensor out, gc_tensor self, gc_tensor index, gc_tensor source, int accumulate) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::put_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(index), tensor_from_ocaml(source), (bool)accumulate);
    return tensor_to_ocaml(results__);
  )
}

int64_t atg_q_per_channel_axis(gc_tensor self) {
  PROTECT(
    return torch::q_per_channel_axis(tensor_from_ocaml(self));
  )
  return 0;
}

raw_tensor atg_q_per_channel_scales(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::q_per_channel_scales(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_q_per_channel_scales_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::q_per_channel_scales_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_q_per_channel_zero_points(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::q_per_channel_zero_points(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_q_per_channel_zero_points_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::q_per_channel_zero_points_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

double atg_q_scale(gc_tensor self) {
  PROTECT(
    return torch::q_scale(tensor_from_ocaml(self));
  )
  return 0;
}

int64_t atg_q_zero_point(gc_tensor self) {
  PROTECT(
    return torch::q_zero_point(tensor_from_ocaml(self));
  )
  return 0;
}

void atg_qr(raw_tensor *out__, gc_tensor self, int some) {
  PROTECT(
    auto results__ = torch::qr(tensor_from_ocaml(self), (bool)some);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_qr_q(raw_tensor *out__, gc_tensor Q, gc_tensor R, gc_tensor self, int some) {
  torch::Tensor Q_local = tensor_from_ocaml(Q);
  torch::Tensor R_local = tensor_from_ocaml(R);
  PROTECT(
    auto results__ = torch::qr_out(Q_local, R_local, tensor_from_ocaml(self), (bool)some);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_quantile(gc_tensor self, gc_tensor q, int64_t dim_v, int dim_null, int keepdim, char * interpolation) {
  PROTECT(
    torch::Tensor results__ = torch::quantile(tensor_from_ocaml(self), tensor_from_ocaml(q), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), (bool)keepdim, std::string(interpolation));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantile_out(gc_tensor out, gc_tensor self, gc_tensor q, int64_t dim_v, int dim_null, int keepdim, char * interpolation) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::quantile_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(q), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), (bool)keepdim, std::string(interpolation));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantile_scalar(gc_tensor self, double q, int64_t dim_v, int dim_null, int keepdim, char * interpolation) {
  PROTECT(
    torch::Tensor results__ = torch::quantile(tensor_from_ocaml(self), q, dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), (bool)keepdim, std::string(interpolation));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantile_scalar_out(gc_tensor out, gc_tensor self, double q, int64_t dim_v, int dim_null, int keepdim, char * interpolation) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::quantile_out(out_local, tensor_from_ocaml(self), q, dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), (bool)keepdim, std::string(interpolation));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantize_per_channel(gc_tensor self, gc_tensor scales, gc_tensor zero_points, int64_t axis, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::quantize_per_channel(tensor_from_ocaml(self), tensor_from_ocaml(scales), tensor_from_ocaml(zero_points), axis, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantize_per_channel_out(gc_tensor out, gc_tensor self, gc_tensor scales, gc_tensor zero_points, int64_t axis, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::quantize_per_channel_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(scales), tensor_from_ocaml(zero_points), axis, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantize_per_tensor(gc_tensor self, double scale, int64_t zero_point, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::quantize_per_tensor(tensor_from_ocaml(self), scale, zero_point, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantize_per_tensor_dynamic(gc_tensor self, int dtype, int reduce_range) {
  PROTECT(
    torch::Tensor results__ = torch::quantize_per_tensor_dynamic(tensor_from_ocaml(self), torch::ScalarType(dtype), (bool)reduce_range);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantize_per_tensor_dynamic_out(gc_tensor out, gc_tensor self, int dtype, int reduce_range) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::quantize_per_tensor_dynamic_out(out_local, tensor_from_ocaml(self), torch::ScalarType(dtype), (bool)reduce_range);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantize_per_tensor_out(gc_tensor out, gc_tensor self, double scale, int64_t zero_point, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::quantize_per_tensor_out(out_local, tensor_from_ocaml(self), scale, zero_point, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantize_per_tensor_tensor_qparams(gc_tensor self, gc_tensor scale, gc_tensor zero_point, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::quantize_per_tensor(tensor_from_ocaml(self), tensor_from_ocaml(scale), tensor_from_ocaml(zero_point), torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantize_per_tensor_tensor_qparams_out(gc_tensor out, gc_tensor self, gc_tensor scale, gc_tensor zero_point, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::quantize_per_tensor_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(scale), tensor_from_ocaml(zero_point), torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg_quantize_per_tensor_tensors(gc_tensor *tensors_data, int tensors_len, gc_tensor scales, gc_tensor zero_points, int dtype) {
  PROTECT(
    auto results__ = torch::quantize_per_tensor(of_carray_tensor(tensors_data, tensors_len), tensor_from_ocaml(scales), tensor_from_ocaml(zero_points), torch::ScalarType(dtype));
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

void atg_quantize_per_tensor_tensors_out(gc_tensor *out_data, int out_len, gc_tensor *tensors_data, int tensors_len, gc_tensor scales, gc_tensor zero_points, int dtype) {
  PROTECT(
    torch::quantize_per_tensor_out(of_carray_tensor(out_data, out_len), of_carray_tensor(tensors_data, tensors_len), tensor_from_ocaml(scales), tensor_from_ocaml(zero_points), torch::ScalarType(dtype));
  )
}

raw_tensor atg_quantized_batch_norm(gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor mean, gc_tensor var, double eps, double output_scale, int64_t output_zero_point) {
  PROTECT(
    torch::Tensor results__ = torch::quantized_batch_norm(tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, tensor_from_ocaml(mean), tensor_from_ocaml(var), eps, output_scale, output_zero_point);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantized_batch_norm_out(gc_tensor out, gc_tensor input, gc_tensor weight, gc_tensor bias, gc_tensor mean, gc_tensor var, double eps, double output_scale, int64_t output_zero_point) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::quantized_batch_norm_out(out_local, tensor_from_ocaml(input), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, tensor_from_ocaml(mean), tensor_from_ocaml(var), eps, output_scale, output_zero_point);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantized_gru_cell(gc_tensor input, gc_tensor hx, gc_tensor w_ih, gc_tensor w_hh, gc_tensor b_ih, gc_tensor b_hh, gc_tensor packed_ih, gc_tensor packed_hh, gc_tensor col_offsets_ih, gc_tensor col_offsets_hh, scalar scale_ih, scalar scale_hh, scalar zero_point_ih, scalar zero_point_hh) {
  PROTECT(
    torch::Tensor results__ = torch::quantized_gru_cell(tensor_from_ocaml(input), tensor_from_ocaml(hx), tensor_from_ocaml(w_ih), tensor_from_ocaml(w_hh), tensor_from_ocaml(b_ih), tensor_from_ocaml(b_hh), tensor_from_ocaml(packed_ih), tensor_from_ocaml(packed_hh), tensor_from_ocaml(col_offsets_ih), tensor_from_ocaml(col_offsets_hh), *scale_ih, *scale_hh, *zero_point_ih, *zero_point_hh);
    return tensor_to_ocaml(results__);
  )
}

void atg_quantized_lstm_cell(raw_tensor *out__, gc_tensor input, gc_tensor *hx_data, int hx_len, gc_tensor w_ih, gc_tensor w_hh, gc_tensor b_ih, gc_tensor b_hh, gc_tensor packed_ih, gc_tensor packed_hh, gc_tensor col_offsets_ih, gc_tensor col_offsets_hh, scalar scale_ih, scalar scale_hh, scalar zero_point_ih, scalar zero_point_hh) {
  PROTECT(
    auto results__ = torch::quantized_lstm_cell(tensor_from_ocaml(input), of_carray_tensor(hx_data, hx_len), tensor_from_ocaml(w_ih), tensor_from_ocaml(w_hh), tensor_from_ocaml(b_ih), tensor_from_ocaml(b_hh), tensor_from_ocaml(packed_ih), tensor_from_ocaml(packed_hh), tensor_from_ocaml(col_offsets_ih), tensor_from_ocaml(col_offsets_hh), *scale_ih, *scale_hh, *zero_point_ih, *zero_point_hh);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_quantized_max_pool1d(gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  PROTECT(
    torch::Tensor results__ = torch::quantized_max_pool1d(tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantized_max_pool1d_out(gc_tensor out, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::quantized_max_pool1d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantized_max_pool2d(gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  PROTECT(
    torch::Tensor results__ = torch::quantized_max_pool2d(tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantized_max_pool2d_out(gc_tensor out, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::quantized_max_pool2d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantized_max_pool3d(gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  PROTECT(
    torch::Tensor results__ = torch::quantized_max_pool3d(tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantized_max_pool3d_out(gc_tensor out, gc_tensor self, int64_t *kernel_size_data, int kernel_size_len, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len, int ceil_mode) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::quantized_max_pool3d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(kernel_size_data, kernel_size_len), torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len), (bool)ceil_mode);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantized_rnn_relu_cell(gc_tensor input, gc_tensor hx, gc_tensor w_ih, gc_tensor w_hh, gc_tensor b_ih, gc_tensor b_hh, gc_tensor packed_ih, gc_tensor packed_hh, gc_tensor col_offsets_ih, gc_tensor col_offsets_hh, scalar scale_ih, scalar scale_hh, scalar zero_point_ih, scalar zero_point_hh) {
  PROTECT(
    torch::Tensor results__ = torch::quantized_rnn_relu_cell(tensor_from_ocaml(input), tensor_from_ocaml(hx), tensor_from_ocaml(w_ih), tensor_from_ocaml(w_hh), tensor_from_ocaml(b_ih), tensor_from_ocaml(b_hh), tensor_from_ocaml(packed_ih), tensor_from_ocaml(packed_hh), tensor_from_ocaml(col_offsets_ih), tensor_from_ocaml(col_offsets_hh), *scale_ih, *scale_hh, *zero_point_ih, *zero_point_hh);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_quantized_rnn_tanh_cell(gc_tensor input, gc_tensor hx, gc_tensor w_ih, gc_tensor w_hh, gc_tensor b_ih, gc_tensor b_hh, gc_tensor packed_ih, gc_tensor packed_hh, gc_tensor col_offsets_ih, gc_tensor col_offsets_hh, scalar scale_ih, scalar scale_hh, scalar zero_point_ih, scalar zero_point_hh) {
  PROTECT(
    torch::Tensor results__ = torch::quantized_rnn_tanh_cell(tensor_from_ocaml(input), tensor_from_ocaml(hx), tensor_from_ocaml(w_ih), tensor_from_ocaml(w_hh), tensor_from_ocaml(b_ih), tensor_from_ocaml(b_hh), tensor_from_ocaml(packed_ih), tensor_from_ocaml(packed_hh), tensor_from_ocaml(col_offsets_ih), tensor_from_ocaml(col_offsets_hh), *scale_ih, *scale_hh, *zero_point_ih, *zero_point_hh);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rad2deg(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::rad2deg(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rad2deg_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::rad2deg_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rad2deg_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::rad2deg_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rand(int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::rand(torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rand_like(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::rand_like(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rand_like_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::rand_like_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rand_out(gc_tensor out, int64_t *size_data, int size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::rand_out(out_local, torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_randint(int64_t high, int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::randint(high, torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_randint_like(gc_tensor self, int64_t high) {
  PROTECT(
    torch::Tensor results__ = torch::randint_like(tensor_from_ocaml(self), high);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_randint_like_low_dtype(gc_tensor self, int64_t low, int64_t high) {
  PROTECT(
    torch::Tensor results__ = torch::randint_like(tensor_from_ocaml(self), low, high);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_randint_like_low_dtype_out(gc_tensor out, gc_tensor self, int64_t low, int64_t high) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::randint_like_out(out_local, tensor_from_ocaml(self), low, high);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_randint_like_out(gc_tensor out, gc_tensor self, int64_t high) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::randint_like_out(out_local, tensor_from_ocaml(self), high);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_randint_low(int64_t low, int64_t high, int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::randint(low, high, torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_randint_low_out(gc_tensor out, int64_t low, int64_t high, int64_t *size_data, int size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::randint_out(out_local, low, high, torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_randint_out(gc_tensor out, int64_t high, int64_t *size_data, int size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::randint_out(out_local, high, torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_randn(int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::randn(torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_randn_like(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::randn_like(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_randn_like_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::randn_like_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_randn_out(gc_tensor out, int64_t *size_data, int size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::randn_out(out_local, torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_random(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::random(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_random_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).random_();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_random_from(gc_tensor self, int64_t from, int64_t to_v, int to_null) {
  PROTECT(
    torch::Tensor results__ = torch::random(tensor_from_ocaml(self), from, to_null ? c10::nullopt : c10::optional<int64_t>(to_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_random_from_(gc_tensor self, int64_t from, int64_t to_v, int to_null) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).random_(from, to_null ? c10::nullopt : c10::optional<int64_t>(to_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_random_from_out(gc_tensor out, gc_tensor self, int64_t from, int64_t to_v, int to_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::random_out(out_local, tensor_from_ocaml(self), from, to_null ? c10::nullopt : c10::optional<int64_t>(to_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_random_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::random_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_random_to(gc_tensor self, int64_t to) {
  PROTECT(
    torch::Tensor results__ = torch::random(tensor_from_ocaml(self), to);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_random_to_(gc_tensor self, int64_t to) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).random_(to);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_random_to_out(gc_tensor out, gc_tensor self, int64_t to) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::random_out(out_local, tensor_from_ocaml(self), to);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_randperm(int64_t n, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::randperm(n, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_randperm_out(gc_tensor out, int64_t n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::randperm_out(out_local, n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_range(scalar start, scalar end, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::range(*start, *end, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_range_out(gc_tensor out, scalar start, scalar end, scalar step) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::range_out(out_local, *start, *end, step ? *step : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_range_out_(gc_tensor out, scalar start, scalar end) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::range_out(out_local, *start, *end);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_range_step(scalar start, scalar end, scalar step, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::range(*start, *end, step ? *step : c10::Scalar{1} , at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_ravel(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::ravel(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_real(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::real(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_reciprocal(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::reciprocal(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_reciprocal_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::reciprocal_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_reciprocal_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::reciprocal_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_reflection_pad1d(gc_tensor self, int64_t *padding_data, int padding_len) {
  PROTECT(
    torch::Tensor results__ = torch::reflection_pad1d(tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_reflection_pad1d_backward(gc_tensor grad_output, gc_tensor self, int64_t *padding_data, int padding_len) {
  PROTECT(
    torch::Tensor results__ = torch::reflection_pad1d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_reflection_pad1d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, int64_t *padding_data, int padding_len) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::reflection_pad1d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_reflection_pad1d_out(gc_tensor out, gc_tensor self, int64_t *padding_data, int padding_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::reflection_pad1d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_reflection_pad2d(gc_tensor self, int64_t *padding_data, int padding_len) {
  PROTECT(
    torch::Tensor results__ = torch::reflection_pad2d(tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_reflection_pad2d_backward(gc_tensor grad_output, gc_tensor self, int64_t *padding_data, int padding_len) {
  PROTECT(
    torch::Tensor results__ = torch::reflection_pad2d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_reflection_pad2d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, int64_t *padding_data, int padding_len) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::reflection_pad2d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_reflection_pad2d_out(gc_tensor out, gc_tensor self, int64_t *padding_data, int padding_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::reflection_pad2d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_reflection_pad3d(gc_tensor self, int64_t *padding_data, int padding_len) {
  PROTECT(
    torch::Tensor results__ = torch::reflection_pad3d(tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_reflection_pad3d_backward(gc_tensor grad_output, gc_tensor self, int64_t *padding_data, int padding_len) {
  PROTECT(
    torch::Tensor results__ = torch::reflection_pad3d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_reflection_pad3d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, int64_t *padding_data, int padding_len) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::reflection_pad3d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_reflection_pad3d_out(gc_tensor out, gc_tensor self, int64_t *padding_data, int padding_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::reflection_pad3d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_relu(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::relu(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_relu6(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::relu6(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_relu6_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::relu6_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_relu_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::relu_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_relu_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::relu_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_remainder(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::remainder(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_remainder_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).remainder_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_remainder_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::remainder_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_remainder_scalar_tensor(scalar self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::remainder(*self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_remainder_scalar_tensor_out(gc_tensor out, scalar self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::remainder_out(out_local, *self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_remainder_tensor(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::remainder(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_remainder_tensor_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).remainder_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_remainder_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::remainder_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_renorm(gc_tensor self, scalar p, int64_t dim, scalar maxnorm) {
  PROTECT(
    torch::Tensor results__ = torch::renorm(tensor_from_ocaml(self), *p, dim, *maxnorm);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_renorm_(gc_tensor self, scalar p, int64_t dim, scalar maxnorm) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).renorm_(*p, dim, *maxnorm);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_renorm_out(gc_tensor out, gc_tensor self, scalar p, int64_t dim, scalar maxnorm) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::renorm_out(out_local, tensor_from_ocaml(self), *p, dim, *maxnorm);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_repeat(gc_tensor self, int64_t *repeats_data, int repeats_len) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).repeat(torch::IntArrayRef(repeats_data, repeats_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_repeat_interleave(gc_tensor repeats, int64_t output_size_v, int output_size_null) {
  PROTECT(
    torch::Tensor results__ = torch::repeat_interleave(tensor_from_ocaml(repeats), output_size_null ? c10::nullopt : c10::optional<int64_t>(output_size_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_repeat_interleave_self_int(gc_tensor self, int64_t repeats, int64_t dim_v, int dim_null, int64_t output_size_v, int output_size_null) {
  PROTECT(
    torch::Tensor results__ = torch::repeat_interleave(tensor_from_ocaml(self), repeats, dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), output_size_null ? c10::nullopt : c10::optional<int64_t>(output_size_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_repeat_interleave_self_tensor(gc_tensor self, gc_tensor repeats, int64_t dim_v, int dim_null, int64_t output_size_v, int output_size_null) {
  PROTECT(
    torch::Tensor results__ = torch::repeat_interleave(tensor_from_ocaml(self), tensor_from_ocaml(repeats), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v), output_size_null ? c10::nullopt : c10::optional<int64_t>(output_size_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_repeat_interleave_tensor_out(gc_tensor out, gc_tensor repeats, int64_t output_size_v, int output_size_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::repeat_interleave_out(out_local, tensor_from_ocaml(repeats), output_size_null ? c10::nullopt : c10::optional<int64_t>(output_size_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_repeat_out(gc_tensor out, gc_tensor self, int64_t *repeats_data, int repeats_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::repeat_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(repeats_data, repeats_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_replication_pad1d(gc_tensor self, int64_t *padding_data, int padding_len) {
  PROTECT(
    torch::Tensor results__ = torch::replication_pad1d(tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_replication_pad1d_backward(gc_tensor grad_output, gc_tensor self, int64_t *padding_data, int padding_len) {
  PROTECT(
    torch::Tensor results__ = torch::replication_pad1d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_replication_pad1d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, int64_t *padding_data, int padding_len) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::replication_pad1d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_replication_pad1d_out(gc_tensor out, gc_tensor self, int64_t *padding_data, int padding_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::replication_pad1d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_replication_pad2d(gc_tensor self, int64_t *padding_data, int padding_len) {
  PROTECT(
    torch::Tensor results__ = torch::replication_pad2d(tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_replication_pad2d_backward(gc_tensor grad_output, gc_tensor self, int64_t *padding_data, int padding_len) {
  PROTECT(
    torch::Tensor results__ = torch::replication_pad2d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_replication_pad2d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, int64_t *padding_data, int padding_len) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::replication_pad2d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_replication_pad2d_out(gc_tensor out, gc_tensor self, int64_t *padding_data, int padding_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::replication_pad2d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_replication_pad3d(gc_tensor self, int64_t *padding_data, int padding_len) {
  PROTECT(
    torch::Tensor results__ = torch::replication_pad3d(tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_replication_pad3d_backward(gc_tensor grad_output, gc_tensor self, int64_t *padding_data, int padding_len) {
  PROTECT(
    torch::Tensor results__ = torch::replication_pad3d_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_replication_pad3d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, int64_t *padding_data, int padding_len) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::replication_pad3d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_replication_pad3d_out(gc_tensor out, gc_tensor self, int64_t *padding_data, int padding_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::replication_pad3d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_requires_grad_(gc_tensor self, int requires_grad) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).requires_grad_((bool)requires_grad);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_reshape(gc_tensor self, int64_t *shape_data, int shape_len) {
  PROTECT(
    torch::Tensor results__ = torch::reshape(tensor_from_ocaml(self), torch::IntArrayRef(shape_data, shape_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_reshape_as(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).reshape_as(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_resize(gc_tensor self, int64_t *size_data, int size_len) {
  PROTECT(
    torch::Tensor results__ = torch::resize(tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_resize_(gc_tensor self, int64_t *size_data, int size_len) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).resize_(torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_resize_as(gc_tensor self, gc_tensor the_template) {
  PROTECT(
    torch::Tensor results__ = torch::resize_as(tensor_from_ocaml(self), tensor_from_ocaml(the_template));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_resize_as_(gc_tensor self, gc_tensor the_template) {
  PROTECT(
    torch::Tensor results__ = torch::resize_as_(tensor_from_ocaml(self), tensor_from_ocaml(the_template));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_resize_as_out(gc_tensor out, gc_tensor self, gc_tensor the_template) {
  PROTECT(
    torch::Tensor results__ = torch::resize_as_out(tensor_from_ocaml(out), tensor_from_ocaml(self), tensor_from_ocaml(the_template));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_resize_as_sparse(gc_tensor self, gc_tensor the_template) {
  PROTECT(
    torch::Tensor results__ = torch::resize_as_sparse(tensor_from_ocaml(self), tensor_from_ocaml(the_template));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_resize_as_sparse_(gc_tensor self, gc_tensor the_template) {
  PROTECT(
    torch::Tensor results__ = torch::resize_as_sparse_(tensor_from_ocaml(self), tensor_from_ocaml(the_template));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_resize_as_sparse_out(gc_tensor out, gc_tensor self, gc_tensor the_template) {
  PROTECT(
    torch::Tensor results__ = torch::resize_as_sparse_out(tensor_from_ocaml(out), tensor_from_ocaml(self), tensor_from_ocaml(the_template));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_resize_out(gc_tensor out, gc_tensor self, int64_t *size_data, int size_len) {
  PROTECT(
    torch::Tensor results__ = torch::resize_out(tensor_from_ocaml(out), tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_resolve_conj(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::resolve_conj(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_resolve_neg(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::resolve_neg(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

int atg_retains_grad(gc_tensor self) {
  PROTECT(
    return tensor_from_ocaml(self).retains_grad();
  )
  return 0;
}

raw_tensor atg_rms_norm(gc_tensor input, int64_t *normalized_shape_data, int normalized_shape_len, gc_tensor weight, double eps_v, int eps_null) {
  PROTECT(
    torch::Tensor results__ = torch::rms_norm(tensor_from_ocaml(input), torch::IntArrayRef(normalized_shape_data, normalized_shape_len), weight ? std::make_optional(tensor_from_ocaml(weight)) : std::nullopt, eps_null ? c10::nullopt : c10::optional<double>(eps_v));
    return tensor_to_ocaml(results__);
  )
}

void atg_rnn_relu(raw_tensor *out__, gc_tensor input, gc_tensor hx, gc_tensor *params_data, int params_len, int has_biases, int64_t num_layers, double dropout, int train, int bidirectional, int batch_first) {
  PROTECT(
    auto results__ = torch::rnn_relu(tensor_from_ocaml(input), tensor_from_ocaml(hx), of_carray_tensor(params_data, params_len), (bool)has_biases, num_layers, dropout, (bool)train, (bool)bidirectional, (bool)batch_first);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_rnn_relu_cell(gc_tensor input, gc_tensor hx, gc_tensor w_ih, gc_tensor w_hh, gc_tensor b_ih, gc_tensor b_hh) {
  PROTECT(
    torch::Tensor results__ = torch::rnn_relu_cell(tensor_from_ocaml(input), tensor_from_ocaml(hx), tensor_from_ocaml(w_ih), tensor_from_ocaml(w_hh), b_ih ? std::make_optional(tensor_from_ocaml(b_ih)) : std::nullopt, b_hh ? std::make_optional(tensor_from_ocaml(b_hh)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

void atg_rnn_relu_data(raw_tensor *out__, gc_tensor data, gc_tensor batch_sizes, gc_tensor hx, gc_tensor *params_data, int params_len, int has_biases, int64_t num_layers, double dropout, int train, int bidirectional) {
  PROTECT(
    auto results__ = torch::rnn_relu(tensor_from_ocaml(data), tensor_from_ocaml(batch_sizes), tensor_from_ocaml(hx), of_carray_tensor(params_data, params_len), (bool)has_biases, num_layers, dropout, (bool)train, (bool)bidirectional);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_rnn_tanh(raw_tensor *out__, gc_tensor input, gc_tensor hx, gc_tensor *params_data, int params_len, int has_biases, int64_t num_layers, double dropout, int train, int bidirectional, int batch_first) {
  PROTECT(
    auto results__ = torch::rnn_tanh(tensor_from_ocaml(input), tensor_from_ocaml(hx), of_carray_tensor(params_data, params_len), (bool)has_biases, num_layers, dropout, (bool)train, (bool)bidirectional, (bool)batch_first);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_rnn_tanh_cell(gc_tensor input, gc_tensor hx, gc_tensor w_ih, gc_tensor w_hh, gc_tensor b_ih, gc_tensor b_hh) {
  PROTECT(
    torch::Tensor results__ = torch::rnn_tanh_cell(tensor_from_ocaml(input), tensor_from_ocaml(hx), tensor_from_ocaml(w_ih), tensor_from_ocaml(w_hh), b_ih ? std::make_optional(tensor_from_ocaml(b_ih)) : std::nullopt, b_hh ? std::make_optional(tensor_from_ocaml(b_hh)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

void atg_rnn_tanh_data(raw_tensor *out__, gc_tensor data, gc_tensor batch_sizes, gc_tensor hx, gc_tensor *params_data, int params_len, int has_biases, int64_t num_layers, double dropout, int train, int bidirectional) {
  PROTECT(
    auto results__ = torch::rnn_tanh(tensor_from_ocaml(data), tensor_from_ocaml(batch_sizes), tensor_from_ocaml(hx), of_carray_tensor(params_data, params_len), (bool)has_biases, num_layers, dropout, (bool)train, (bool)bidirectional);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_roll(gc_tensor self, int64_t *shifts_data, int shifts_len, int64_t *dims_data, int dims_len) {
  PROTECT(
    torch::Tensor results__ = torch::roll(tensor_from_ocaml(self), torch::IntArrayRef(shifts_data, shifts_len), torch::IntArrayRef(dims_data, dims_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_roll_out(gc_tensor out, gc_tensor self, int64_t *shifts_data, int shifts_len, int64_t *dims_data, int dims_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::roll_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(shifts_data, shifts_len), torch::IntArrayRef(dims_data, dims_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rot90(gc_tensor self, int64_t k, int64_t *dims_data, int dims_len) {
  PROTECT(
    torch::Tensor results__ = torch::rot90(tensor_from_ocaml(self), k, torch::IntArrayRef(dims_data, dims_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rot90_out(gc_tensor out, gc_tensor self, int64_t k, int64_t *dims_data, int dims_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::rot90_out(out_local, tensor_from_ocaml(self), k, torch::IntArrayRef(dims_data, dims_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_round(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::round(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_round_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::round_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_round_decimals(gc_tensor self, int64_t decimals) {
  PROTECT(
    torch::Tensor results__ = torch::round(tensor_from_ocaml(self), decimals);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_round_decimals_(gc_tensor self, int64_t decimals) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::round_(self_local, decimals);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_round_decimals_out(gc_tensor out, gc_tensor self, int64_t decimals) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::round_out(out_local, tensor_from_ocaml(self), decimals);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_round_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::round_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_row_indices(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).row_indices();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_row_indices_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::row_indices_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_row_indices_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::row_indices_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_row_stack(gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    torch::Tensor results__ = torch::row_stack(of_carray_tensor(tensors_data, tensors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_row_stack_out(gc_tensor out, gc_tensor *tensors_data, int tensors_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::row_stack_out(out_local, of_carray_tensor(tensors_data, tensors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rrelu(gc_tensor self, scalar lower, scalar upper, int training) {
  PROTECT(
    torch::Tensor results__ = torch::rrelu(tensor_from_ocaml(self), lower ? *lower : c10::Scalar{0.125} , upper ? *upper : c10::Scalar{0.3333333333333333} , (bool)training);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rrelu_(gc_tensor self, scalar lower, scalar upper, int training) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::rrelu_(self_local, lower ? *lower : c10::Scalar{0.125} , upper ? *upper : c10::Scalar{0.3333333333333333} , (bool)training);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rrelu_with_noise(gc_tensor self, gc_tensor noise, scalar lower, scalar upper, int training) {
  torch::Tensor noise_local = tensor_from_ocaml(noise);
  PROTECT(
    torch::Tensor results__ = torch::rrelu_with_noise(tensor_from_ocaml(self), noise_local, lower ? *lower : c10::Scalar{0.125} , upper ? *upper : c10::Scalar{0.3333333333333333} , (bool)training);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rrelu_with_noise_(gc_tensor self, gc_tensor noise, scalar lower, scalar upper, int training) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  torch::Tensor noise_local = tensor_from_ocaml(noise);
  PROTECT(
    torch::Tensor results__ = torch::rrelu_with_noise_(self_local, noise_local, lower ? *lower : c10::Scalar{0.125} , upper ? *upper : c10::Scalar{0.3333333333333333} , (bool)training);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rrelu_with_noise_backward(gc_tensor grad_output, gc_tensor self, gc_tensor noise, scalar lower, scalar upper, int training, int self_is_result) {
  PROTECT(
    torch::Tensor results__ = torch::rrelu_with_noise_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(noise), *lower, *upper, (bool)training, (bool)self_is_result);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rrelu_with_noise_backward_out(gc_tensor out, gc_tensor grad_output, gc_tensor self, gc_tensor noise, scalar lower, scalar upper, int training, int self_is_result) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::rrelu_with_noise_backward_out(out_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(noise), *lower, *upper, (bool)training, (bool)self_is_result);
    return tensor_to_ocaml(results__);
  )
}

void atg_rrelu_with_noise_functional(raw_tensor *out__, gc_tensor self, gc_tensor noise, scalar lower, scalar upper, int training) {
  PROTECT(
    auto results__ = torch::rrelu_with_noise_functional(tensor_from_ocaml(self), tensor_from_ocaml(noise), lower ? *lower : c10::Scalar{0.125} , upper ? *upper : c10::Scalar{0.3333333333333333} , (bool)training);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_rrelu_with_noise_out(gc_tensor out, gc_tensor self, gc_tensor noise, scalar lower, scalar upper, int training) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  torch::Tensor noise_local = tensor_from_ocaml(noise);
  PROTECT(
    torch::Tensor results__ = torch::rrelu_with_noise_out(out_local, tensor_from_ocaml(self), noise_local, lower ? *lower : c10::Scalar{0.125} , upper ? *upper : c10::Scalar{0.3333333333333333} , (bool)training);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rsqrt(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::rsqrt(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rsqrt_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::rsqrt_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rsqrt_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::rsqrt_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rsub(gc_tensor self, gc_tensor other, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::rsub(tensor_from_ocaml(self), tensor_from_ocaml(other), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rsub_scalar(gc_tensor self, scalar other, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::rsub(tensor_from_ocaml(self), *other, alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rsub_scalar_out(gc_tensor out, gc_tensor self, scalar other, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::rsub_out(out_local, tensor_from_ocaml(self), *other, alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_rsub_tensor_out(gc_tensor out, gc_tensor self, gc_tensor other, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::rsub_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scalar_tensor(scalar s, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::scalar_tensor(*s, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scalar_tensor_out(gc_tensor out, scalar s) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::scalar_tensor_out(out_local, *s);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scaled_dot_product_attention(gc_tensor query, gc_tensor key, gc_tensor value, gc_tensor attn_mask, double dropout_p, int is_causal, double scale_v, int scale_null, int enable_gqa) {
  PROTECT(
    torch::Tensor results__ = torch::scaled_dot_product_attention(tensor_from_ocaml(query), tensor_from_ocaml(key), tensor_from_ocaml(value), attn_mask ? std::make_optional(tensor_from_ocaml(attn_mask)) : std::nullopt, dropout_p, (bool)is_causal, scale_null ? c10::nullopt : c10::optional<double>(scale_v), (bool)enable_gqa);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scatter(gc_tensor self, int64_t dim, gc_tensor index, gc_tensor src) {
  PROTECT(
    torch::Tensor results__ = torch::scatter(tensor_from_ocaml(self), dim, tensor_from_ocaml(index), tensor_from_ocaml(src));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scatter_(gc_tensor self, int64_t dim, gc_tensor index, gc_tensor src) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).scatter_(dim, tensor_from_ocaml(index), tensor_from_ocaml(src));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scatter_add(gc_tensor self, int64_t dim, gc_tensor index, gc_tensor src) {
  PROTECT(
    torch::Tensor results__ = torch::scatter_add(tensor_from_ocaml(self), dim, tensor_from_ocaml(index), tensor_from_ocaml(src));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scatter_add_(gc_tensor self, int64_t dim, gc_tensor index, gc_tensor src) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).scatter_add_(dim, tensor_from_ocaml(index), tensor_from_ocaml(src));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scatter_add_out(gc_tensor out, gc_tensor self, int64_t dim, gc_tensor index, gc_tensor src) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::scatter_add_out(out_local, tensor_from_ocaml(self), dim, tensor_from_ocaml(index), tensor_from_ocaml(src));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scatter_reduce(gc_tensor self, int64_t dim, gc_tensor index, gc_tensor src, char * reduce) {
  PROTECT(
    torch::Tensor results__ = torch::scatter(tensor_from_ocaml(self), dim, tensor_from_ocaml(index), tensor_from_ocaml(src), std::string(reduce));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scatter_reduce_(gc_tensor self, int64_t dim, gc_tensor index, gc_tensor src, char * reduce) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).scatter_(dim, tensor_from_ocaml(index), tensor_from_ocaml(src), std::string(reduce));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scatter_reduce_out(gc_tensor out, gc_tensor self, int64_t dim, gc_tensor index, gc_tensor src, char * reduce) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::scatter_out(out_local, tensor_from_ocaml(self), dim, tensor_from_ocaml(index), tensor_from_ocaml(src), std::string(reduce));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scatter_src_out(gc_tensor out, gc_tensor self, int64_t dim, gc_tensor index, gc_tensor src) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::scatter_out(out_local, tensor_from_ocaml(self), dim, tensor_from_ocaml(index), tensor_from_ocaml(src));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scatter_value(gc_tensor self, int64_t dim, gc_tensor index, scalar value) {
  PROTECT(
    torch::Tensor results__ = torch::scatter(tensor_from_ocaml(self), dim, tensor_from_ocaml(index), *value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scatter_value_(gc_tensor self, int64_t dim, gc_tensor index, scalar value) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).scatter_(dim, tensor_from_ocaml(index), *value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scatter_value_out(gc_tensor out, gc_tensor self, int64_t dim, gc_tensor index, scalar value) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::scatter_out(out_local, tensor_from_ocaml(self), dim, tensor_from_ocaml(index), *value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scatter_value_reduce(gc_tensor self, int64_t dim, gc_tensor index, scalar value, char * reduce) {
  PROTECT(
    torch::Tensor results__ = torch::scatter(tensor_from_ocaml(self), dim, tensor_from_ocaml(index), *value, std::string(reduce));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scatter_value_reduce_(gc_tensor self, int64_t dim, gc_tensor index, scalar value, char * reduce) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).scatter_(dim, tensor_from_ocaml(index), *value, std::string(reduce));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_scatter_value_reduce_out(gc_tensor out, gc_tensor self, int64_t dim, gc_tensor index, scalar value, char * reduce) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::scatter_out(out_local, tensor_from_ocaml(self), dim, tensor_from_ocaml(index), *value, std::string(reduce));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_searchsorted(gc_tensor sorted_sequence, gc_tensor self, int out_int32, int right, char * side_v, int side_null, gc_tensor sorter) {
  PROTECT(
    torch::Tensor results__ = torch::searchsorted(tensor_from_ocaml(sorted_sequence), tensor_from_ocaml(self), (bool)out_int32, (bool)right, side_null ? c10::nullopt : c10::optional<c10::string_view>(side_v), sorter ? std::make_optional(tensor_from_ocaml(sorter)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_searchsorted_scalar(gc_tensor sorted_sequence, scalar self, int out_int32, int right, char * side_v, int side_null, gc_tensor sorter) {
  PROTECT(
    torch::Tensor results__ = torch::searchsorted(tensor_from_ocaml(sorted_sequence), *self, (bool)out_int32, (bool)right, side_null ? c10::nullopt : c10::optional<c10::string_view>(side_v), sorter ? std::make_optional(tensor_from_ocaml(sorter)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_searchsorted_scalar_out(gc_tensor out, gc_tensor sorted_sequence, scalar self, int out_int32, int right, char * side_v, int side_null, gc_tensor sorter) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::searchsorted_out(out_local, tensor_from_ocaml(sorted_sequence), *self, (bool)out_int32, (bool)right, side_null ? c10::nullopt : c10::optional<c10::string_view>(side_v), sorter ? std::make_optional(tensor_from_ocaml(sorter)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_searchsorted_tensor_out(gc_tensor out, gc_tensor sorted_sequence, gc_tensor self, int out_int32, int right, char * side_v, int side_null, gc_tensor sorter) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::searchsorted_out(out_local, tensor_from_ocaml(sorted_sequence), tensor_from_ocaml(self), (bool)out_int32, (bool)right, side_null ? c10::nullopt : c10::optional<c10::string_view>(side_v), sorter ? std::make_optional(tensor_from_ocaml(sorter)) : std::nullopt);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_segment_reduce(gc_tensor data, char * reduce, gc_tensor lengths, gc_tensor indices, gc_tensor offsets, int64_t axis, int unsafe, scalar initial) {
  PROTECT(
    torch::Tensor results__ = torch::segment_reduce(tensor_from_ocaml(data), std::string(reduce), lengths ? std::make_optional(tensor_from_ocaml(lengths)) : std::nullopt, indices ? std::make_optional(tensor_from_ocaml(indices)) : std::nullopt, offsets ? std::make_optional(tensor_from_ocaml(offsets)) : std::nullopt, axis, (bool)unsafe, *initial);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_segment_reduce_out(gc_tensor out, gc_tensor data, char * reduce, gc_tensor lengths, gc_tensor indices, gc_tensor offsets, int64_t axis, int unsafe, scalar initial) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::segment_reduce_out(out_local, tensor_from_ocaml(data), std::string(reduce), lengths ? std::make_optional(tensor_from_ocaml(lengths)) : std::nullopt, indices ? std::make_optional(tensor_from_ocaml(indices)) : std::nullopt, offsets ? std::make_optional(tensor_from_ocaml(offsets)) : std::nullopt, axis, (bool)unsafe, *initial);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_select(gc_tensor self, int64_t dim, int64_t index) {
  PROTECT(
    torch::Tensor results__ = torch::select(tensor_from_ocaml(self), dim, index);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_select_backward(gc_tensor grad_output, int64_t *input_sizes_data, int input_sizes_len, int64_t dim, int64_t index) {
  PROTECT(
    torch::Tensor results__ = torch::select_backward(tensor_from_ocaml(grad_output), torch::IntArrayRef(input_sizes_data, input_sizes_len), dim, index);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_select_backward_out(gc_tensor out, gc_tensor grad_output, int64_t *input_sizes_data, int input_sizes_len, int64_t dim, int64_t index) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::select_backward_out(out_local, tensor_from_ocaml(grad_output), torch::IntArrayRef(input_sizes_data, input_sizes_len), dim, index);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_select_copy(gc_tensor self, int64_t dim, int64_t index) {
  PROTECT(
    torch::Tensor results__ = torch::select_copy(tensor_from_ocaml(self), dim, index);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_select_copy_int_out(gc_tensor out, gc_tensor self, int64_t dim, int64_t index) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::select_copy_out(out_local, tensor_from_ocaml(self), dim, index);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_select_scatter(gc_tensor self, gc_tensor src, int64_t dim, int64_t index) {
  PROTECT(
    torch::Tensor results__ = torch::select_scatter(tensor_from_ocaml(self), tensor_from_ocaml(src), dim, index);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_select_scatter_out(gc_tensor out, gc_tensor self, gc_tensor src, int64_t dim, int64_t index) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::select_scatter_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(src), dim, index);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_selu(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::selu(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_selu_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::selu_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_set(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::set(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_set_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).set_();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_set_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::set_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_set_requires_grad(gc_tensor self, int r) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).set_requires_grad((bool)r);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_set_source_tensor(gc_tensor self, gc_tensor source) {
  PROTECT(
    torch::Tensor results__ = torch::set(tensor_from_ocaml(self), tensor_from_ocaml(source));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_set_source_tensor_(gc_tensor self, gc_tensor source) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).set_(tensor_from_ocaml(source));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_set_source_tensor_out(gc_tensor out, gc_tensor self, gc_tensor source) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::set_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(source));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_set_source_tensor_storage_offset_(gc_tensor self, gc_tensor source, int64_t storage_offset, int64_t *size_data, int size_len, int64_t *stride_data, int stride_len) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).set_(tensor_from_ocaml(source), storage_offset, torch::IntArrayRef(size_data, size_len), torch::IntArrayRef(stride_data, stride_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sgn(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::sgn(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sgn_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).sgn_();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sgn_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::sgn_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sigmoid(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::sigmoid(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sigmoid_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::sigmoid_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sigmoid_backward(gc_tensor grad_output, gc_tensor output) {
  PROTECT(
    torch::Tensor results__ = torch::sigmoid_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(output));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sigmoid_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor output) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::sigmoid_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(output));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sigmoid_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::sigmoid_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sign(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::sign(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sign_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).sign_();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sign_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::sign_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_signbit(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::signbit(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_signbit_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::signbit_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_silu(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::silu(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_silu_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::silu_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_silu_backward(gc_tensor grad_output, gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::silu_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_silu_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::silu_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_silu_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::silu_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sin(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::sin(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sin_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::sin_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sin_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::sin_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sinc(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::sinc(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sinc_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::sinc_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sinc_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::sinc_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sinh(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::sinh(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sinh_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::sinh_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sinh_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::sinh_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

int64_t atg_size(gc_tensor self, int64_t dim) {
  PROTECT(
    return torch::size(tensor_from_ocaml(self), dim);
  )
  return 0;
}

raw_tensor atg_slice(gc_tensor self, int64_t dim, int64_t start_v, int start_null, int64_t end_v, int end_null, int64_t step) {
  PROTECT(
    torch::Tensor results__ = torch::slice(tensor_from_ocaml(self), dim, start_null ? c10::nullopt : c10::optional<int64_t>(start_v), end_null ? c10::nullopt : c10::optional<int64_t>(end_v), step);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_slice_backward(gc_tensor grad_output, int64_t *input_sizes_data, int input_sizes_len, int64_t dim, int64_t start, int64_t end, int64_t step) {
  PROTECT(
    torch::Tensor results__ = torch::slice_backward(tensor_from_ocaml(grad_output), torch::IntArrayRef(input_sizes_data, input_sizes_len), dim, start, end, step);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_slice_backward_out(gc_tensor out, gc_tensor grad_output, int64_t *input_sizes_data, int input_sizes_len, int64_t dim, int64_t start, int64_t end, int64_t step) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::slice_backward_out(out_local, tensor_from_ocaml(grad_output), torch::IntArrayRef(input_sizes_data, input_sizes_len), dim, start, end, step);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_slice_copy(gc_tensor self, int64_t dim, int64_t start_v, int start_null, int64_t end_v, int end_null, int64_t step) {
  PROTECT(
    torch::Tensor results__ = torch::slice_copy(tensor_from_ocaml(self), dim, start_null ? c10::nullopt : c10::optional<int64_t>(start_v), end_null ? c10::nullopt : c10::optional<int64_t>(end_v), step);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_slice_copy_tensor_out(gc_tensor out, gc_tensor self, int64_t dim, int64_t start_v, int start_null, int64_t end_v, int end_null, int64_t step) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::slice_copy_out(out_local, tensor_from_ocaml(self), dim, start_null ? c10::nullopt : c10::optional<int64_t>(start_v), end_null ? c10::nullopt : c10::optional<int64_t>(end_v), step);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_slice_inverse(gc_tensor self, gc_tensor src, int64_t dim, int64_t start_v, int start_null, int64_t end_v, int end_null, int64_t step) {
  PROTECT(
    torch::Tensor results__ = torch::slice_inverse(tensor_from_ocaml(self), tensor_from_ocaml(src), dim, start_null ? c10::nullopt : c10::optional<int64_t>(start_v), end_null ? c10::nullopt : c10::optional<int64_t>(end_v), step);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_slice_scatter(gc_tensor self, gc_tensor src, int64_t dim, int64_t start_v, int start_null, int64_t end_v, int end_null, int64_t step) {
  PROTECT(
    torch::Tensor results__ = torch::slice_scatter(tensor_from_ocaml(self), tensor_from_ocaml(src), dim, start_null ? c10::nullopt : c10::optional<int64_t>(start_v), end_null ? c10::nullopt : c10::optional<int64_t>(end_v), step);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_slice_scatter_out(gc_tensor out, gc_tensor self, gc_tensor src, int64_t dim, int64_t start_v, int start_null, int64_t end_v, int end_null, int64_t step) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::slice_scatter_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(src), dim, start_null ? c10::nullopt : c10::optional<int64_t>(start_v), end_null ? c10::nullopt : c10::optional<int64_t>(end_v), step);
    return tensor_to_ocaml(results__);
  )
}

void atg_slogdet(raw_tensor *out__, gc_tensor self) {
  PROTECT(
    auto results__ = torch::slogdet(tensor_from_ocaml(self));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_slogdet_out(raw_tensor *out__, gc_tensor sign, gc_tensor logabsdet, gc_tensor self) {
  torch::Tensor sign_local = tensor_from_ocaml(sign);
  torch::Tensor logabsdet_local = tensor_from_ocaml(logabsdet);
  PROTECT(
    auto results__ = torch::slogdet_out(sign_local, logabsdet_local, tensor_from_ocaml(self));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_slow_conv3d(gc_tensor self, gc_tensor weight, int64_t *kernel_size_data, int kernel_size_len, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len) {
  PROTECT(
    torch::Tensor results__ = torch::slow_conv3d(tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(kernel_size_data, kernel_size_len), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_slow_conv3d_out(gc_tensor out, gc_tensor self, gc_tensor weight, int64_t *kernel_size_data, int kernel_size_len, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::slow_conv3d_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(kernel_size_data, kernel_size_len), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_slow_conv_dilated2d(gc_tensor self, gc_tensor weight, int64_t *kernel_size_data, int kernel_size_len, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len) {
  PROTECT(
    torch::Tensor results__ = torch::slow_conv_dilated2d(tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(kernel_size_data, kernel_size_len), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_slow_conv_dilated2d_out(gc_tensor out, gc_tensor self, gc_tensor weight, int64_t *kernel_size_data, int kernel_size_len, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::slow_conv_dilated2d_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(kernel_size_data, kernel_size_len), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_slow_conv_dilated3d(gc_tensor self, gc_tensor weight, int64_t *kernel_size_data, int kernel_size_len, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len) {
  PROTECT(
    torch::Tensor results__ = torch::slow_conv_dilated3d(tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(kernel_size_data, kernel_size_len), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_slow_conv_dilated3d_out(gc_tensor out, gc_tensor self, gc_tensor weight, int64_t *kernel_size_data, int kernel_size_len, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *dilation_data, int dilation_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::slow_conv_dilated3d_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(kernel_size_data, kernel_size_len), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(dilation_data, dilation_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_slow_conv_transpose2d(gc_tensor self, gc_tensor weight, int64_t *kernel_size_data, int kernel_size_len, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *output_padding_data, int output_padding_len, int64_t *dilation_data, int dilation_len) {
  PROTECT(
    torch::Tensor results__ = torch::slow_conv_transpose2d(tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(kernel_size_data, kernel_size_len), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(output_padding_data, output_padding_len), torch::IntArrayRef(dilation_data, dilation_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_slow_conv_transpose2d_out(gc_tensor out, gc_tensor self, gc_tensor weight, int64_t *kernel_size_data, int kernel_size_len, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *output_padding_data, int output_padding_len, int64_t *dilation_data, int dilation_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::slow_conv_transpose2d_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(kernel_size_data, kernel_size_len), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(output_padding_data, output_padding_len), torch::IntArrayRef(dilation_data, dilation_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_slow_conv_transpose3d(gc_tensor self, gc_tensor weight, int64_t *kernel_size_data, int kernel_size_len, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *output_padding_data, int output_padding_len, int64_t *dilation_data, int dilation_len) {
  PROTECT(
    torch::Tensor results__ = torch::slow_conv_transpose3d(tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(kernel_size_data, kernel_size_len), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(output_padding_data, output_padding_len), torch::IntArrayRef(dilation_data, dilation_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_slow_conv_transpose3d_out(gc_tensor out, gc_tensor self, gc_tensor weight, int64_t *kernel_size_data, int kernel_size_len, gc_tensor bias, int64_t *stride_data, int stride_len, int64_t *padding_data, int padding_len, int64_t *output_padding_data, int output_padding_len, int64_t *dilation_data, int dilation_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::slow_conv_transpose3d_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(weight), torch::IntArrayRef(kernel_size_data, kernel_size_len), bias ? std::make_optional(tensor_from_ocaml(bias)) : std::nullopt, torch::IntArrayRef(stride_data, stride_len), torch::IntArrayRef(padding_data, padding_len), torch::IntArrayRef(output_padding_data, output_padding_len), torch::IntArrayRef(dilation_data, dilation_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_smm(gc_tensor self, gc_tensor mat2) {
  PROTECT(
    torch::Tensor results__ = torch::smm(tensor_from_ocaml(self), tensor_from_ocaml(mat2));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_smooth_l1_loss(gc_tensor self, gc_tensor target, int64_t reduction, double beta) {
  PROTECT(
    torch::Tensor results__ = torch::smooth_l1_loss(tensor_from_ocaml(self), tensor_from_ocaml(target), reduction, beta);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_smooth_l1_loss_backward(gc_tensor grad_output, gc_tensor self, gc_tensor target, int64_t reduction, double beta) {
  PROTECT(
    torch::Tensor results__ = torch::smooth_l1_loss_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), reduction, beta);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_smooth_l1_loss_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, gc_tensor target, int64_t reduction, double beta) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::smooth_l1_loss_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), reduction, beta);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_smooth_l1_loss_out(gc_tensor out, gc_tensor self, gc_tensor target, int64_t reduction, double beta) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::smooth_l1_loss_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(target), reduction, beta);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_soft_margin_loss(gc_tensor self, gc_tensor target, int64_t reduction) {
  PROTECT(
    torch::Tensor results__ = torch::soft_margin_loss(tensor_from_ocaml(self), tensor_from_ocaml(target), reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_soft_margin_loss_backward(gc_tensor grad_output, gc_tensor self, gc_tensor target, int64_t reduction) {
  PROTECT(
    torch::Tensor results__ = torch::soft_margin_loss_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_soft_margin_loss_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, gc_tensor target, int64_t reduction) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::soft_margin_loss_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), tensor_from_ocaml(target), reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_soft_margin_loss_out(gc_tensor out, gc_tensor self, gc_tensor target, int64_t reduction) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::soft_margin_loss_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(target), reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_softmax(gc_tensor self, int64_t dim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::softmax(tensor_from_ocaml(self), dim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_softmax_int_out(gc_tensor out, gc_tensor self, int64_t dim, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::softmax_out(out_local, tensor_from_ocaml(self), dim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_softplus(gc_tensor self, scalar beta, scalar threshold) {
  PROTECT(
    torch::Tensor results__ = torch::softplus(tensor_from_ocaml(self), beta ? *beta : c10::Scalar{1} , threshold ? *threshold : c10::Scalar{20} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_softplus_backward(gc_tensor grad_output, gc_tensor self, scalar beta, scalar threshold) {
  PROTECT(
    torch::Tensor results__ = torch::softplus_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), *beta, *threshold);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_softplus_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, scalar beta, scalar threshold) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::softplus_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), *beta, *threshold);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_softplus_out(gc_tensor out, gc_tensor self, scalar beta, scalar threshold) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::softplus_out(out_local, tensor_from_ocaml(self), beta ? *beta : c10::Scalar{1} , threshold ? *threshold : c10::Scalar{20} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_softshrink(gc_tensor self, scalar lambd) {
  PROTECT(
    torch::Tensor results__ = torch::softshrink(tensor_from_ocaml(self), lambd ? *lambd : c10::Scalar{0.5} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_softshrink_backward(gc_tensor grad_output, gc_tensor self, scalar lambd) {
  PROTECT(
    torch::Tensor results__ = torch::softshrink_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), *lambd);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_softshrink_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, scalar lambd) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::softshrink_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), *lambd);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_softshrink_out(gc_tensor out, gc_tensor self, scalar lambd) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::softshrink_out(out_local, tensor_from_ocaml(self), lambd ? *lambd : c10::Scalar{0.5} );
    return tensor_to_ocaml(results__);
  )
}

void atg_sort(raw_tensor *out__, gc_tensor self, int64_t dim, int descending) {
  PROTECT(
    auto results__ = torch::sort(tensor_from_ocaml(self), dim, (bool)descending);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_sort_stable(raw_tensor *out__, gc_tensor self, int stable, int64_t dim, int descending) {
  PROTECT(
    auto results__ = torch::sort(tensor_from_ocaml(self), (bool)stable, dim, (bool)descending);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_sort_values(raw_tensor *out__, gc_tensor values, gc_tensor indices, gc_tensor self, int64_t dim, int descending) {
  torch::Tensor values_local = tensor_from_ocaml(values);
  torch::Tensor indices_local = tensor_from_ocaml(indices);
  PROTECT(
    auto results__ = torch::sort_out(values_local, indices_local, tensor_from_ocaml(self), dim, (bool)descending);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_sort_values_stable(raw_tensor *out__, gc_tensor values, gc_tensor indices, gc_tensor self, int stable, int64_t dim, int descending) {
  torch::Tensor values_local = tensor_from_ocaml(values);
  torch::Tensor indices_local = tensor_from_ocaml(indices);
  PROTECT(
    auto results__ = torch::sort_out(values_local, indices_local, tensor_from_ocaml(self), (bool)stable, dim, (bool)descending);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_sparse_bsc_tensor(gc_tensor ccol_indices, gc_tensor row_indices, gc_tensor values, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_bsc_tensor(tensor_from_ocaml(ccol_indices), tensor_from_ocaml(row_indices), tensor_from_ocaml(values), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_bsc_tensor_ccol_row_value_size(gc_tensor ccol_indices, gc_tensor row_indices, gc_tensor values, int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_bsc_tensor(tensor_from_ocaml(ccol_indices), tensor_from_ocaml(row_indices), tensor_from_ocaml(values), torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_bsr_tensor(gc_tensor crow_indices, gc_tensor col_indices, gc_tensor values, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_bsr_tensor(tensor_from_ocaml(crow_indices), tensor_from_ocaml(col_indices), tensor_from_ocaml(values), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_bsr_tensor_crow_col_value_size(gc_tensor crow_indices, gc_tensor col_indices, gc_tensor values, int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_bsr_tensor(tensor_from_ocaml(crow_indices), tensor_from_ocaml(col_indices), tensor_from_ocaml(values), torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_compressed_tensor(gc_tensor compressed_indices, gc_tensor plain_indices, gc_tensor values, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_compressed_tensor(tensor_from_ocaml(compressed_indices), tensor_from_ocaml(plain_indices), tensor_from_ocaml(values), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_compressed_tensor_comp_plain_value_size(gc_tensor compressed_indices, gc_tensor plain_indices, gc_tensor values, int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_compressed_tensor(tensor_from_ocaml(compressed_indices), tensor_from_ocaml(plain_indices), tensor_from_ocaml(values), torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_coo_tensor(int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_coo_tensor(torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_coo_tensor_indices(gc_tensor indices, gc_tensor values, int options_kind, int options_device, int is_coalesced) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_coo_tensor(tensor_from_ocaml(indices), tensor_from_ocaml(values), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)), (bool)is_coalesced);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_coo_tensor_indices_size(gc_tensor indices, gc_tensor values, int64_t *size_data, int size_len, int options_kind, int options_device, int is_coalesced) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_coo_tensor(tensor_from_ocaml(indices), tensor_from_ocaml(values), torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)), (bool)is_coalesced);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_coo_tensor_size_out(gc_tensor out, int64_t *size_data, int size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::sparse_coo_tensor_out(out_local, torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_csc_tensor(gc_tensor ccol_indices, gc_tensor row_indices, gc_tensor values, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_csc_tensor(tensor_from_ocaml(ccol_indices), tensor_from_ocaml(row_indices), tensor_from_ocaml(values), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_csc_tensor_ccol_row_value_size(gc_tensor ccol_indices, gc_tensor row_indices, gc_tensor values, int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_csc_tensor(tensor_from_ocaml(ccol_indices), tensor_from_ocaml(row_indices), tensor_from_ocaml(values), torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_csr_tensor(gc_tensor crow_indices, gc_tensor col_indices, gc_tensor values, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_csr_tensor(tensor_from_ocaml(crow_indices), tensor_from_ocaml(col_indices), tensor_from_ocaml(values), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_csr_tensor_crow_col_value_size(gc_tensor crow_indices, gc_tensor col_indices, gc_tensor values, int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_csr_tensor(tensor_from_ocaml(crow_indices), tensor_from_ocaml(col_indices), tensor_from_ocaml(values), torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

int64_t atg_sparse_dim(gc_tensor self) {
  PROTECT(
    return tensor_from_ocaml(self).sparse_dim();
  )
  return 0;
}

raw_tensor atg_sparse_mask(gc_tensor self, gc_tensor mask) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).sparse_mask(tensor_from_ocaml(mask));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_mask_out(gc_tensor out, gc_tensor self, gc_tensor mask) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::sparse_mask_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mask));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_resize(gc_tensor self, int64_t *size_data, int size_len, int64_t sparse_dim, int64_t dense_dim) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_resize(tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), sparse_dim, dense_dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_resize_(gc_tensor self, int64_t *size_data, int size_len, int64_t sparse_dim, int64_t dense_dim) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).sparse_resize_(torch::IntArrayRef(size_data, size_len), sparse_dim, dense_dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_resize_and_clear(gc_tensor self, int64_t *size_data, int size_len, int64_t sparse_dim, int64_t dense_dim) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_resize_and_clear(tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), sparse_dim, dense_dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_resize_and_clear_(gc_tensor self, int64_t *size_data, int size_len, int64_t sparse_dim, int64_t dense_dim) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).sparse_resize_and_clear_(torch::IntArrayRef(size_data, size_len), sparse_dim, dense_dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_resize_and_clear_out(gc_tensor out, gc_tensor self, int64_t *size_data, int size_len, int64_t sparse_dim, int64_t dense_dim) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_resize_and_clear_out(tensor_from_ocaml(out), tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), sparse_dim, dense_dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_resize_out(gc_tensor out, gc_tensor self, int64_t *size_data, int size_len, int64_t sparse_dim, int64_t dense_dim) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_resize_out(tensor_from_ocaml(out), tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len), sparse_dim, dense_dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_sampled_addmm(gc_tensor self, gc_tensor mat1, gc_tensor mat2, scalar beta, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::sparse_sampled_addmm(tensor_from_ocaml(self), tensor_from_ocaml(mat1), tensor_from_ocaml(mat2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sparse_sampled_addmm_out(gc_tensor out, gc_tensor self, gc_tensor mat1, gc_tensor mat2, scalar beta, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::sparse_sampled_addmm_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mat1), tensor_from_ocaml(mat2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_airy_ai(gc_tensor x) {
  PROTECT(
    torch::Tensor results__ = torch::special_airy_ai(tensor_from_ocaml(x));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_airy_ai_out(gc_tensor out, gc_tensor x) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_airy_ai_out(out_local, tensor_from_ocaml(x));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_bessel_j0(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_bessel_j0(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_bessel_j0_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_bessel_j0_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_bessel_j1(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_bessel_j1(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_bessel_j1_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_bessel_j1_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_bessel_y0(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_bessel_y0(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_bessel_y0_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_bessel_y0_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_bessel_y1(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_bessel_y1(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_bessel_y1_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_bessel_y1_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_t(gc_tensor x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_t(tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_t_n_scalar(gc_tensor x, scalar n) {
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_t(tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_t_n_scalar_out(gc_tensor out, gc_tensor x, scalar n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_t_out(out_local, tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_t_out(gc_tensor out, gc_tensor x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_t_out(out_local, tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_t_x_scalar(scalar x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_t(*x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_t_x_scalar_out(gc_tensor out, scalar x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_t_out(out_local, *x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_u(gc_tensor x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_u(tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_u_n_scalar(gc_tensor x, scalar n) {
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_u(tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_u_n_scalar_out(gc_tensor out, gc_tensor x, scalar n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_u_out(out_local, tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_u_out(gc_tensor out, gc_tensor x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_u_out(out_local, tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_u_x_scalar(scalar x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_u(*x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_u_x_scalar_out(gc_tensor out, scalar x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_u_out(out_local, *x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_v(gc_tensor x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_v(tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_v_n_scalar(gc_tensor x, scalar n) {
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_v(tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_v_n_scalar_out(gc_tensor out, gc_tensor x, scalar n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_v_out(out_local, tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_v_out(gc_tensor out, gc_tensor x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_v_out(out_local, tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_v_x_scalar(scalar x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_v(*x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_v_x_scalar_out(gc_tensor out, scalar x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_v_out(out_local, *x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_w(gc_tensor x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_w(tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_w_n_scalar(gc_tensor x, scalar n) {
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_w(tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_w_n_scalar_out(gc_tensor out, gc_tensor x, scalar n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_w_out(out_local, tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_w_out(gc_tensor out, gc_tensor x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_w_out(out_local, tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_w_x_scalar(scalar x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_w(*x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_chebyshev_polynomial_w_x_scalar_out(gc_tensor out, scalar x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_chebyshev_polynomial_w_out(out_local, *x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_digamma(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_digamma(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_digamma_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_digamma_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_entr(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_entr(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_entr_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_entr_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_erf(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_erf(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_erf_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_erf_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_erfc(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_erfc(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_erfc_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_erfc_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_erfcx(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_erfcx(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_erfcx_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_erfcx_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_erfinv(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_erfinv(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_erfinv_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_erfinv_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_exp2(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_exp2(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_exp2_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_exp2_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_expit(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_expit(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_expit_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_expit_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_expm1(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_expm1(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_expm1_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_expm1_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_gammainc(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::special_gammainc(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_gammainc_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_gammainc_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_gammaincc(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::special_gammaincc(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_gammaincc_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_gammaincc_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_gammaln(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_gammaln(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_gammaln_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_gammaln_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_hermite_polynomial_h(gc_tensor x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_hermite_polynomial_h(tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_hermite_polynomial_h_n_scalar(gc_tensor x, scalar n) {
  PROTECT(
    torch::Tensor results__ = torch::special_hermite_polynomial_h(tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_hermite_polynomial_h_n_scalar_out(gc_tensor out, gc_tensor x, scalar n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_hermite_polynomial_h_out(out_local, tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_hermite_polynomial_h_out(gc_tensor out, gc_tensor x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_hermite_polynomial_h_out(out_local, tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_hermite_polynomial_h_x_scalar(scalar x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_hermite_polynomial_h(*x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_hermite_polynomial_h_x_scalar_out(gc_tensor out, scalar x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_hermite_polynomial_h_out(out_local, *x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_hermite_polynomial_he(gc_tensor x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_hermite_polynomial_he(tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_hermite_polynomial_he_n_scalar(gc_tensor x, scalar n) {
  PROTECT(
    torch::Tensor results__ = torch::special_hermite_polynomial_he(tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_hermite_polynomial_he_n_scalar_out(gc_tensor out, gc_tensor x, scalar n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_hermite_polynomial_he_out(out_local, tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_hermite_polynomial_he_out(gc_tensor out, gc_tensor x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_hermite_polynomial_he_out(out_local, tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_hermite_polynomial_he_x_scalar(scalar x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_hermite_polynomial_he(*x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_hermite_polynomial_he_x_scalar_out(gc_tensor out, scalar x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_hermite_polynomial_he_out(out_local, *x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_i0(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_i0(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_i0_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_i0_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_i0e(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_i0e(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_i0e_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_i0e_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_i1(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_i1(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_i1_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_i1_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_i1e(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_i1e(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_i1e_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_i1e_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_laguerre_polynomial_l(gc_tensor x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_laguerre_polynomial_l(tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_laguerre_polynomial_l_n_scalar(gc_tensor x, scalar n) {
  PROTECT(
    torch::Tensor results__ = torch::special_laguerre_polynomial_l(tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_laguerre_polynomial_l_n_scalar_out(gc_tensor out, gc_tensor x, scalar n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_laguerre_polynomial_l_out(out_local, tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_laguerre_polynomial_l_out(gc_tensor out, gc_tensor x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_laguerre_polynomial_l_out(out_local, tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_laguerre_polynomial_l_x_scalar(scalar x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_laguerre_polynomial_l(*x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_laguerre_polynomial_l_x_scalar_out(gc_tensor out, scalar x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_laguerre_polynomial_l_out(out_local, *x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_legendre_polynomial_p(gc_tensor x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_legendre_polynomial_p(tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_legendre_polynomial_p_n_scalar(gc_tensor x, scalar n) {
  PROTECT(
    torch::Tensor results__ = torch::special_legendre_polynomial_p(tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_legendre_polynomial_p_n_scalar_out(gc_tensor out, gc_tensor x, scalar n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_legendre_polynomial_p_out(out_local, tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_legendre_polynomial_p_out(gc_tensor out, gc_tensor x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_legendre_polynomial_p_out(out_local, tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_legendre_polynomial_p_x_scalar(scalar x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_legendre_polynomial_p(*x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_legendre_polynomial_p_x_scalar_out(gc_tensor out, scalar x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_legendre_polynomial_p_out(out_local, *x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_log1p(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_log1p(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_log1p_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_log1p_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_log_ndtr(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_log_ndtr(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_log_ndtr_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_log_ndtr_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_log_softmax(gc_tensor self, int64_t dim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::special_log_softmax(tensor_from_ocaml(self), dim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_logit(gc_tensor self, double eps_v, int eps_null) {
  PROTECT(
    torch::Tensor results__ = torch::special_logit(tensor_from_ocaml(self), eps_null ? c10::nullopt : c10::optional<double>(eps_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_logit_out(gc_tensor out, gc_tensor self, double eps_v, int eps_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_logit_out(out_local, tensor_from_ocaml(self), eps_null ? c10::nullopt : c10::optional<double>(eps_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_logsumexp(gc_tensor self, int64_t *dim_data, int dim_len, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::special_logsumexp(tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_logsumexp_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_logsumexp_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_modified_bessel_i0(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_modified_bessel_i0(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_modified_bessel_i0_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_modified_bessel_i0_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_modified_bessel_i1(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_modified_bessel_i1(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_modified_bessel_i1_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_modified_bessel_i1_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_modified_bessel_k0(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_modified_bessel_k0(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_modified_bessel_k0_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_modified_bessel_k0_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_modified_bessel_k1(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_modified_bessel_k1(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_modified_bessel_k1_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_modified_bessel_k1_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_multigammaln(gc_tensor self, int64_t p) {
  PROTECT(
    torch::Tensor results__ = torch::special_multigammaln(tensor_from_ocaml(self), p);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_multigammaln_out(gc_tensor out, gc_tensor self, int64_t p) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_multigammaln_out(out_local, tensor_from_ocaml(self), p);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_ndtr(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_ndtr(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_ndtr_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_ndtr_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_ndtri(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_ndtri(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_ndtri_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_ndtri_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_polygamma(int64_t n, gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_polygamma(n, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_polygamma_out(gc_tensor out, int64_t n, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_polygamma_out(out_local, n, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_psi(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_psi(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_psi_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_psi_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_round(gc_tensor self, int64_t decimals) {
  PROTECT(
    torch::Tensor results__ = torch::special_round(tensor_from_ocaml(self), decimals);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_round_out(gc_tensor out, gc_tensor self, int64_t decimals) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_round_out(out_local, tensor_from_ocaml(self), decimals);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_scaled_modified_bessel_k0(gc_tensor x) {
  PROTECT(
    torch::Tensor results__ = torch::special_scaled_modified_bessel_k0(tensor_from_ocaml(x));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_scaled_modified_bessel_k0_out(gc_tensor out, gc_tensor x) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_scaled_modified_bessel_k0_out(out_local, tensor_from_ocaml(x));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_scaled_modified_bessel_k1(gc_tensor x) {
  PROTECT(
    torch::Tensor results__ = torch::special_scaled_modified_bessel_k1(tensor_from_ocaml(x));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_scaled_modified_bessel_k1_out(gc_tensor out, gc_tensor x) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_scaled_modified_bessel_k1_out(out_local, tensor_from_ocaml(x));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_t(gc_tensor x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_t(tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_t_n_scalar(gc_tensor x, scalar n) {
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_t(tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_t_n_scalar_out(gc_tensor out, gc_tensor x, scalar n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_t_out(out_local, tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_t_out(gc_tensor out, gc_tensor x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_t_out(out_local, tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_t_x_scalar(scalar x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_t(*x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_t_x_scalar_out(gc_tensor out, scalar x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_t_out(out_local, *x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_u(gc_tensor x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_u(tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_u_n_scalar(gc_tensor x, scalar n) {
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_u(tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_u_n_scalar_out(gc_tensor out, gc_tensor x, scalar n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_u_out(out_local, tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_u_out(gc_tensor out, gc_tensor x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_u_out(out_local, tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_u_x_scalar(scalar x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_u(*x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_u_x_scalar_out(gc_tensor out, scalar x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_u_out(out_local, *x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_v(gc_tensor x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_v(tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_v_n_scalar(gc_tensor x, scalar n) {
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_v(tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_v_n_scalar_out(gc_tensor out, gc_tensor x, scalar n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_v_out(out_local, tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_v_out(gc_tensor out, gc_tensor x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_v_out(out_local, tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_v_x_scalar(scalar x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_v(*x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_v_x_scalar_out(gc_tensor out, scalar x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_v_out(out_local, *x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_w(gc_tensor x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_w(tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_w_n_scalar(gc_tensor x, scalar n) {
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_w(tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_w_n_scalar_out(gc_tensor out, gc_tensor x, scalar n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_w_out(out_local, tensor_from_ocaml(x), *n);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_w_out(gc_tensor out, gc_tensor x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_w_out(out_local, tensor_from_ocaml(x), tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_w_x_scalar(scalar x, gc_tensor n) {
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_w(*x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_shifted_chebyshev_polynomial_w_x_scalar_out(gc_tensor out, scalar x, gc_tensor n) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_shifted_chebyshev_polynomial_w_out(out_local, *x, tensor_from_ocaml(n));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_sinc(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::special_sinc(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_sinc_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_sinc_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_softmax(gc_tensor self, int64_t dim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::special_softmax(tensor_from_ocaml(self), dim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_spherical_bessel_j0(gc_tensor x) {
  PROTECT(
    torch::Tensor results__ = torch::special_spherical_bessel_j0(tensor_from_ocaml(x));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_spherical_bessel_j0_out(gc_tensor out, gc_tensor x) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_spherical_bessel_j0_out(out_local, tensor_from_ocaml(x));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_xlog1py(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::special_xlog1py(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_xlog1py_other_scalar(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::special_xlog1py(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_xlog1py_other_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_xlog1py_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_xlog1py_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_xlog1py_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_xlog1py_self_scalar(scalar self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::special_xlog1py(*self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_xlog1py_self_scalar_out(gc_tensor out, scalar self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_xlog1py_out(out_local, *self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_xlogy(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::special_xlogy(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_xlogy_other_scalar(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::special_xlogy(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_xlogy_other_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_xlogy_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_xlogy_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_xlogy_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_xlogy_self_scalar(scalar self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::special_xlogy(*self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_xlogy_self_scalar_out(gc_tensor out, scalar self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_xlogy_out(out_local, *self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_zeta(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::special_zeta(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_zeta_other_scalar(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::special_zeta(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_zeta_other_scalar_out(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_zeta_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_zeta_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_zeta_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_zeta_self_scalar(scalar self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::special_zeta(*self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_special_zeta_self_scalar_out(gc_tensor out, scalar self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::special_zeta_out(out_local, *self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg_split(gc_tensor self, int64_t split_size, int64_t dim) {
  PROTECT(
    auto results__ = torch::split(tensor_from_ocaml(self), split_size, dim);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor *atg_split_copy(gc_tensor self, int64_t split_size, int64_t dim) {
  PROTECT(
    auto results__ = torch::split_copy(tensor_from_ocaml(self), split_size, dim);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

void atg_split_copy_tensor_out(gc_tensor *out_data, int out_len, gc_tensor self, int64_t split_size, int64_t dim) {
  PROTECT(
    torch::split_copy_out(of_carray_tensor(out_data, out_len), tensor_from_ocaml(self), split_size, dim);
  )
}

raw_tensor *atg_split_sizes(gc_tensor self, int64_t *split_size_data, int split_size_len, int64_t dim) {
  PROTECT(
    auto results__ = torch::split(tensor_from_ocaml(self), torch::IntArrayRef(split_size_data, split_size_len), dim);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor *atg_split_with_sizes(gc_tensor self, int64_t *split_sizes_data, int split_sizes_len, int64_t dim) {
  PROTECT(
    auto results__ = torch::split_with_sizes(tensor_from_ocaml(self), torch::IntArrayRef(split_sizes_data, split_sizes_len), dim);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor *atg_split_with_sizes_copy(gc_tensor self, int64_t *split_sizes_data, int split_sizes_len, int64_t dim) {
  PROTECT(
    auto results__ = torch::split_with_sizes_copy(tensor_from_ocaml(self), torch::IntArrayRef(split_sizes_data, split_sizes_len), dim);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

void atg_split_with_sizes_copy_out(gc_tensor *out_data, int out_len, gc_tensor self, int64_t *split_sizes_data, int split_sizes_len, int64_t dim) {
  PROTECT(
    torch::split_with_sizes_copy_out(of_carray_tensor(out_data, out_len), tensor_from_ocaml(self), torch::IntArrayRef(split_sizes_data, split_sizes_len), dim);
  )
}

raw_tensor atg_sqrt(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::sqrt(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sqrt_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::sqrt_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sqrt_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::sqrt_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_square(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::square(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_square_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::square_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_square_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::square_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_squeeze(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::squeeze(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_squeeze_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).squeeze_();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_squeeze_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::squeeze_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_squeeze_copy_dim(gc_tensor self, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::squeeze_copy(tensor_from_ocaml(self), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_squeeze_copy_dim_out(gc_tensor out, gc_tensor self, int64_t dim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::squeeze_copy_out(out_local, tensor_from_ocaml(self), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_squeeze_copy_dims(gc_tensor self, int64_t *dim_data, int dim_len) {
  PROTECT(
    torch::Tensor results__ = torch::squeeze_copy(tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_squeeze_copy_dims_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::squeeze_copy_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_squeeze_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::squeeze_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_squeeze_dim(gc_tensor self, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::squeeze(tensor_from_ocaml(self), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_squeeze_dim_(gc_tensor self, int64_t dim) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).squeeze_(dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_squeeze_dims(gc_tensor self, int64_t *dim_data, int dim_len) {
  PROTECT(
    torch::Tensor results__ = torch::squeeze(tensor_from_ocaml(self), torch::IntArrayRef(dim_data, dim_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_squeeze_dims_(gc_tensor self, int64_t *dim_data, int dim_len) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).squeeze_(torch::IntArrayRef(dim_data, dim_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sspaddmm(gc_tensor self, gc_tensor mat1, gc_tensor mat2, scalar beta, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::sspaddmm(tensor_from_ocaml(self), tensor_from_ocaml(mat1), tensor_from_ocaml(mat2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sspaddmm_out(gc_tensor out, gc_tensor self, gc_tensor mat1, gc_tensor mat2, scalar beta, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::sspaddmm_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(mat1), tensor_from_ocaml(mat2), beta ? *beta : c10::Scalar{1} , alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_stack(gc_tensor *tensors_data, int tensors_len, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::stack(of_carray_tensor(tensors_data, tensors_len), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_stack_out(gc_tensor out, gc_tensor *tensors_data, int tensors_len, int64_t dim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::stack_out(out_local, of_carray_tensor(tensors_data, tensors_len), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_std(gc_tensor self, int unbiased) {
  PROTECT(
    torch::Tensor results__ = torch::std(tensor_from_ocaml(self), (bool)unbiased);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_std_correction(gc_tensor self, int64_t *dim_data, int dim_len, scalar correction, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::std(tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), *correction, (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_std_correction_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, scalar correction, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::std_out(out_local, tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), *correction, (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_std_dim(gc_tensor self, int64_t *dim_data, int dim_len, int unbiased, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::std(tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)unbiased, (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

void atg_std_mean(raw_tensor *out__, gc_tensor self, int unbiased) {
  PROTECT(
    auto results__ = torch::std_mean(tensor_from_ocaml(self), (bool)unbiased);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_std_mean_correction(raw_tensor *out__, gc_tensor self, int64_t *dim_data, int dim_len, scalar correction, int keepdim) {
  PROTECT(
    auto results__ = torch::std_mean(tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), *correction, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_std_mean_correction_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor self, int64_t *dim_data, int dim_len, scalar correction, int keepdim) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::std_mean_out(out0_local, out1_local, tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), *correction, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_std_mean_dim(raw_tensor *out__, gc_tensor self, int64_t *dim_data, int dim_len, int unbiased, int keepdim) {
  PROTECT(
    auto results__ = torch::std_mean(tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)unbiased, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_std_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int unbiased, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::std_out(out_local, tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)unbiased, (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_stft(gc_tensor self, int64_t n_fft, int64_t hop_length_v, int hop_length_null, int64_t win_length_v, int win_length_null, gc_tensor window, int normalized, int onesided, int return_complex, int align_to_window) {
  PROTECT(
    torch::Tensor results__ = torch::stft(tensor_from_ocaml(self), n_fft, hop_length_null ? c10::nullopt : c10::optional<int64_t>(hop_length_v), win_length_null ? c10::nullopt : c10::optional<int64_t>(win_length_v), window ? std::make_optional(tensor_from_ocaml(window)) : std::nullopt, (bool)normalized, (bool)onesided, (bool)return_complex, (bool)align_to_window);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_stft_center(gc_tensor self, int64_t n_fft, int64_t hop_length_v, int hop_length_null, int64_t win_length_v, int win_length_null, gc_tensor window, int center, char * pad_mode, int normalized, int onesided, int return_complex, int align_to_window) {
  PROTECT(
    torch::Tensor results__ = torch::stft(tensor_from_ocaml(self), n_fft, hop_length_null ? c10::nullopt : c10::optional<int64_t>(hop_length_v), win_length_null ? c10::nullopt : c10::optional<int64_t>(win_length_v), window ? std::make_optional(tensor_from_ocaml(window)) : std::nullopt, (bool)center, std::string(pad_mode), (bool)normalized, (bool)onesided, (bool)return_complex, (bool)align_to_window);
    return tensor_to_ocaml(results__);
  )
}

int64_t atg_stride(gc_tensor self, int64_t dim) {
  PROTECT(
    return torch::stride(tensor_from_ocaml(self), dim);
  )
  return 0;
}

raw_tensor atg_sub(gc_tensor self, gc_tensor other, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::sub(tensor_from_ocaml(self), tensor_from_ocaml(other), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sub_(gc_tensor self, gc_tensor other, scalar alpha) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).sub_(tensor_from_ocaml(other), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sub_out(gc_tensor out, gc_tensor self, gc_tensor other, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::sub_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sub_scalar(gc_tensor self, scalar other, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::sub(tensor_from_ocaml(self), *other, alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sub_scalar_(gc_tensor self, scalar other, scalar alpha) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).sub_(*other, alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sub_scalar_out(gc_tensor out, gc_tensor self, scalar other, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::sub_out(out_local, tensor_from_ocaml(self), *other, alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_subtract(gc_tensor self, gc_tensor other, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::subtract(tensor_from_ocaml(self), tensor_from_ocaml(other), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_subtract_(gc_tensor self, gc_tensor other, scalar alpha) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).subtract_(tensor_from_ocaml(other), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_subtract_out(gc_tensor out, gc_tensor self, gc_tensor other, scalar alpha) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::subtract_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other), alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_subtract_scalar(gc_tensor self, scalar other, scalar alpha) {
  PROTECT(
    torch::Tensor results__ = torch::subtract(tensor_from_ocaml(self), *other, alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_subtract_scalar_(gc_tensor self, scalar other, scalar alpha) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).subtract_(*other, alpha ? *alpha : c10::Scalar{1} );
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sum(gc_tensor self, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::sum(tensor_from_ocaml(self), torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sum_dim_intlist(gc_tensor self, int64_t *dim_data, int dim_len, int keepdim, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::sum(tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sum_intlist_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int keepdim, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::sum_out(out_local, tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)keepdim, torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sum_out(gc_tensor out, gc_tensor self, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::sum_out(out_local, tensor_from_ocaml(self), torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_sum_to_size(gc_tensor self, int64_t *size_data, int size_len) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).sum_to_size(torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

void atg_svd(raw_tensor *out__, gc_tensor self, int some, int compute_uv) {
  PROTECT(
    auto results__ = torch::svd(tensor_from_ocaml(self), (bool)some, (bool)compute_uv);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_svd_u(raw_tensor *out__, gc_tensor U, gc_tensor S, gc_tensor V, gc_tensor self, int some, int compute_uv) {
  torch::Tensor U_local = tensor_from_ocaml(U);
  torch::Tensor S_local = tensor_from_ocaml(S);
  torch::Tensor V_local = tensor_from_ocaml(V);
  PROTECT(
    auto results__ = torch::svd_out(U_local, S_local, V_local, tensor_from_ocaml(self), (bool)some, (bool)compute_uv);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

raw_tensor atg_swapaxes(gc_tensor self, int64_t axis0, int64_t axis1) {
  PROTECT(
    torch::Tensor results__ = torch::swapaxes(tensor_from_ocaml(self), axis0, axis1);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_swapaxes_(gc_tensor self, int64_t axis0, int64_t axis1) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).swapaxes_(axis0, axis1);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_swapdims(gc_tensor self, int64_t dim0, int64_t dim1) {
  PROTECT(
    torch::Tensor results__ = torch::swapdims(tensor_from_ocaml(self), dim0, dim1);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_swapdims_(gc_tensor self, int64_t dim0, int64_t dim1) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).swapdims_(dim0, dim1);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_t(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::t(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_t_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).t_();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_t_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::t_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_t_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::t_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_take(gc_tensor self, gc_tensor index) {
  PROTECT(
    torch::Tensor results__ = torch::take(tensor_from_ocaml(self), tensor_from_ocaml(index));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_take_along_dim(gc_tensor self, gc_tensor indices, int64_t dim_v, int dim_null) {
  PROTECT(
    torch::Tensor results__ = torch::take_along_dim(tensor_from_ocaml(self), tensor_from_ocaml(indices), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_take_along_dim_out(gc_tensor out, gc_tensor self, gc_tensor indices, int64_t dim_v, int dim_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::take_along_dim_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(indices), dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_take_out(gc_tensor out, gc_tensor self, gc_tensor index) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::take_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(index));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_tan(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::tan(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_tan_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::tan_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_tan_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::tan_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_tanh(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::tanh(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_tanh_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::tanh_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_tanh_backward(gc_tensor grad_output, gc_tensor output) {
  PROTECT(
    torch::Tensor results__ = torch::tanh_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(output));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_tanh_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor output) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::tanh_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(output));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_tanh_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::tanh_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg_tensor_split(gc_tensor self, int64_t sections, int64_t dim) {
  PROTECT(
    auto results__ = torch::tensor_split(tensor_from_ocaml(self), sections, dim);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor *atg_tensor_split_indices(gc_tensor self, int64_t *indices_data, int indices_len, int64_t dim) {
  PROTECT(
    auto results__ = torch::tensor_split(tensor_from_ocaml(self), torch::IntArrayRef(indices_data, indices_len), dim);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor *atg_tensor_split_tensor_indices_or_sections(gc_tensor self, gc_tensor tensor_indices_or_sections, int64_t dim) {
  PROTECT(
    auto results__ = torch::tensor_split(tensor_from_ocaml(self), tensor_from_ocaml(tensor_indices_or_sections), dim);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor atg_tensordot(gc_tensor self, gc_tensor other, int64_t *dims_self_data, int dims_self_len, int64_t *dims_other_data, int dims_other_len) {
  PROTECT(
    torch::Tensor results__ = torch::tensordot(tensor_from_ocaml(self), tensor_from_ocaml(other), torch::IntArrayRef(dims_self_data, dims_self_len), torch::IntArrayRef(dims_other_data, dims_other_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_tensordot_out(gc_tensor out, gc_tensor self, gc_tensor other, int64_t *dims_self_data, int dims_self_len, int64_t *dims_other_data, int dims_other_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::tensordot_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other), torch::IntArrayRef(dims_self_data, dims_self_len), torch::IntArrayRef(dims_other_data, dims_other_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_threshold(gc_tensor self, scalar threshold, scalar value) {
  PROTECT(
    torch::Tensor results__ = torch::threshold(tensor_from_ocaml(self), *threshold, *value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_threshold_(gc_tensor self, scalar threshold, scalar value) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::threshold_(self_local, *threshold, *value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_threshold_backward(gc_tensor grad_output, gc_tensor self, scalar threshold) {
  PROTECT(
    torch::Tensor results__ = torch::threshold_backward(tensor_from_ocaml(grad_output), tensor_from_ocaml(self), *threshold);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_threshold_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, gc_tensor self, scalar threshold) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::threshold_backward_out(grad_input_local, tensor_from_ocaml(grad_output), tensor_from_ocaml(self), *threshold);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_threshold_out(gc_tensor out, gc_tensor self, scalar threshold, scalar value) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::threshold_out(out_local, tensor_from_ocaml(self), *threshold, *value);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_tile(gc_tensor self, int64_t *dims_data, int dims_len) {
  PROTECT(
    torch::Tensor results__ = torch::tile(tensor_from_ocaml(self), torch::IntArrayRef(dims_data, dims_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_to(gc_tensor self, int device) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).to(device_of_int(device));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_to_dense(gc_tensor self, int dtype, int masked_grad) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).to_dense(torch::ScalarType(dtype), (bool)masked_grad);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_to_dense_backward(gc_tensor grad, gc_tensor input, int masked_grad) {
  PROTECT(
    torch::Tensor results__ = torch::to_dense_backward(tensor_from_ocaml(grad), tensor_from_ocaml(input), (bool)masked_grad);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_to_device(gc_tensor self, int device, int dtype, int non_blocking, int copy) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).to(device_of_int(device), torch::ScalarType(dtype), (bool)non_blocking, (bool)copy);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_to_dtype(gc_tensor self, int dtype, int non_blocking, int copy) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).to(torch::ScalarType(dtype), (bool)non_blocking, (bool)copy);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_to_dtype_layout(gc_tensor self, int options_kind, int options_device, int non_blocking, int copy) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).to(at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)), (bool)non_blocking, (bool)copy);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_to_mkldnn(gc_tensor self, int dtype) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).to_mkldnn(torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_to_mkldnn_backward(gc_tensor grad, gc_tensor input) {
  PROTECT(
    torch::Tensor results__ = torch::to_mkldnn_backward(tensor_from_ocaml(grad), tensor_from_ocaml(input));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_to_mkldnn_out(gc_tensor out, gc_tensor self, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::to_mkldnn_out(out_local, tensor_from_ocaml(self), torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_to_other(gc_tensor self, gc_tensor other, int non_blocking, int copy) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).to(tensor_from_ocaml(other), (bool)non_blocking, (bool)copy);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_to_padded_tensor(gc_tensor self, double padding, int64_t *output_size_data, int output_size_len) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).to_padded_tensor(padding, output_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(output_size_data, output_size_len)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_to_padded_tensor_out(gc_tensor out, gc_tensor self, double padding, int64_t *output_size_data, int output_size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::to_padded_tensor_out(out_local, tensor_from_ocaml(self), padding, output_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(output_size_data, output_size_len)));
    return tensor_to_ocaml(results__);
  )
}

void atg_topk(raw_tensor *out__, gc_tensor self, int64_t k, int64_t dim, int largest, int sorted) {
  PROTECT(
    auto results__ = torch::topk(tensor_from_ocaml(self), k, dim, (bool)largest, (bool)sorted);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_topk_values(raw_tensor *out__, gc_tensor values, gc_tensor indices, gc_tensor self, int64_t k, int64_t dim, int largest, int sorted) {
  torch::Tensor values_local = tensor_from_ocaml(values);
  torch::Tensor indices_local = tensor_from_ocaml(indices);
  PROTECT(
    auto results__ = torch::topk_out(values_local, indices_local, tensor_from_ocaml(self), k, dim, (bool)largest, (bool)sorted);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_totype(gc_tensor self, int scalar_type) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).toType(torch::ScalarType(scalar_type));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_trace(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::trace(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_trace_backward(gc_tensor grad, int64_t *sizes_data, int sizes_len) {
  PROTECT(
    torch::Tensor results__ = torch::trace_backward(tensor_from_ocaml(grad), torch::IntArrayRef(sizes_data, sizes_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_trace_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::trace_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_transpose(gc_tensor self, int64_t dim0, int64_t dim1) {
  PROTECT(
    torch::Tensor results__ = torch::transpose(tensor_from_ocaml(self), dim0, dim1);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_transpose_(gc_tensor self, int64_t dim0, int64_t dim1) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).transpose_(dim0, dim1);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_transpose_copy(gc_tensor self, int64_t dim0, int64_t dim1) {
  PROTECT(
    torch::Tensor results__ = torch::transpose_copy(tensor_from_ocaml(self), dim0, dim1);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_transpose_copy_int_out(gc_tensor out, gc_tensor self, int64_t dim0, int64_t dim1) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::transpose_copy_out(out_local, tensor_from_ocaml(self), dim0, dim1);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_trapezoid(gc_tensor y, gc_tensor x, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::trapezoid(tensor_from_ocaml(y), tensor_from_ocaml(x), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_trapezoid_dx(gc_tensor y, scalar dx, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::trapezoid(tensor_from_ocaml(y), dx ? *dx : c10::Scalar{1} , dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_trapz(gc_tensor y, gc_tensor x, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::trapz(tensor_from_ocaml(y), tensor_from_ocaml(x), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_trapz_dx(gc_tensor y, double dx, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::trapz(tensor_from_ocaml(y), dx, dim);
    return tensor_to_ocaml(results__);
  )
}

void atg_triangular_solve(raw_tensor *out__, gc_tensor self, gc_tensor A, int upper, int transpose, int unitriangular) {
  PROTECT(
    auto results__ = torch::triangular_solve(tensor_from_ocaml(self), tensor_from_ocaml(A), (bool)upper, (bool)transpose, (bool)unitriangular);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_triangular_solve_x(raw_tensor *out__, gc_tensor X, gc_tensor M, gc_tensor self, gc_tensor A, int upper, int transpose, int unitriangular) {
  torch::Tensor X_local = tensor_from_ocaml(X);
  torch::Tensor M_local = tensor_from_ocaml(M);
  PROTECT(
    auto results__ = torch::triangular_solve_out(X_local, M_local, tensor_from_ocaml(self), tensor_from_ocaml(A), (bool)upper, (bool)transpose, (bool)unitriangular);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_tril(gc_tensor self, int64_t diagonal) {
  PROTECT(
    torch::Tensor results__ = torch::tril(tensor_from_ocaml(self), diagonal);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_tril_(gc_tensor self, int64_t diagonal) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).tril_(diagonal);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_tril_indices(int64_t row, int64_t col, int64_t offset, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::tril_indices(row, col, offset, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_tril_indices_out(gc_tensor out, int64_t row, int64_t col, int64_t offset) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::tril_indices_out(out_local, row, col, offset);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_tril_out(gc_tensor out, gc_tensor self, int64_t diagonal) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::tril_out(out_local, tensor_from_ocaml(self), diagonal);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_triplet_margin_loss(gc_tensor anchor, gc_tensor positive, gc_tensor negative, double margin, double p, double eps, int swap, int64_t reduction) {
  PROTECT(
    torch::Tensor results__ = torch::triplet_margin_loss(tensor_from_ocaml(anchor), tensor_from_ocaml(positive), tensor_from_ocaml(negative), margin, p, eps, (bool)swap, reduction);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_triu(gc_tensor self, int64_t diagonal) {
  PROTECT(
    torch::Tensor results__ = torch::triu(tensor_from_ocaml(self), diagonal);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_triu_(gc_tensor self, int64_t diagonal) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).triu_(diagonal);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_triu_indices(int64_t row, int64_t col, int64_t offset, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::triu_indices(row, col, offset, at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_triu_indices_out(gc_tensor out, int64_t row, int64_t col, int64_t offset) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::triu_indices_out(out_local, row, col, offset);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_triu_out(gc_tensor out, gc_tensor self, int64_t diagonal) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::triu_out(out_local, tensor_from_ocaml(self), diagonal);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_true_divide(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::true_divide(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_true_divide_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).true_divide_(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_true_divide_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::true_divide_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_true_divide_scalar(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::true_divide(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_true_divide_scalar_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).true_divide_(*other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_trunc(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::trunc(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_trunc_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::trunc_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_trunc_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::trunc_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_type_as(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).type_as(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg_unbind(gc_tensor self, int64_t dim) {
  PROTECT(
    auto results__ = torch::unbind(tensor_from_ocaml(self), dim);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor *atg_unbind_copy(gc_tensor self, int64_t dim) {
  PROTECT(
    auto results__ = torch::unbind_copy(tensor_from_ocaml(self), dim);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

void atg_unbind_copy_int_out(gc_tensor *out_data, int out_len, gc_tensor self, int64_t dim) {
  PROTECT(
    torch::unbind_copy_out(of_carray_tensor(out_data, out_len), tensor_from_ocaml(self), dim);
  )
}

raw_tensor atg_unflatten(gc_tensor self, int64_t dim, int64_t *sizes_data, int sizes_len) {
  PROTECT(
    torch::Tensor results__ = torch::unflatten(tensor_from_ocaml(self), dim, torch::IntArrayRef(sizes_data, sizes_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg_unflatten_dense_tensors(gc_tensor flat, gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    auto results__ = torch::unflatten_dense_tensors(tensor_from_ocaml(flat), of_carray_tensor(tensors_data, tensors_len));
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor atg_unfold(gc_tensor self, int64_t dimension, int64_t size, int64_t step) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).unfold(dimension, size, step);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_unfold_backward(gc_tensor grad_in, int64_t *input_sizes_data, int input_sizes_len, int64_t dim, int64_t size, int64_t step) {
  PROTECT(
    torch::Tensor results__ = torch::unfold_backward(tensor_from_ocaml(grad_in), torch::IntArrayRef(input_sizes_data, input_sizes_len), dim, size, step);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_unfold_backward_out(gc_tensor out, gc_tensor grad_in, int64_t *input_sizes_data, int input_sizes_len, int64_t dim, int64_t size, int64_t step) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::unfold_backward_out(out_local, tensor_from_ocaml(grad_in), torch::IntArrayRef(input_sizes_data, input_sizes_len), dim, size, step);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_unfold_copy(gc_tensor self, int64_t dimension, int64_t size, int64_t step) {
  PROTECT(
    torch::Tensor results__ = torch::unfold_copy(tensor_from_ocaml(self), dimension, size, step);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_unfold_copy_out(gc_tensor out, gc_tensor self, int64_t dimension, int64_t size, int64_t step) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::unfold_copy_out(out_local, tensor_from_ocaml(self), dimension, size, step);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_uniform(gc_tensor self, double from, double to) {
  PROTECT(
    torch::Tensor results__ = torch::uniform(tensor_from_ocaml(self), from, to);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_uniform_(gc_tensor self, double from, double to) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).uniform_(from, to);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_uniform_out(gc_tensor out, gc_tensor self, double from, double to) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::uniform_out(out_local, tensor_from_ocaml(self), from, to);
    return tensor_to_ocaml(results__);
  )
}

void atg_unique_consecutive(raw_tensor *out__, gc_tensor self, int return_inverse, int return_counts, int64_t dim_v, int dim_null) {
  PROTECT(
    auto results__ = torch::unique_consecutive(tensor_from_ocaml(self), (bool)return_inverse, (bool)return_counts, dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_unique_consecutive_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor self, int return_inverse, int return_counts, int64_t dim_v, int dim_null) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  PROTECT(
    auto results__ = torch::unique_consecutive_out(out0_local, out1_local, out2_local, tensor_from_ocaml(self), (bool)return_inverse, (bool)return_counts, dim_null ? c10::nullopt : c10::optional<int64_t>(dim_v));
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_unique_dim(raw_tensor *out__, gc_tensor self, int64_t dim, int sorted, int return_inverse, int return_counts) {
  PROTECT(
    auto results__ = torch::unique_dim(tensor_from_ocaml(self), dim, (bool)sorted, (bool)return_inverse, (bool)return_counts);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_unique_dim_consecutive(raw_tensor *out__, gc_tensor self, int64_t dim, int return_inverse, int return_counts) {
  PROTECT(
    auto results__ = torch::unique_dim_consecutive(tensor_from_ocaml(self), dim, (bool)return_inverse, (bool)return_counts);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_unique_dim_consecutive_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor self, int64_t dim, int return_inverse, int return_counts) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  PROTECT(
    auto results__ = torch::unique_dim_consecutive_out(out0_local, out1_local, out2_local, tensor_from_ocaml(self), dim, (bool)return_inverse, (bool)return_counts);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

void atg_unique_dim_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor out2, gc_tensor self, int64_t dim, int sorted, int return_inverse, int return_counts) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  torch::Tensor out2_local = tensor_from_ocaml(out2);
  PROTECT(
    auto results__ = torch::unique_dim_out(out0_local, out1_local, out2_local, tensor_from_ocaml(self), dim, (bool)sorted, (bool)return_inverse, (bool)return_counts);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
    out__[2] = tensor_to_ocaml(std::get<2>(results__));
  )
}

raw_tensor *atg_unsafe_chunk(gc_tensor self, int64_t chunks, int64_t dim) {
  PROTECT(
    auto results__ = torch::unsafe_chunk(tensor_from_ocaml(self), chunks, dim);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor *atg_unsafe_split(gc_tensor self, int64_t split_size, int64_t dim) {
  PROTECT(
    auto results__ = torch::unsafe_split(tensor_from_ocaml(self), split_size, dim);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

void atg_unsafe_split_tensor_out(gc_tensor *out_data, int out_len, gc_tensor self, int64_t split_size, int64_t dim) {
  PROTECT(
    torch::unsafe_split_out(of_carray_tensor(out_data, out_len), tensor_from_ocaml(self), split_size, dim);
  )
}

raw_tensor *atg_unsafe_split_with_sizes(gc_tensor self, int64_t *split_sizes_data, int split_sizes_len, int64_t dim) {
  PROTECT(
    auto results__ = torch::unsafe_split_with_sizes(tensor_from_ocaml(self), torch::IntArrayRef(split_sizes_data, split_sizes_len), dim);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

void atg_unsafe_split_with_sizes_out(gc_tensor *out_data, int out_len, gc_tensor self, int64_t *split_sizes_data, int split_sizes_len, int64_t dim) {
  PROTECT(
    torch::unsafe_split_with_sizes_out(of_carray_tensor(out_data, out_len), tensor_from_ocaml(self), torch::IntArrayRef(split_sizes_data, split_sizes_len), dim);
  )
}

raw_tensor atg_unsqueeze(gc_tensor self, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::unsqueeze(tensor_from_ocaml(self), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_unsqueeze_(gc_tensor self, int64_t dim) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).unsqueeze_(dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_unsqueeze_copy(gc_tensor self, int64_t dim) {
  PROTECT(
    torch::Tensor results__ = torch::unsqueeze_copy(tensor_from_ocaml(self), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_unsqueeze_copy_out(gc_tensor out, gc_tensor self, int64_t dim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::unsqueeze_copy_out(out_local, tensor_from_ocaml(self), dim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_bicubic2d(gc_tensor self, int64_t *output_size_data, int output_size_len, int align_corners, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_bicubic2d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), (bool)align_corners, scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_bicubic2d_backward(gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, int align_corners, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_bicubic2d_backward(tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), (bool)align_corners, scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_bicubic2d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, int align_corners, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::upsample_bicubic2d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), (bool)align_corners, scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_bicubic2d_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len, int align_corners, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::upsample_bicubic2d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), (bool)align_corners, scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_bicubic2d_vec(gc_tensor input, int64_t *output_size_data, int output_size_len, int align_corners, double *scale_factors_data, int scale_factors_len) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_bicubic2d(tensor_from_ocaml(input), output_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(output_size_data, output_size_len)), (bool)align_corners, at::ArrayRef<double>(scale_factors_data, scale_factors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_bilinear2d(gc_tensor self, int64_t *output_size_data, int output_size_len, int align_corners, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_bilinear2d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), (bool)align_corners, scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_bilinear2d_backward(gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, int align_corners, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_bilinear2d_backward(tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), (bool)align_corners, scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_bilinear2d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, int align_corners, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::upsample_bilinear2d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), (bool)align_corners, scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_bilinear2d_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len, int align_corners, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::upsample_bilinear2d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), (bool)align_corners, scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_bilinear2d_vec(gc_tensor input, int64_t *output_size_data, int output_size_len, int align_corners, double *scale_factors_data, int scale_factors_len) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_bilinear2d(tensor_from_ocaml(input), output_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(output_size_data, output_size_len)), (bool)align_corners, at::ArrayRef<double>(scale_factors_data, scale_factors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_bilinear2d_vec_out(gc_tensor out, gc_tensor input, int64_t *output_size_data, int output_size_len, int align_corners, double *scale_factors_data, int scale_factors_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::upsample_bilinear2d_out(out_local, tensor_from_ocaml(input), output_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(output_size_data, output_size_len)), (bool)align_corners, at::ArrayRef<double>(scale_factors_data, scale_factors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_linear1d(gc_tensor self, int64_t *output_size_data, int output_size_len, int align_corners, double scales_v, int scales_null) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_linear1d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), (bool)align_corners, scales_null ? c10::nullopt : c10::optional<double>(scales_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_linear1d_backward(gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, int align_corners, double scales_v, int scales_null) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_linear1d_backward(tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), (bool)align_corners, scales_null ? c10::nullopt : c10::optional<double>(scales_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_linear1d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, int align_corners, double scales_v, int scales_null) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::upsample_linear1d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), (bool)align_corners, scales_null ? c10::nullopt : c10::optional<double>(scales_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_linear1d_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len, int align_corners, double scales_v, int scales_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::upsample_linear1d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), (bool)align_corners, scales_null ? c10::nullopt : c10::optional<double>(scales_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_linear1d_vec(gc_tensor input, int64_t *output_size_data, int output_size_len, int align_corners, double *scale_factors_data, int scale_factors_len) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_linear1d(tensor_from_ocaml(input), output_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(output_size_data, output_size_len)), (bool)align_corners, at::ArrayRef<double>(scale_factors_data, scale_factors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_nearest1d(gc_tensor self, int64_t *output_size_data, int output_size_len, double scales_v, int scales_null) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_nearest1d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), scales_null ? c10::nullopt : c10::optional<double>(scales_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_nearest1d_backward(gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, double scales_v, int scales_null) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_nearest1d_backward(tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), scales_null ? c10::nullopt : c10::optional<double>(scales_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_nearest1d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, double scales_v, int scales_null) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::upsample_nearest1d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), scales_null ? c10::nullopt : c10::optional<double>(scales_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_nearest1d_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len, double scales_v, int scales_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::upsample_nearest1d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), scales_null ? c10::nullopt : c10::optional<double>(scales_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_nearest1d_vec(gc_tensor input, int64_t *output_size_data, int output_size_len, double *scale_factors_data, int scale_factors_len) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_nearest1d(tensor_from_ocaml(input), output_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(output_size_data, output_size_len)), at::ArrayRef<double>(scale_factors_data, scale_factors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_nearest2d(gc_tensor self, int64_t *output_size_data, int output_size_len, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_nearest2d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_nearest2d_backward(gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_nearest2d_backward(tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_nearest2d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::upsample_nearest2d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_nearest2d_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::upsample_nearest2d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_nearest2d_vec(gc_tensor input, int64_t *output_size_data, int output_size_len, double *scale_factors_data, int scale_factors_len) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_nearest2d(tensor_from_ocaml(input), output_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(output_size_data, output_size_len)), at::ArrayRef<double>(scale_factors_data, scale_factors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_nearest2d_vec_out(gc_tensor out, gc_tensor input, int64_t *output_size_data, int output_size_len, double *scale_factors_data, int scale_factors_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::upsample_nearest2d_out(out_local, tensor_from_ocaml(input), output_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(output_size_data, output_size_len)), at::ArrayRef<double>(scale_factors_data, scale_factors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_nearest3d(gc_tensor self, int64_t *output_size_data, int output_size_len, double scales_d_v, int scales_d_null, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_nearest3d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), scales_d_null ? c10::nullopt : c10::optional<double>(scales_d_v), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_nearest3d_backward(gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, double scales_d_v, int scales_d_null, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_nearest3d_backward(tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), scales_d_null ? c10::nullopt : c10::optional<double>(scales_d_v), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_nearest3d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, double scales_d_v, int scales_d_null, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::upsample_nearest3d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), scales_d_null ? c10::nullopt : c10::optional<double>(scales_d_v), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_nearest3d_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len, double scales_d_v, int scales_d_null, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::upsample_nearest3d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), scales_d_null ? c10::nullopt : c10::optional<double>(scales_d_v), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_nearest3d_vec(gc_tensor input, int64_t *output_size_data, int output_size_len, double *scale_factors_data, int scale_factors_len) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_nearest3d(tensor_from_ocaml(input), output_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(output_size_data, output_size_len)), at::ArrayRef<double>(scale_factors_data, scale_factors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_trilinear3d(gc_tensor self, int64_t *output_size_data, int output_size_len, int align_corners, double scales_d_v, int scales_d_null, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_trilinear3d(tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), (bool)align_corners, scales_d_null ? c10::nullopt : c10::optional<double>(scales_d_v), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_trilinear3d_backward(gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, int align_corners, double scales_d_v, int scales_d_null, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_trilinear3d_backward(tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), (bool)align_corners, scales_d_null ? c10::nullopt : c10::optional<double>(scales_d_v), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_trilinear3d_backward_grad_input(gc_tensor grad_input, gc_tensor grad_output, int64_t *output_size_data, int output_size_len, int64_t *input_size_data, int input_size_len, int align_corners, double scales_d_v, int scales_d_null, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor grad_input_local = tensor_from_ocaml(grad_input);
  PROTECT(
    torch::Tensor results__ = torch::upsample_trilinear3d_backward_out(grad_input_local, tensor_from_ocaml(grad_output), torch::IntArrayRef(output_size_data, output_size_len), torch::IntArrayRef(input_size_data, input_size_len), (bool)align_corners, scales_d_null ? c10::nullopt : c10::optional<double>(scales_d_v), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_trilinear3d_out(gc_tensor out, gc_tensor self, int64_t *output_size_data, int output_size_len, int align_corners, double scales_d_v, int scales_d_null, double scales_h_v, int scales_h_null, double scales_w_v, int scales_w_null) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::upsample_trilinear3d_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(output_size_data, output_size_len), (bool)align_corners, scales_d_null ? c10::nullopt : c10::optional<double>(scales_d_v), scales_h_null ? c10::nullopt : c10::optional<double>(scales_h_v), scales_w_null ? c10::nullopt : c10::optional<double>(scales_w_v));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_upsample_trilinear3d_vec(gc_tensor input, int64_t *output_size_data, int output_size_len, int align_corners, double *scale_factors_data, int scale_factors_len) {
  PROTECT(
    torch::Tensor results__ = torch::upsample_trilinear3d(tensor_from_ocaml(input), output_size_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(output_size_data, output_size_len)), (bool)align_corners, at::ArrayRef<double>(scale_factors_data, scale_factors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_value_selecting_reduction_backward(gc_tensor grad, int64_t dim, gc_tensor indices, int64_t *sizes_data, int sizes_len, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::value_selecting_reduction_backward(tensor_from_ocaml(grad), dim, tensor_from_ocaml(indices), torch::IntArrayRef(sizes_data, sizes_len), (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_values(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).values();
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_values_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::values_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_values_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::values_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_vander(gc_tensor x, int64_t n_v, int n_null, int increasing) {
  PROTECT(
    torch::Tensor results__ = torch::vander(tensor_from_ocaml(x), n_null ? c10::nullopt : c10::optional<int64_t>(n_v), (bool)increasing);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_var(gc_tensor self, int unbiased) {
  PROTECT(
    torch::Tensor results__ = torch::var(tensor_from_ocaml(self), (bool)unbiased);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_var_correction(gc_tensor self, int64_t *dim_data, int dim_len, scalar correction, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::var(tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), *correction, (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_var_correction_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, scalar correction, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::var_out(out_local, tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), *correction, (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_var_dim(gc_tensor self, int64_t *dim_data, int dim_len, int unbiased, int keepdim) {
  PROTECT(
    torch::Tensor results__ = torch::var(tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)unbiased, (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

void atg_var_mean(raw_tensor *out__, gc_tensor self, int unbiased) {
  PROTECT(
    auto results__ = torch::var_mean(tensor_from_ocaml(self), (bool)unbiased);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_var_mean_correction(raw_tensor *out__, gc_tensor self, int64_t *dim_data, int dim_len, scalar correction, int keepdim) {
  PROTECT(
    auto results__ = torch::var_mean(tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), *correction, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_var_mean_correction_out(raw_tensor *out__, gc_tensor out0, gc_tensor out1, gc_tensor self, int64_t *dim_data, int dim_len, scalar correction, int keepdim) {
  torch::Tensor out0_local = tensor_from_ocaml(out0);
  torch::Tensor out1_local = tensor_from_ocaml(out1);
  PROTECT(
    auto results__ = torch::var_mean_out(out0_local, out1_local, tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), *correction, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

void atg_var_mean_dim(raw_tensor *out__, gc_tensor self, int64_t *dim_data, int dim_len, int unbiased, int keepdim) {
  PROTECT(
    auto results__ = torch::var_mean(tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)unbiased, (bool)keepdim);
    out__[0] = tensor_to_ocaml(std::get<0>(results__));
    out__[1] = tensor_to_ocaml(std::get<1>(results__));
  )
}

raw_tensor atg_var_out(gc_tensor out, gc_tensor self, int64_t *dim_data, int dim_len, int unbiased, int keepdim) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::var_out(out_local, tensor_from_ocaml(self), dim_data == nullptr ? c10::nullopt : c10::optional<torch::IntArrayRef>(torch::IntArrayRef(dim_data, dim_len)), (bool)unbiased, (bool)keepdim);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_vdot(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::vdot(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_vdot_out(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::vdot_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_view(gc_tensor self, int64_t *size_data, int size_len) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).view(torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_view_as(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).view_as(tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_view_as_complex(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::view_as_complex(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_view_as_complex_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::view_as_complex_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_view_as_complex_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::view_as_complex_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_view_as_real(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::view_as_real(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_view_as_real_copy(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::view_as_real_copy(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_view_as_real_copy_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::view_as_real_copy_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_view_copy(gc_tensor self, int64_t *size_data, int size_len) {
  PROTECT(
    torch::Tensor results__ = torch::view_copy(tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_view_copy_dtype(gc_tensor self, int dtype) {
  PROTECT(
    torch::Tensor results__ = torch::view_copy(tensor_from_ocaml(self), torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_view_copy_dtype_out(gc_tensor out, gc_tensor self, int dtype) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::view_copy_out(out_local, tensor_from_ocaml(self), torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_view_copy_out(gc_tensor out, gc_tensor self, int64_t *size_data, int size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::view_copy_out(out_local, tensor_from_ocaml(self), torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_view_dtype(gc_tensor self, int dtype) {
  PROTECT(
    torch::Tensor results__ = tensor_from_ocaml(self).view(torch::ScalarType(dtype));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg_vsplit(gc_tensor self, int64_t sections) {
  PROTECT(
    auto results__ = torch::vsplit(tensor_from_ocaml(self), sections);
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor *atg_vsplit_array(gc_tensor self, int64_t *indices_data, int indices_len) {
  PROTECT(
    auto results__ = torch::vsplit(tensor_from_ocaml(self), torch::IntArrayRef(indices_data, indices_len));
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor atg_vstack(gc_tensor *tensors_data, int tensors_len) {
  PROTECT(
    torch::Tensor results__ = torch::vstack(of_carray_tensor(tensors_data, tensors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_vstack_out(gc_tensor out, gc_tensor *tensors_data, int tensors_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::vstack_out(out_local, of_carray_tensor(tensors_data, tensors_len));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor *atg_where(gc_tensor condition) {
  PROTECT(
    auto results__ = torch::where(tensor_from_ocaml(condition));
    int sz = results__.size();
    raw_tensor *out__ = (raw_tensor*)malloc((sz + 1) * sizeof(raw_tensor));
    for (int i = 0; i < sz; ++i)
      out__[i] = tensor_to_ocaml(results__[i]);
    out__[sz] = nullptr;
    return out__;
  )
}

raw_tensor atg_where_scalar(gc_tensor condition, scalar self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::where(tensor_from_ocaml(condition), *self, *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_where_scalarother(gc_tensor condition, gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::where(tensor_from_ocaml(condition), tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_where_scalarself(gc_tensor condition, scalar self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::where(tensor_from_ocaml(condition), *self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_where_self(gc_tensor condition, gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::where(tensor_from_ocaml(condition), tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_where_self_out(gc_tensor out, gc_tensor condition, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::where_out(out_local, tensor_from_ocaml(condition), tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_xlogy(gc_tensor self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::xlogy(tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_xlogy_(gc_tensor self, gc_tensor other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::xlogy_(self_local, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_xlogy_outscalar_other(gc_tensor out, gc_tensor self, scalar other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::xlogy_out(out_local, tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_xlogy_outscalar_self(gc_tensor out, scalar self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::xlogy_out(out_local, *self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_xlogy_outtensor(gc_tensor out, gc_tensor self, gc_tensor other) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::xlogy_out(out_local, tensor_from_ocaml(self), tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_xlogy_scalar_other(gc_tensor self, scalar other) {
  PROTECT(
    torch::Tensor results__ = torch::xlogy(tensor_from_ocaml(self), *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_xlogy_scalar_other_(gc_tensor self, scalar other) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::xlogy_(self_local, *other);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_xlogy_scalar_self(scalar self, gc_tensor other) {
  PROTECT(
    torch::Tensor results__ = torch::xlogy(*self, tensor_from_ocaml(other));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_zero(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::zero(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_zero_(gc_tensor self) {
  torch::Tensor self_local = tensor_from_ocaml(self);
  PROTECT(
    torch::Tensor results__ = torch::zero_(self_local);
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_zero_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::zero_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_zeros(int64_t *size_data, int size_len, int options_kind, int options_device) {
  PROTECT(
    torch::Tensor results__ = torch::zeros(torch::IntArrayRef(size_data, size_len), at::device(device_of_int(options_device)).dtype(at::ScalarType(options_kind)));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_zeros_like(gc_tensor self) {
  PROTECT(
    torch::Tensor results__ = torch::zeros_like(tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_zeros_like_out(gc_tensor out, gc_tensor self) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::zeros_like_out(out_local, tensor_from_ocaml(self));
    return tensor_to_ocaml(results__);
  )
}

raw_tensor atg_zeros_out(gc_tensor out, int64_t *size_data, int size_len) {
  torch::Tensor out_local = tensor_from_ocaml(out);
  PROTECT(
    torch::Tensor results__ = torch::zeros_out(out_local, torch::IntArrayRef(size_data, size_len));
    return tensor_to_ocaml(results__);
  )
}

